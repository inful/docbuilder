var relearn_searchindex = [
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e adr",
    "content": "ADR-000: Uniform Error Handling Across DocBuilder Date: 2025-10-03\nUpdated: 2025-12-14\nStatus ✅ Implemented - Consolidated error systems completed December 2025\nContext DocBuilder currently mixes error patterns: direct fmt.Errorf/errors.New, partial use of foundation.Result, and ad-hoc wrapping. This causes inconsistent user messages, logging, and exit/HTTP codes.\nDecision Adopt a single error model based on internal/errors with:\nCategory, severity, code, operation (op), cause, context fields, retry-eligible flag Helper constructors/wrappers: New, Wrap, and option setters: WithCode, WithOp, WithField, WithRetryable, WithHTTPStatus, WithExitCode Boundary adapters for CLI/HTTP and standardized logging fields CI guard to prevent raw error creation in non-test code Taxonomy Categories: Config, Auth, Git, Docs, Hugo, Pipeline, State, Daemon, Network, IO, Validation, NotFound, Conflict, Timeout, Canceled, RateLimit, Unknown Severity: Info, Warning, Error, Fatal Codes (examples): ConfigNotFound, ConfigInvalidYAML, ConfigValidationFailed, GitAuthFailed, GitFetchFailed, DocsWalkFailed, FrontMatterInvalid, PlanCircularDependency, StateReadFailed, StateWriteFailed, ScheduleInvalid Layer Behavior Libraries (config/git/docs/Hugo/pipeline/state): return typed errors, include op, code, retry-eligible, attach context fields; no logging Services: aggregate/wrap with higher-level op and identifiers (repo/path/url) CLI: map errors → exit codes with ExitCodeFor(err) and format user-facing messages with FormatForUser(err) HTTP: map errors → HTTP status with HTTPStatusFor(err), return JSON { error: { code, category, message, correlationId }, details? } Logging (boundary only): structured fields from error (category, code, op, retry-eligible, identifiers), level from severity Mapping Rules CLI exit codes: 0 OK; 2 Validation/Config; 10 Auth; 11 Git; 12 Docs; 13 Hugo; 20 Network (retry-eligible); 1 default HTTP status: 400 Validation; 401/403 Auth; 404 NotFound; 409 Conflict; 429 RateLimit; 504 Timeout; 500 Unknown/Internal; 503 service unavailable Migration Plan Harden internal/errors API (options/extractors for Code/Op/Retryable/HTTP/Exit) Add adapters: internal/cli/error_adapter.go (ExitCodeFor, FormatForUser) internal/daemon/http_error_adapter.go (HTTPStatusFor, JSON response builder) internal/logx/log_err.go (structured logging helper) Refactor batch 1: internal/config, internal/git, internal/docs, internal/hugo, internal/pipeline Refactor batch 2: internal/daemon, internal/state Align foundation.Result[T] usages to carry typed internal/errors errors Tests: mapping tests, adapter tests, update assertions to check category/code CI enforcement: script to fail on raw fmt.Errorf/errors.New outside tests and internal/errors Documentation: this ADR + CONTRIBUTING note Edge Cases context.Canceled → Category Canceled, Info, not retry-eligible; HTTP 499/408; CLI non-zero depending on command context.DeadlineExceeded → Timeout, retry-eligible; HTTP 504; CLI 20 Multi-errors: wrap errors.Join once, keep causes for Is/As Preserve errors.Is/As by retaining root cause Examples Before: return fmt.Errorf(\"failed to read config file: %w\", err)\nAfter: return errors.Wrap(err, errors.CategoryConfig, errors.SeverityError, \"read config file\", errors.WithOp(\"config.Load\"), errors.WithCode(errors.ConfigReadFailed))\nCLI: os.Exit(clierrors.ExitCodeFor(err))\nHTTP: status := httperrors.HTTPStatusFor(err) and return JSON problem response\nConsequences Pros: consistent UX/telemetry, easier support, better retry and policy decisions Cons: initial refactor effort, small learning curve Rollout Day 1: error API + adapters + tests Day 2–3: batch 1 refactor + tests Day 4–5: batch 2 refactor + tests Enable CI guard; iterate on any stragglers Implementation Notes (December 2025) The error system was successfully consolidated using internal/foundation/errors/ as the single source of truth:\nWhat Was Implemented:\n✅ Type-safe ErrorCategory enum (replaces string-based ErrorCode) ✅ Fluent builder API with WithContext(), WithSeverity(), WithRetry() ✅ HTTP adapter (internal/foundation/errors/http_adapter.go) ✅ CLI adapter (internal/foundation/errors/cli_adapter.go) ✅ Retry semantics built into error classification ✅ Structured context via ErrorContext map ✅ Convenience constructors: ValidationError(), NotFoundError(), etc. Migration Completed:\n✅ Removed duplicate internal/foundation/errors.go (240 lines) ✅ Migrated internal/state/ package (12 files) ✅ Migrated internal/services/ package (2 files) ✅ Migrated internal/config/ package (4 files) ✅ Updated all tests to use new API ✅ All 43 packages passing tests ✅ Zero linting issues Key Pattern Changes:\nfoundation.ValidationError() → errors.ValidationError() foundation.ErrorCodeValidation → errors.CategoryValidation classified.Code → classified.Category() WithContext(Fields{\"k\": v}) → WithContext(\"k\", v) (chained) AsClassified(err, \u0026c) bool → c, ok := AsClassified(err) Progress Checklist Finalize internal/foundation/errors API (constructors, options, extractors) Implement CLI adapter (internal/foundation/errors/cli_adapter.go) Implement HTTP adapter (internal/foundation/errors/http_adapter.go) Refactor batch 1: config, state, services Update tests to assert category and add mapping tests Wire adapters into cmd/docbuilder/main.go (future work) Update daemon HTTP handlers to use adapter (future work) Add CI guard for raw error creation (future work) Update documentation (STYLE_GUIDE.md, copilot-instructions.md)",
    "description": "ADR-000: Uniform Error Handling Across DocBuilder Date: 2025-10-03\nUpdated: 2025-12-14\nStatus ✅ Implemented - Consolidated error systems completed December 2025\nContext DocBuilder currently mixes error patterns: direct fmt.Errorf/errors.New, partial use of foundation.Result, and ad-hoc wrapping. This causes inconsistent user messages, logging, and exit/HTTP codes.",
    "tags": [
      "Error-Handling",
      "Foundation",
      "Architecture"
    ],
    "title": "ADR-000: Uniform Error Handling",
    "uri": "/docs/adr/adr-000-uniform-error-handling/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e adr",
    "content": "Date: 2025-12-12\nStatus Proposed\nContext DocBuilder’s core value is transforming documentation from Git repositories into Hugo-rendered sites. Currently, we lack comprehensive integration tests that verify the entire pipeline: repository retrieval → content discovery → Hugo generation → theme-specific transformations → final rendered output.\nCurrent State Unit tests: Cover individual components (git, docs, hugo packages) CLI integration tests: Test command execution but not output correctness Manual verification: Developers run builds and visually inspect Hugo sites Regression risk: Refactoring breaks transformations without detection Feature validation: No systematic way to verify new content transformations Problems No end-to-end validation: Can’t verify the complete pipeline produces correct output Refactoring fear: Large-scale changes (like pipeline refactor) lack safety net Bug reproduction: No standard way to create minimal reproducible test cases Feature documentation: Supported transformations not demonstrated in tests Configuration validation: Can’t systematically verify Relearn theme configuration Decision Implement a golden testing framework that:\nUses test repositories stored as directory structures in test/testdata/repos/ Creates temporary Git repos from these structures during test execution Runs full build pipeline with test-specific configurations Verifies output against stored golden files (Hugo config, content structure, rendered samples) Supports update workflow via -update-golden flag for intentional changes Architecture test/ testdata/ repos/ # Source repository structures transforms/ frontmatter-injection/ # editURL and metadata injection cross-repo-links/ # Link transformation image-paths/ # Asset path handling edge-cases/ empty-docs/ # Repository with no markdown malformed-frontmatter/ # Invalid YAML handling regression/ issue-XXX/ # Specific bug reproductions configs/ # Test configurations basic-build.yaml multi-repo.yaml custom-params.yaml golden/ # Verified output snapshots basic-build/ hugo-config.golden.yaml # Generated Hugo configuration content-structure.golden.json # File structure with front matter rendered-samples.golden.json # Selected HTML pages (optional) multi-repo/ hugo-config.golden.yaml content-structure.golden.json integration/ # Integration test package golden_test.go # Golden test framework helpers.go # Test repo setup, verification testdata_test.go # Tests using testdata repos Golden File Format Hugo Config (hugo-config.golden.yaml):\n1 2 3 baseURL: http://localhost:1313/ title: Test Documentation theme: hextra Content Structure (content-structure.golden.json):\n1 2 3 4 5 6 7 8 9 10 11 12 13 ## Hugo Configuration baseURL: \"http://localhost:1313/\" title: \"Test Documentation\" theme: \"relearn\" # Always Relearn module: imports: - path: \"github.com/McShelby/hugo-theme-relearn\" params: themeVariant: \"relearn-light\" disableSearch: false # ... other Relearn params Content Structure (content-structure.golden.json):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"files\": { \"content/_index.md\": { \"frontmatter\": { \"title\": \"Home\" }, \"contentHash\": \"sha256:abc123...\" }, \"content/test-docs/guide.md\": { \"frontmatter\": { \"title\": \"Guide\", \"editURL\": \"https://github.com/org/repo/blob/main/docs/guide.md\" }, \"contentHash\": \"sha256:def456...\" } }, \"structure\": { \"content/\": { \"_index.md\": {}, \"test-docs/\": { \"_index.md\": {}, \"guide.md\": {} } } } } Rendered Samples (rendered-samples.golden.json - optional):\n1 2 3 4 5 6 7 8 { \"public/test-docs/guide/index.html\": { \"selectors\": { \"h1\": \"Guide\", \".edit-link\": \"https://github.com/org/repo/blob/main/docs/guide.md\" } } } Test Implementation Pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func TestGolden_BasicBuild(t *testing.T) { if testing.Short() { t.Skip(\"Skipping golden test in short mode\") } // Create temp git repo from testdata structure repoPath := setupTestRepo(t, \"testdata/repos/basic-build\") defer cleanupTestRepo(t, repoPath) // Load and configure test cfg := loadGoldenConfig(t, \"testdata/configs/basic-build.yaml\") cfg.Repositories[0].URL = repoPath // Point to temp repo // Execute full build outputDir := t.TempDir() svc := build.NewDefaultService(cfg) err := svc.Build(context.Background(), outputDir) require.NoError(t, err) // Verify outputs goldenDir := \"testdata/golden/basic-build\" verifyHugoConfig(t, outputDir, goldenDir+\"/hugo-config.golden.yaml\") verifyContentStructure(t, outputDir, goldenDir+\"/content-structure.golden.json\") // Optional: verify rendered HTML if *updateGolden || fileExists(goldenDir+\"/rendered-samples.golden.json\") { verifyRenderedSamples(t, outputDir, goldenDir+\"/rendered-samples.golden.json\") } } ### Update Workflow ```bash go test ./test/integration -update-golden go test ./test/integration -run TestGolden_BasicBuild -update-golden go test ./test/integration -skip-render Rationale Why Golden Testing? Deterministic output: Hugo builds are reproducible with fixed input Comprehensive verification: Catches subtle regressions in transformations Living documentation: Test repos demonstrate supported features Regression confidence: Safe to refactor with verified snapshots Bug reproduction: Minimal test cases for issue reports Why Not Full HTML Snapshots? Size: Complete Hugo sites are large (MBs per test) Brittleness: Relearn theme updates change HTML constantly Diffability: Binary archives are hard to review in Git Focus: We care about our transformations, not theme rendering Why Separate Levels? Hugo config: Always verify (our direct output) Content structure: Lightweight, covers 90% of bugs Rendered HTML: Optional, only for transformation features (math, callouts, etc.) Why JSON for Structure? Human-readable diffs in Git Easy to update with -update-golden Programmatic verification (no string matching) Selective verification (check only relevant fields) Consequences Positive Refactoring confidence: Pipeline changes, configuration updates are safe Bug prevention: Regressions caught before merge Feature documentation: Tests show what’s supported Developer productivity: Fast feedback on changes CI integration: Automated verification on every PR Negative Test maintenance: Golden files need updates when output intentionally changes Initial investment: Writing helpers and first test cases takes time Storage: Git repo grows with golden files (mitigated by JSON, not archives) Test duration: Full builds slower than unit tests (use -short to skip) Neutral Test location: New test/integration/ package separate from unit tests Naming convention: TestGolden_* prefix for golden tests Flag dependencies: Requires -update-golden flag support Status Implemented - Golden testing framework is in place and actively used.\nNotes Single Relearn theme simplifies testing (no theme variation needed) Focus on transform pipeline and configuration generation Integration tests cover full build workflow Alternatives Considered Alternative 1: Snapshot Entire Hugo Site Rejected: Too large, brittle, not Git-friendly\nAlternative 2: Compare Only File Counts Rejected: Too coarse, misses content errors\nAlternative 3: String Matching on hugo.yaml Rejected: Brittle to formatting, whitespace\nAlternative 4: No Integration Tests Rejected: Refactoring risk too high, manual testing unreliable\nReferences Testing Go Applications Using Golden Files Go Wiki: Table Driven Tests DocBuilder: internal/hugo/*_golden_test.go (existing pattern) Hugo: Testing Documentation",
    "description": "Date: 2025-12-12\nStatus Proposed\nContext DocBuilder’s core value is transforming documentation from Git repositories into Hugo-rendered sites. Currently, we lack comprehensive integration tests that verify the entire pipeline: repository retrieval → content discovery → Hugo generation → theme-specific transformations → final rendered output.",
    "tags": [
      "Testing",
      "Golden-Tests",
      "Quality-Assurance"
    ],
    "title": "ADR-001: Golden Testing Strategy for Output Verification",
    "uri": "/docs/adr/adr-001-golden-testing-strategy/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e adr",
    "content": "Status Accepted - Implemented 2025-12-13\nContext Current Architecture (Mostly Correct) DocBuilder already implements an in-memory content pipeline with proper separation of concerns:\nDiscovery Stage (stageDiscoverDocs): Reads files from source repositories into memory once Transform Stage (copyContentFiles): Loads content into memory (file.LoadContent()) Runs dependency-ordered transform pipeline (front matter, link rewriting, etc.) Processes content entirely in memory via Page struct Writes final transformed output to disk once Index Stage (stageIndexes): Generates repository/section index pages Hugo Render (stageRunHugo): Runs Hugo on the prepared content tree The transform pipeline is already in-memory and works correctly with:\n✅ Single source read during discovery ✅ In-memory transformation via Page struct ✅ Dependency-based transform ordering ✅ Single write after all transforms complete ✅ Front matter patching and merging ✅ Link rewriting through the pipeline The Actual Problem: Index Stage Bypass One specific issue exists in /internal/hugo/indexes.go where README.md files are promoted to _index.md:\nThe useReadmeAsIndex function:\nRe-reads the source README.md file from disk (bypassing transformed content) Manually parses and manipulates front matter (duplicating transform logic) Overwrites the already-transformed file at the index location 1 2 3 4 5 6 7 8 9 10 11 // indexes.go (current problematic code) func (g *Generator) useReadmeAsIndex(...) { // ❌ Bypasses pipeline: re-reads source instead of using transformed content readmeContent, _ := os.ReadFile(readmeSourcePath) // ❌ Duplicates logic: manually parses front matter fm := parseFrontMatter(readmeContent) // ❌ Loses transforms: overwrites pipeline output with source content os.WriteFile(indexPath, readmeContent, 0644) } Impact: When README.md is promoted to _index.md, transformations applied by the pipeline (especially link rewrites) are lost because the index stage writes the original untransformed content.\nWhy This Happened The index stage was written before the transform pipeline was fully established. It predates the current dependency-based transform system and operates on the assumption that it needs to read source files directly.\nDecision Fix the index stage to use already-transformed content instead of re-reading source files. This is a targeted fix that eliminates the pipeline bypass without requiring a full architectural refactor.\nCore Insight The existing architecture is already correct - we don’t need to refactor the pipeline. We only need to:\nCapture transformed content after the pipeline runs Make it available to the index stage Stop re-reading source files in index generation Minimal Changes Required Change 1: Add field to track transformed content\n1 2 3 4 5 6 7 // internal/docs/discovery.go type DocFile struct { // ... existing fields ... Content []byte // Original source (already exists) TransformedBytes []byte // NEW: After transform pipeline // ... existing fields ... } Change 2: Capture transformed content in copyContentFiles\n1 2 3 4 5 6 7 8 9 10 11 12 // internal/hugo/content_copy.go func (g *Generator) copyContentFiles(ctx context.Context, docFiles []docs.DocFile) error { for i := range docFiles { // ... existing transform pipeline code ... // NEW: Store transformed bytes for later use docFiles[i].TransformedBytes = p.Raw // Existing: Write to disk os.WriteFile(outputPath, p.Raw, 0644) } } Change 3: Fix index generation to use transformed content\n1 2 3 4 5 6 7 8 9 10 11 12 // internal/hugo/indexes.go func (g *Generator) useReadmeAsIndex(file docs.DocFile, ...) error { // BEFORE: rawContent, _ := os.ReadFile(readmeSourcePath) // AFTER: Use already-transformed content if len(file.TransformedBytes) == 0 { return fmt.Errorf(\"README not yet transformed: %s\", file.Path) } // No need to re-parse or manipulate - just copy transformed content indexPath := filepath.Join(g.buildRoot(), \"content\", ...) return os.WriteFile(indexPath, file.TransformedBytes, 0644) } Architecture After Fix ┌──────────────┐ │ Discovery │ Load source files once │ │ DocFile.Content populated └──────┬───────┘ │ ┌──────▼───────┐ │ Transform │ Process through pipeline (in-memory) │ Pipeline │ DocFile.TransformedBytes populated ← NEW └──────┬───────┘ │ ┌──────▼───────┐ │Content Write │ Write transformed content to disk │ │ (copyContentFiles) └──────┬───────┘ │ ┌──────▼───────┐ │Index Stage │ Use DocFile.TransformedBytes ← FIXED │ │ (no re-reading source) └──────┬───────┘ │ ┌──────▼───────┐ │ Hugo Render │ Build final static site └──────────────┘ Key principle: Transform pipeline remains authoritative. Index stage becomes a pure consumer of transformed content.\nConsequences Positive Pipeline Integrity: All content flows through transform pipeline with no bypasses Bug Fix: README.md → _index.md conversion preserves link rewrites and other transforms Eliminates Duplicate Logic: Front matter parsing happens only in transform pipeline Minimal Changes: ~15 lines of code vs. full refactor Low Risk: Doesn’t change core architecture, just fixes data flow Better Testability: Can verify transformed content is used consistently Future-Proof: Makes it easier to add new transforms knowing they’ll apply everywhere Negative Minimal Memory Overhead: Adds TransformedBytes field to DocFile Mitigation: Negligible impact (content already in memory during transform) Only populated for markdown files, not assets Pass-by-value consideration: docFiles slice must be passed by reference or returned Current code already uses []docs.DocFile slice which shares backing array May need to ensure mutations are visible across function boundaries Trade-offs Avoided By not doing a full refactor, we avoid:\n❌ Rewriting working transform pipeline ❌ Changing stage interfaces ❌ Updating all transform implementations ❌ Extensive test updates ❌ Risk of introducing new bugs in working code Implementation Plan Phase 1: Foundation (Day 1-2) Files Modified: 1 file, 2 lines\nAdd TransformedBytes []byte field to DocFile struct in internal/docs/discovery.go Add godoc comment explaining field purpose Run tests to ensure no breakage from schema change Acceptance: Field compiles, tests pass\nPhase 2: Capture Transformed Content (Day 2-3) Files Modified: 1 file, ~3 lines\nIn internal/hugo/content_copy.go, after transform pipeline completes: 1 2 // After: shim.SerializeFn() docFiles[i].TransformedBytes = p.Raw Ensure this happens inside the loop that processes each file Add debug logging to verify field is populated Acceptance: TransformedBytes populated for markdown files after pipeline\nTesting:\nAdd test to verify TransformedBytes matches p.Raw Verify assets skip this (only markdown files) Phase 3: Fix Index Generation (Day 3-5) Files Modified: 1 file, ~10-15 lines\n3a. Modify useReadmeAsIndex function:\nReplace os.ReadFile(readmeSourcePath) with file.TransformedBytes Remove manual front matter parsing (already in transformed content) Add validation that TransformedBytes is populated Simplify logic - just copy transformed bytes to index location 3b. Update calling code:\nEnsure useReadmeAsIndex receives DocFile with TransformedBytes Pass full DocFile instead of just paths where needed Acceptance: README.md promoted to _index.md preserves transforms\nTesting:\nTest README.md with relative links becomes _index.md with rewritten links Test README.md with added front matter from pipeline is preserved Test multiple repositories with README files Phase 4: Integration Testing (Day 5-7) Files Modified: Test files only\nRun existing TestPipelineReadmeLinks - should now pass Add test for front matter preservation in README → _index.md Test with Relearn theme configuration Test edge cases: README without front matter README in subdirectories Repositories without README files Run full integration test suite Acceptance: All existing tests pass, README transforms preserved\nPhase 5: Documentation \u0026 Cleanup (Day 7-8) Update CONTENT_TRANSFORMS.md to document that transforms apply to all files including index promotions Add comments in indexes.go explaining why we use TransformedBytes Update this ADR status to “Accepted” Add CHANGELOG entry for bug fix Optional Cleanup (can be separate PR):\nRemove now-unused manual front matter parsing in index stage Consolidate duplicate path resolution logic Add metrics for transform pipeline coverage Timeline Summary Total Effort: 1-2 weeks (with testing) Code Changes: ~20 lines across 2 files Test Changes: ~50-100 lines for comprehensive coverage Risk Level: Low (targeted fix, no architectural changes) Rollback Plan If issues discovered:\nImmediate: Revert useReadmeAsIndex to read from disk (restore 1 function) Short-term: Add feature flag to toggle between old/new behavior Long-term: Keep TransformedBytes field for future use, fix bugs incrementally Success Criteria ✅ README.md files promoted to _index.md preserve all transforms ✅ Links in README → _index.md are correctly rewritten ✅ Front matter patches from pipeline are present in index files ✅ No regression in existing functionality ✅ All tests pass ✅ No performance degradation References Transform pipeline implementation Index generation DocFile struct Transform pipeline design BuildState architecture Related Issues README.md link rewriting bypass when promoted to _index.md Front matter patches not applied to index files Duplicate front matter parsing logic in index stage Notes Discovery Process This ADR was created after investigating why README.md files lost transform pipeline changes when promoted to _index.md. Initial analysis suggested the entire pipeline needed refactoring, but deeper investigation revealed:\nThe transform pipeline already works correctly - it processes content in-memory with proper dependency ordering The bug is isolated - only the index generation stage bypasses the pipeline The fix is minimal - capture and reuse transformed content instead of re-reading sources Key Learnings Don’t assume the architecture is broken - investigate thoroughly before proposing large refactors The codebase already implements best practices - in-memory processing, dependency resolution, staged execution Targeted fixes are often better - 20 lines beats rewriting thousands Future Enhancements This fix enables:\nConfidence that all transforms apply universally Easier debugging (single authoritative transformed content) Future optimization: avoid duplicate writes for README/index cases Created: 2025-12-13\nUpdated: 2025-12-13 (revised after codebase analysis)\nAuthor: Development Team\nDecision: Proposed → Implementation Ready",
    "description": "Status Accepted - Implemented 2025-12-13\nContext Current Architecture (Mostly Correct) DocBuilder already implements an in-memory content pipeline with proper separation of concerns:\nDiscovery Stage (stageDiscoverDocs): Reads files from source repositories into memory once Transform Stage (copyContentFiles): Loads content into memory (file.LoadContent()) Runs dependency-ordered transform pipeline (front matter, link rewriting, etc.) Processes content entirely in memory via Page struct Writes final transformed output to disk once Index Stage (stageIndexes): Generates repository/section index pages Hugo Render (stageRunHugo): Runs Hugo on the prepared content tree The transform pipeline is already in-memory and works correctly with:",
    "tags": [
      "Pipeline",
      "Content-Processing",
      "Architecture",
      "Performance"
    ],
    "title": "ADR-002: Fix Index Stage Pipeline Bypass",
    "uri": "/docs/adr/adr-002-in-memory-content-pipeline/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e adr",
    "content": "Date: 2025-12-16\nStatus Implemented - December 16, 2025 (Now Default)\nImplementation Summary The fixed transform pipeline is fully implemented and is now the default and only content processing system in DocBuilder.\nDeliverables:\n✅ New pipeline package (internal/hugo/pipeline/) ✅ Document type replacing Page/PageShim ✅ Generator and Transform function types ✅ 3 generators (main/repo/section indexes) ✅ 11 transforms (parse FM, normalize indexes, build FM, extract title, strip heading, rewrite links/images, keywords, metadata, edit link, serialize) ✅ Comprehensive unit tests (71 passing in internal/hugo) ✅ Old system removed - Transform registry, patch system, and all legacy code deleted Migration Complete (December 16, 2025):\nRemoved internal/hugo/transforms/ directory (24 files, registry-based system) Removed internal/hugo/fmcore/ directory (3 files, patch merge system) Removed visualize command and Page/PageShim abstractions Simplified integration code (content_copy.go: 216→13 lines) Net code reduction: -6,233 lines (-88%) Test Status:\nPipeline unit tests: PASS (6 test functions, 12 sub-tests) Hugo package tests: PASS (71 tests) Full short test suite: PASS (all packages) golangci-lint: 0 issues Context Current Architecture DocBuilder’s content transformation system uses a registry-based, dependency-ordered pipeline with a front matter patching system:\nComponents:\nTransformRegistry: Global registry where transforms register themselves via init() Transform interface: Requires Name(), Priority(), DependsOn(), and Apply(page *Page) Patch Dependency resolution: Topological sort based on DependsOn() declarations Patch system: Three merge modes (MergeDeep, MergeReplace, MergeSetIfMissing) with priority-based ordering Protected keys: Reserved front matter fields that block MergeDeep patches Current transforms:\nfront_matter_builder_v2 (priority 50): Initializes base front matter extract_index_title (priority 55): Extracts H1 from README/index files strip_heading: Removes first H1 from content relative_link_rewriter: Fixes relative markdown links image_link_rewriter: Fixes image paths Various metadata injectors (repo info, edit links, etc.) Example transform:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func init() { RegisterTransform(\u0026ExtractIndexTitle{}) } type ExtractIndexTitle struct{} func (t *ExtractIndexTitle) Name() string { return \"extract_index_title\" } func (t *ExtractIndexTitle) Priority() int { return 55 } func (t *ExtractIndexTitle) DependsOn() []string { return []string{\"front_matter_builder_v2\"} } func (t *ExtractIndexTitle) Apply(pg *Page) Patch { // Extract title from H1 return Patch{ Mode: fmcore.MergeReplace, // Required to override protected \"title\" key Priority: 55, FrontMatterUpdates: map[string]any{ \"title\": extractedTitle, }, } } Problems with Current Architecture Hidden complexity: Dependencies and execution order are not obvious from reading the code\nNon-local reasoning: Understanding transform behavior requires checking:\nRegistration order in init() Declared dependencies in DependsOn() Priority values across multiple transforms Protected key system in patching logic Merge mode semantics (MergeDeep vs MergeReplace) Debugging difficulty:\nRecent bug: extract_index_title extracted correct title but was silently blocked by protected keys Required temporary debug logging to discover the issue Solution was non-obvious: change MergeDeep to MergeReplace Indirection overhead:\nRegistry pattern adds abstraction without benefit Topological sort runs on every build Patch merging adds cognitive overhead False flexibility:\nUsers cannot configure transforms dynamically Registry/dependency system suggests extensibility we don’t support Added complexity without delivering value Maintenance burden:\nAdding transforms requires understanding registration, priorities, dependencies, and patch semantics Easy to introduce subtle bugs (wrong merge mode, missing dependency, priority conflicts) Key Insight DocBuilder is greenfield and we control the pipeline. We don’t need dynamic transform registration or user-configurable pipelines. We need a solid, predictable pipeline for our specific use case.\nDecision Replace the registry-based, patch-driven pipeline with a fixed, explicit transform pipeline.\nCore Principles Fixed execution order: Transforms are called in explicit sequence defined in code Direct mutation: Transforms modify Document directly (no patching) No dynamic registration: No init() registry, no dependency declarations Simple interfaces: Transform = function that modifies a document Transparent behavior: Reading the pipeline code shows exact execution order New Architecture Core Interfaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // FileTransform modifies a document in the pipeline. // Can optionally return new documents to inject into the pipeline. // New documents will be queued and processed through ALL transforms from the beginning. type FileTransform func(doc *Document) ([]*Document, error) // FileGenerator creates new documents based on analysis of discovered documents. // Generators run before transforms to create missing files (e.g., _index.md). type FileGenerator func(ctx *GenerationContext) ([]*Document, error) // GenerationContext provides access to discovered files for analysis. type GenerationContext struct { Discovered []*Document // All discovered files from repositories Config *config.Config } // Document represents a file being processed through the pipeline. type Document struct { // Content is the markdown body (transformed in-place) Content string // FrontMatter is the YAML front matter (modified directly) FrontMatter map[string]any // Metadata for transforms to use Path string // Hugo content path (e.g., \"repo-name/section/file.md\") IsIndex bool // True if this is _index.md or README.md Repository string // Source repository name SourceCommit string // Git commit SHA SourceURL string // Repository URL for edit links Generated bool // True if this was generated (not discovered) } Pipeline Execution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 // processContent runs the complete content processing pipeline. func (g *Generator) processContent(discovered []*Document) ([]*Document, error) { // Phase 1: Generation - Create missing files generators := []FileGenerator{ generateMainIndex, // 1. Create site _index.md generateRepositoryIndexes, // 2. Create repo _index.md files generateSectionIndexes, // 3. Create section _index.md files } ctx := \u0026GenerationContext{ Discovered: discovered, Config: g.config, } var generated []*Document for _, generator := range generators { docs, err := generator(ctx) if err != nil { return nil, fmt.Errorf(\"generation failed: %w\", err) } generated = append(generated, docs...) } // Combine discovered + generated allDocs := append(discovered, generated...) // Phase 2: Transformation - Process all documents transforms := []FileTransform{ computeBaseFrontMatter, // 1. Initialize FrontMatter from file extractIndexTitle, // 2. Extract H1 title from index files stripHeading, // 3. Strip H1 if appropriate rewriteRelativeLinks, // 4. Fix markdown links rewriteImageLinks, // 5. Fix image paths generateFromKeywords, // 6. Create new files based on keywords (e.g., ) addRepositoryMetadata, // 7. Add repo/commit/source metadata addEditLink, // 8. Generate edit URL } // Process documents iteratively - newly generated docs go through all transforms processedDocs := make([]*Document, 0, len(allDocs)) queue := append([]*Document{}, allDocs...) for len(queue) \u003e 0 { doc := queue[0] queue = queue[1:] // Run all transforms on this document for _, transform := range transforms { newDocs, err := transform(doc) if err != nil { return nil, fmt.Errorf(\"transform failed for %s: %w\", doc.Path, err) } // Prevent generated documents from creating new documents if len(newDocs) \u003e 0 \u0026\u0026 doc.Generated { return nil, fmt.Errorf( \"generated document %s attempted to create new documents (transforms should not generate from generated docs)\", doc.Path, ) } // Queue new documents for full transform pipeline if len(newDocs) \u003e 0 { queue = append(queue, newDocs...) } } processedDocs = append(processedDocs, doc) } return processedDocs, nil } Example Generator (Creates New Files):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // generateSectionIndexes creates _index.md for sections that don't have one. func generateSectionIndexes(ctx *GenerationContext) ([]*Document, error) { // Group discovered files by section sections := make(map[string][]*Document) for _, doc := range ctx.Discovered { section := filepath.Dir(doc.Path) sections[section] = append(sections[section], doc) } var generated []*Document for section, docs := range sections { // Check if index already exists hasIndex := false for _, doc := range docs { if doc.IsIndex { hasIndex = true break } } if !hasIndex { // Generate missing index indexDoc := \u0026Document{ Path: filepath.Join(section, \"_index.md\"), IsIndex: true, Generated: true, Content: generateIndexContent(section, docs), FrontMatter: map[string]any{ \"title\": titleCase(filepath.Base(section)), \"type\": \"docs\", }, } generated = append(generated, indexDoc) } } return generated, nil } Example Transform (Modifies Existing Files):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // extractIndexTitle extracts the first H1 heading as the title for index files. // Only applies if no text exists before the H1. func extractIndexTitle(doc *Document) ([]*Document, error) { if !doc.IsIndex { return nil, nil // Only process index files, no new docs } h1Pattern := regexp.MustCompile(`(?m)^# (.+)$`) loc := h1Pattern.FindStringIndex(doc.Content) if loc == nil { return nil, nil // No H1 found, no new docs } // Check for text before H1 textBeforeH1 := strings.TrimSpace(doc.Content[:loc[0]]) if textBeforeH1 != \"\" { return nil, nil // Use filename as title, no new docs } // Extract title and set directly matches := h1Pattern.FindStringSubmatch(doc.Content) doc.FrontMatter[\"title\"] = matches[1] return nil, nil // Modified doc in-place, no new docs } Example Transform (Generates New Files Based on Keywords):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // generateFromKeywords scans for special keywords and generates related files. // Example: tag creates a glossary page from all terms. // // If this transform returns new documents while processing a Generated document, // the pipeline will return an error automatically - no need to check here. func generateFromKeywords(doc *Document) ([]*Document, error) { var newDocs []*Document // Check for marker if strings.Contains(doc.Content, \"\") { // Extract all glossary terms from this document terms := extractGlossaryTerms(doc.Content) if len(terms) \u003e 0 { // Generate glossary document // This will go through ALL transforms: front matter, link rewriting, etc. glossaryDoc := \u0026Document{ Path: filepath.Join(doc.Repository, \"glossary.md\"), IsIndex: false, Generated: true, // Mark as generated Content: renderGlossary(terms), FrontMatter: map[string]any{ \"title\": \"Glossary\", \"type\": \"docs\", \"generated\": true, \"source_doc\": doc.Path, }, Repository: doc.Repository, SourceCommit: doc.SourceCommit, SourceURL: doc.SourceURL, } newDocs = append(newDocs, glossaryDoc) } // Remove marker from original content doc.Content = strings.ReplaceAll(doc.Content, \"\", \"\") } // Check for other keywords... // if strings.Contains(doc.Content, \"@api-reference\") { ... } return newDocs, nil } Migration Path Phase 1: Create New Pipeline (Parallel)\nDefine Document, FileTransform, FileGenerator, GenerationContext types Create processContent() with generation + transform phases Convert existing index generation logic to generators Convert existing transforms to new interface (one by one) Add comprehensive tests for new pipeline Phase 2: Switch Over\nUpdate copyContentFiles() to use new pipeline Run integration tests to verify behavior Fix any discrepancies Phase 3: Cleanup\nRemove old Transform interface Remove TransformRegistry Remove topological sort logic Remove patch system (Patch, MergeMode, protected keys) Remove old transform files Phase 4: Documentation\nUpdate copilot instructions Document transform pipeline in architecture docs Add examples for adding new transforms Consequences Positive ✅ Predictable: Execution order is explicit in code\n✅ Debuggable: Set breakpoint in pipeline, step through transforms sequentially\n✅ Testable: Test individual transforms/generators or full pipeline easily\n✅ Maintainable: No magic, no hidden dependencies, no indirection\n✅ Fast: No registry lookups, no topological sorting, no patch merging\n✅ Simple onboarding: New developers see exact transform order immediately\n✅ Reliable: Fixed pipeline means consistent, reproducible behavior\n✅ Separation of concerns: Generation (creating files) separate from transformation (modifying files)\n✅ Dynamic generation: Transforms can create new files based on content analysis (keywords, patterns, etc.)\n✅ Composable: New documents flow through remaining transforms automatically\nNegative ⚠️ Less flexible: Cannot dynamically add/remove transforms (but we don’t need this)\n⚠️ Migration effort: Need to convert all existing transforms\nNeutral Pipeline is now explicitly ordered instead of dependency-ordered Transforms mutate directly instead of returning patches Code location becomes important (pipeline defined in generator.go) Alternatives Considered 1. Keep Current System, Fix Bugs Description: Continue using registry + patches, improve documentation\nRejected because:\nDoesn’t address root cause (unnecessary complexity) Bug was symptom of overly complex system Future maintainers will face same issues 2. Plugin Architecture Description: Make transforms truly pluggable with user configuration\nRejected because:\nMassive scope increase Users don’t need this flexibility Introduces security/stability risks Not aligned with project goals 3. Middleware Pattern Description: Chain of responsibility with explicit next() calls\nRejected because:\nMore complex than simple function list Doesn’t add value for our use case Makes testing harder (mocking next()) Implementation Plan ✅ Completed December 16, 2025\nPhase 1: Core Pipeline (Completed)\nCreated internal/hugo/pipeline/ package Implemented Document type with front matter and content fields Built Processor with two-phase execution (generators → transforms) Added queue-based processing for dynamic document generation Phase 2: Transforms Migration (Completed)\nConverted all 10 essential transforms to FileTransform functions Implemented 3 generators for index file creation Removed dependency on registry, patches, and Page abstraction All transforms use direct mutation pattern Phase 3: Integration (Completed)\nCreated copyContentFilesPipeline() integration function Added environment variable feature flag (DOCBUILDER_NEW_PIPELINE=1) Maintained backward compatibility with old system Updated copilot instructions Phase 4: Testing \u0026 Validation (Completed)\nUnit tests for all generators and transforms Edge case coverage (empty FM, no FM, malformed FM) Integration via feature flag tested All tests passing, linter clean Remaining Work (Separate from this ADR):\nRemove old registry/patch system Update golden test expectations (theme system issue) Make new pipeline the default Documentation updates Actual effort: 1 day (vs estimated 3-5 days)\nImplementation Details File Structure internal/hugo/pipeline/ ├── document.go # Document type, NewDocumentFromDocFile ├── processor.go # Processor with ProcessContent ├── generators.go # generateMainIndex, generateRepositoryIndex, generateSectionIndex ├── transforms.go # All 10 transforms └── pipeline_test.go # Comprehensive unit tests Key Design Decisions Direct Mutation: Documents are modified in-place, no patch merging Type Safety: Compile-time verification of transform signatures Queue-Based: Generators can add new documents during processing Stateless Transforms: Pure functions with no global state Feature Flag: Environment variable enables new pipeline without code changes Open Questions All questions resolved during implementation:\nError handling: ✅ Transforms return errors, pipeline fails fast Transform state: ✅ Pass context via RepositoryMetadata parameter Partial failures: ✅ Fail fast on first error (single-pass pipeline) Testing strategy: ✅ Both unit tests per transform and integration tests Front matter parsing: ✅ Handle edge cases (empty FM, no FM, malformed FM) Generator ordering: ✅ All generators run before any transforms References Issue: “README H1 duplicate headers” (revealed patch system complexity) ADR-002: In-Memory Content Pipeline (established single-pass architecture) Copilot Instructions: Transform pipeline section (needs update) Style Guide: Function naming conventions (already compatible) Decision Makers @inful (Lead Developer) Notes This refactor aligns with DocBuilder’s greenfield status and aggressive refactoring posture. We’re optimizing for clarity and maintainability over theoretical flexibility we don’t need.",
    "description": "Date: 2025-12-16\nStatus Implemented - December 16, 2025 (Now Default)\nImplementation Summary The fixed transform pipeline is fully implemented and is now the default and only content processing system in DocBuilder.\nDeliverables:\n✅ New pipeline package (internal/hugo/pipeline/) ✅ Document type replacing Page/PageShim ✅ Generator and Transform function types ✅ 3 generators (main/repo/section indexes) ✅ 11 transforms (parse FM, normalize indexes, build FM, extract title, strip heading, rewrite links/images, keywords, metadata, edit link, serialize) ✅ Comprehensive unit tests (71 passing in internal/hugo) ✅ Old system removed - Transform registry, patch system, and all legacy code deleted Migration Complete (December 16, 2025):",
    "tags": [
      "Pipeline",
      "Transforms",
      "Architecture",
      "Simplification"
    ],
    "title": "ADR-003: Fixed Transform Pipeline",
    "uri": "/docs/adr/adr-003-fixed-transform-pipeline/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e adr",
    "content": "Date: 2025-12-29\nStatus Proposed\nContext DocBuilder processes documentation from multiple Git repositories, transforming markdown files into Hugo-compatible sites. Currently, developers discover issues only after committing and running builds:\nCurrent Pain Points Late feedback: Filename issues (spaces, mixed-case) discovered during Hugo build Silent failures: Invalid frontmatter causes pages to render incorrectly or not at all Broken links: Cross-references break when files are renamed without updating links Path inconsistencies: Mixed naming conventions (README.md, api-guide.md, My Document.md) in same repository Asset orphans: Images referenced but not committed, or committed but never referenced Hugo quirks: Reserved filenames (_index.md vs index.md) behave differently but look similar Impact Developers commit documentation that fails to build CI/CD pipelines fail unexpectedly Manual inspection required to diagnose issues Inconsistent documentation quality across repositories Time wasted on avoidable build failures Hugo and DocBuilder Best Practices Hugo has specific expectations:\nFilenames become URL slugs: My Document.md → /my%20document/ (problematic) Case sensitivity varies by OS: README.md vs readme.md causes issues Special files: _index.md (section landing), index.md (leaf bundle) Asset paths must match exactly (case-sensitive even on macOS/Windows during deployment) DocBuilder’s discovery system (internal/docs/) walks repositories and expects:\nLowercase filenames for predictable Hugo paths No spaces or special characters (except -, _, .) Valid UTF-8 frontmatter Relative paths for cross-document links Decision Implement a documentation linting system with multiple integration points:\nCLI command: docbuilder lint [path] for manual validation Git hooks: Traditional pre-commit hooks or lefthook for automatic checking CI/CD integration: GitHub Actions / GitLab CI step for PR validation Architecture internal/ lint/ linter.go # Core linting engine rules.go # Rule definitions and severity formatters.go # Human-readable output fixer.go # Automatic fix transformations cmd/docbuilder/ commands/ lint.go # CLI command implementation scripts/ install-hooks.sh # Traditional pre-commit hook installer lefthook.yml # Lefthook configuration (optional) .github/ workflows/ lint-docs.yml # CI validation workflow Linting Rules Rules are fixed and opinionated based on Hugo/DocBuilder best practices. No configuration override.\nFilename Rules (Errors - Block Build) Allowed pattern: [a-z0-9-_.] with exceptions for whitelisted double extensions:\n.drawio.png - Draw.io embedded PNG diagrams .drawio.svg - Draw.io embedded SVG diagrams Rule Severity Rationale Uppercase letters in filename Error Causes URL inconsistency, case-sensitivity issues Spaces in filename Error Breaks Hugo URL generation, creates %20 in paths Special characters (not [a-z0-9-_.]) Error Unsupported by Hugo slugify, potential shell escaping issues Leading/trailing hyphens or underscores Error Creates malformed URLs (/-docs/ or /_temp/) Double extensions (except whitelisted) Error Processed as markdown, causes build errors. Allowed: .drawio.png, .drawio.svg (embedded diagrams) Reserved names without prefix (tags.md, categories.md) Error Conflicts with Hugo taxonomy URLs Error Message Example:\nERROR: Invalid filename detected File: docs/API Guide.md Issue: Contains space characters and uppercase letters Fix: Rename to: docs/api-guide.md Spaces in filenames create problematic URLs (/api%20guide/) and may break cross-references. Hugo expects lowercase, hyphen-separated filenames. To fix automatically: docbuilder lint --fix docs/ Content Rules (Errors - Block Build) Rule Severity Rationale Malformed frontmatter YAML Error Hugo fails to parse, page skipped silently Missing closing --- in frontmatter Error Entire file treated as frontmatter Invalid frontmatter keys (duplicates) Error Undefined Hugo behavior Broken internal links ([text](/docs/adr/missing)) Error 404s in production, poor UX Image references to non-existent files Error Missing images break layout Error Message Example:\nERROR: Invalid frontmatter detected File: docs/installation.md Line: 3 Issue: YAML parsing failed: mapping values are not allowed here --- title: Installation Guide date: 2025-12-29 invalid key without colon --- Frontmatter must be valid YAML enclosed by --- markers. Check for proper indentation and key: value format. Structure Rules (Warnings - Allow but Notify) Rule Severity Rationale Missing _index.md in directory with docs Warning Directory won’t have landing page, appears empty in nav Deeply nested structure (\u003e4 levels) Warning Poor navigation UX, consider flattening Orphaned assets (unreferenced images) Warning Bloats repository, may be leftover from deletions Mixed file naming styles in same directory Warning Inconsistent developer experience Warning Message Example:\nWARNING: Missing section index file Directory: docs/api/ Issue: Contains 5 markdown files but no _index.md Impact: Section will not appear in navigation sidebar Create _index.md to define this section: --- title: \"API Documentation\" weight: 2 --- This section contains API guides and references. Asset Rules (Warnings - Allow but Notify) Rule Severity Rationale Image filename with spaces/uppercase Warning Works but creates inconsistent URLs Absolute URLs to internal assets Warning Breaks in local development, not portable Large binary files (\u003e5MB) Warning Git performance, consider external hosting Embedded diagram formats (.drawio.png, .drawio.svg) Info Valid double extension for editable diagrams, explicitly allowed Implementation Phases Phase 1: Core Linting Engine (Week 1)\nImplement internal/lint package with rule engine Filename validation (errors only) Human-readable formatter Unit tests for each rule Phase 2: CLI and Manual Workflow (Week 1-2)\nAdd docbuilder lint command Intelligent default path detection (docs/ or documentation/) Support single file, directory, and recursive modes Exit codes: 0 (clean), 1 (warnings), 2 (errors) Colorized terminal output (red errors, yellow warnings) Phase 3: Auto-Fix Capability (Week 2)\nImplement safe file renaming with link resolution: Rename files: My Doc.md → my-doc.md Scan all markdown files for links to renamed files Update internal references preserving link style (relative/absolute) Handle image links, inline links, reference-style links Preserve anchor fragments (#section) in links Preserve Git history (using git mv) Require --fix flag and confirmation prompt showing: Files to be renamed Markdown files that will be updated Total number of links to be modified Dry-run mode: --fix --dry-run shows all changes without applying Generate detailed fix report with before/after comparison Phase 4: Integration Hooks (Week 3)\nTraditional pre-commit hook script (scripts/install-hooks.sh) Lefthook configuration (lefthook.yml) GitHub Actions workflow example GitLab CI template Documentation in docs/how-to/setup-linting.md Phase 5: Content and Structure Rules (Future)\nFrontmatter validation Link checking Orphaned asset detection Structure recommendations Lint Command Interface Default Behavior: When run without arguments, docbuilder lint uses intelligent path detection:\nIf docs/ directory exists in current directory → lint docs/ If documentation/ directory exists → lint documentation/ Otherwise → lint current directory (.) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docbuilder lint docbuilder lint . docbuilder lint ./docs docbuilder lint --fix docbuilder lint --fix --yes docbuilder lint --fix --dry-run docbuilder lint --quiet docbuilder lint --format=json docbuilder lint --explain Exit Codes 0: No issues found (clean) 1: Warnings present but no errors 2: Errors found (build would fail) 3: Lint execution error (filesystem access, etc.) Output Format Linting documentation in: ./docs ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ ✗ docs/API Guide.md ERROR: Invalid filename └─ Contains uppercase letters and spaces Current: docs/API Guide.md Suggested: docs/api-guide.md Why: Spaces create %20 in URLs; uppercase causes case-sensitivity issues across platforms. Fix: docbuilder lint --fix docs/ ⚠ docs/api/_index.md WARNING: Missing section title └─ Frontmatter has no title field Add title to frontmatter: --- title: \"API Documentation\" --- ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Results: 2 files scanned 1 error (blocks build) 1 warning (should fix) Exit code: 2 Safety Guarantees for Auto-Fix The --fix flag will only perform transformations that are provably safe:\nFilename normalization: Lowercase + hyphenate, preserving whitelisted double extensions (reversible) Link updates: Update relative links in same repo (validated before commit) Git integration: Use git mv to preserve history Atomic operations: All-or-nothing (rollback on any failure) Backup prompt: Confirms user wants to proceed Dry-run first: Shows changes before applying Default Path Detection To minimize friction, docbuilder lint intelligently detects documentation directories:\nDetection Order:\nCheck for docs/ directory in current path Check for documentation/ directory in current path Fallback to current directory (.) Override Behavior:\nExplicit path argument always takes precedence: docbuilder lint ./custom-docs Use docbuilder lint . to explicitly lint current directory Rationale:\nMost projects use docs/ or documentation/ as standard convention Reduces cognitive load for developers (just run docbuilder lint) Follows principle of least surprise Works naturally in CI/CD where working directory is project root Will NOT auto-fix:\nFrontmatter structure (too complex, context-dependent) External links (can’t validate without network) Content rewrites (subjective) Cross-repository links (affects multiple repos) Git Hooks Integration Option 1: Lefthook (Recommended) Lefthook is a fast, modern Git hooks manager. Add to lefthook.yml in repository root:\n1 2 3 4 5 6 7 8 9 10 11 12 13 pre-commit: parallel: true commands: lint-docs: glob: \"*.{md,markdown,png,jpg,jpeg,gif,svg}\" run: docbuilder lint {staged_files} --quiet stage_fixed: true # Auto-stage files fixed with --fix flag # Optional: auto-fix on commit lint-docs-fix: glob: \"*.{md,markdown,png,jpg,jpeg,gif,svg}\" run: docbuilder lint {staged_files} --fix --yes stage_fixed: true Installation:\n1 2 3 4 brew install lefthook # macOS go install github.com/evilmartians/lefthook@latest lefthook install Benefits:\nFast parallel execution Easier to configure and maintain Portable configuration (checked into repo) Supports multiple hooks and commands Auto-staging of fixed files Option 2: Traditional Pre-Commit Hook Install via: docbuilder lint install-hook\nGenerated hook at .git/hooks/pre-commit:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/sh STAGED_FILES=$(git diff --cached --name-only --diff-filter=ACM | grep -E '\\.(md|markdown|png|jpg|jpeg|gif|svg)$') if [ -n \"$STAGED_FILES\" ]; then echo \"Linting staged documentation files...\" docbuilder lint $STAGED_FILES --quiet LINT_EXIT=$? if [ $LINT_EXIT -eq 2 ]; then echo \"\" echo \"❌ Lint errors found. Commit blocked.\" echo \"Fix errors or run: docbuilder lint --fix\" exit 1 elif [ $LINT_EXIT -eq 1 ]; then echo \"\" echo \"⚠️ Lint warnings present. Consider fixing before commit.\" echo \"To auto-fix: docbuilder lint --fix\" # Allow commit but show warning exit 0 fi fi exit 0 CI/CD Integration GitHub Actions (.github/workflows/lint-docs.yml):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 name: Lint Documentation on: [pull_request] jobs: lint: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Install DocBuilder run: | curl -L https://github.com/org/docbuilder/releases/latest/download/docbuilder-linux-amd64 -o docbuilder chmod +x docbuilder sudo mv docbuilder /usr/local/bin/ - name: Lint Documentation run: docbuilder lint ./docs --format=json \u003e lint-report.json - name: Upload Report if: always() uses: actions/upload-artifact@v3 with: name: lint-report path: lint-report.json - name: Comment PR if: failure() uses: actions/github-script@v6 with: script: | const report = require('./lint-report.json'); // Post formatted comment with errors GitLab CI (.gitlab-ci.yml):\n1 2 3 4 5 6 7 8 9 10 lint-docs: stage: test image: docbuilder:latest script: - docbuilder lint ./docs --format=json | tee lint-report.json artifacts: when: always reports: junit: lint-report.json allow_failure: false Auto-Fix Implementation: Link Resolution When the --fix flag is used, renaming files requires updating all internal markdown links that reference those files. This is critical to prevent broken documentation after auto-fixing filename issues.\nLink Resolution Strategy Supported Link Types The fixer must handle all common markdown link patterns:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u003c!-- 1. Inline links (most common) --\u003e [API Guide](/docs/adr/API_Guide) [API Guide](/docs/adr/API_Guide) [API Guide](/docs/docs/API_Guide) \u003c!-- 2. Absolute repository paths --\u003e [API Guide](/docs/API_Guide.md) \u003c!-- 3. Reference-style links --\u003e [API Guide][api-ref] [api-ref]: API_Guide.md \"API Documentation\" \u003c!-- 4. Image links --\u003e ![Architecture](/docs/adr/architecture_diagram.png) ![Logo](/docs/adr/assets/company_logo.svg) \u003c!-- 5. Links with anchors (preserve fragment) --\u003e [Authentication](/docs/adr/API_Guide.md#authentication) [Overview](/docs/adr/API_Guide.md#overview) \u003c!-- 6. Links in code blocks (ignore) --\u003e ```bash #### Resolution Algorithm **Phase 1: Discover Links** ```go type LinkReference struct { SourceFile string // File containing the link LineNumber int // Line number of link LinkType LinkType // Inline, Reference, Image Target string // Link target (path) Fragment string // Anchor fragment (#section) FullMatch string // Complete original text for replacement } func (f *Fixer) findLinksToFile(targetPath string) ([]LinkReference, error) { // 1. Walk all .md files in documentation directory // 2. For each file, scan for links using regex patterns: // - Inline: \\[([^\\]]+)\\]\\(([^)]+)\\) // - Reference: ^\\[([^\\]]+)\\]:\\s*(.+?)(?:\\s+\"([^\"]*)\")?$ // - Image: !\\[([^\\]]*)\\]\\(([^)]+)\\) // 3. Parse link target to extract path and fragment // 4. Resolve relative paths to absolute workspace paths // 5. Compare resolved path with target file // 6. Collect all matches as LinkReference structs } Phase 2: Path Resolution\n1 2 3 4 5 6 7 8 9 10 func resolveRelativePath(sourceFile, linkTarget string) (string, error) { // Given: sourceFile = \"docs/guides/tutorial.md\" // linkTarget = \"../api/API_Guide.md\" // // 1. Get directory of source file: \"docs/guides/\" // 2. Join with link target: \"docs/guides/../api/API_Guide.md\" // 3. Clean path: \"docs/api/API_Guide.md\" // 4. Resolve to absolute workspace path // 5. Return canonical path for comparison } Phase 3: Generate Replacement\n1 2 3 4 5 6 7 8 9 10 11 12 func (f *Fixer) updateLink(ref LinkReference, oldPath, newPath string) string { // Preserve link style (relative vs absolute) // Update filename while keeping directory structure // Preserve anchor fragments // Preserve reference link titles // Example: // Original: [API](/docs/api/API_Guide.md#auth) // Old path: docs/api/API_Guide.md // New path: docs/api/api-guide.md // Result: [API](/docs/api/api-guide.md#auth) } Phase 4: Apply Updates\n1 2 3 4 5 6 7 8 9 func (f *Fixer) applyLinkUpdates(updates []LinkUpdate) error { // 1. Group updates by source file // 2. For each file, apply all updates atomically: // - Read file content // - Apply replacements (in reverse line order to preserve offsets) // - Write updated content // 3. If any update fails, rollback all changes // 4. Generate fix report showing what was updated } Edge Cases and Safety Case 1: External URLs\n1 [GitHub Docs](https://github.com/docs/API_Guide.md) Resolution: Skip. Only update links to local files. Detect by checking for protocol scheme (http://, https://).\nCase 2: Broken Links\n1 [Old Guide](/docs/adr/Deleted_File) Resolution: If link target doesn’t exist and matches old filename pattern, report separately as “potential broken link” but don’t update.\nCase 3: Multiple Files Same Name\n1 2 docs/api/README.md docs/guides/README.md Resolution: Use full path matching. Only update links that resolve to the specific file being renamed.\nCase 4: Circular References\n1 2 3 4 5 \u003c!-- File A.md --\u003e [See B](/docs/adr/B) \u003c!-- File B.md --\u003e [See A](/docs/adr/A) Resolution: No special handling needed. Each file rename updates its own references independently.\nCase 5: Links in Code Blocks\n1 2 3 ```bash curl https://example.com/API_Guide.md ``` Resolution: Don’t update links inside code blocks. Use markdown parser to identify fenced code blocks and skip them.\nCase 6: Case-Insensitive Filesystems\n1 [Guide](/docs/adr/api_guide) # Links to API_Guide.md on macOS/Windows Resolution: Perform case-insensitive path comparison when checking if link targets the file being renamed.\nUser Confirmation Flow When --fix flag is used without --yes, show interactive confirmation:\nFound 3 files with naming issues: Files to rename: 1. docs/API_Guide.md → docs/api-guide.md 2. docs/User Manual.md → docs/user-manual.md 3. images/Company_Logo.png → images/company-logo.png Links to update: • docs/index.md (2 links) • docs/guides/getting-started.md (1 link) • docs/tutorials/quickstart.md (4 links) Total: 7 links in 3 files will be updated This will: ✓ Rename 3 files using git mv (preserves history) ✓ Update 7 internal links in 3 markdown files ✓ Create backup: .docbuilder-backup-20251229-143052/ Proceed with fixes? [y/N]: _ Dry-Run Output docbuilder lint --fix --dry-run shows what would change without applying:\nDRY RUN: No changes will be applied [File Renames] docs/API_Guide.md → docs/api-guide.md [Link Updates] docs/index.md:12 Before: [API Guide](/docs/adr/API_Guide) After: [API Guide](/docs/adr/api-guide) docs/index.md:45 Before: ![Diagram](/docs/adr/../images/architecture_diagram.png) After: ![Diagram](/docs/adr/../images/architecture-diagram.png) docs/guides/getting-started.md:8 Before: See the [API Guide](/docs/API_Guide.md#authentication) for details. After: See the [API Guide](/docs/api-guide.md#authentication) for details. Summary: 3 files would be renamed 7 links would be updated across 3 files Implementation Phases Phase 3a: Basic Renaming\nFile rename with git mv support Confirmation prompts Dry-run mode Phase 3b: Link Discovery\nScan markdown files for links Parse inline, reference, and image links Resolve relative paths Phase 3c: Link Updates\nGenerate replacement text Apply updates atomically Rollback on failure Phase 3d: Edge Cases\nSkip external URLs Handle code blocks Case-insensitive matching Phase 3e: Reporting\nDetailed fix report Dry-run preview Interactive confirmation Testing Strategy Follow ADR-001 golden testing approach:\ntest/ testdata/ lint/ valid/ correct-filenames/ # All lowercase, hyphens proper-frontmatter/ # Valid YAML correct-links/ # All internal links valid whitelisted-extensions/ # .drawio.png, .drawio.svg invalid/ mixed-case/ # MixedCase.md files spaces/ # My Document.md special-chars/ # file@name.md, file#tag.md bad-frontmatter/ # Malformed YAML broken-links/ # Links to non-existent files invalid-double-ext/ # .md.backup, .markdown.old fix/ links/ # Test files for link resolution before/ docs/ API_Guide.md # File to be renamed index.md # Contains link to API_Guide.md guides/ tutorial.md # Contains relative link ../API_Guide.md images/ Diagram.png # Image to be renamed after/ # Expected state after --fix applied docs/ api-guide.md # Renamed file index.md # Link updated to api-guide.md guides/ tutorial.md # Link updated to ../api-guide.md images/ diagram.png # Renamed image golden/ mixed-case.golden.json # Expected error output spaces.golden.json # Expected error output drawio-allowed.golden.json # Verify .drawio.* passes fix-with-links.golden.json # Expected fix report with link updates fix-dry-run.golden.txt # Expected dry-run preview output Test Coverage:\nEach rule has unit test with valid/invalid cases Integration tests run linter on test directories Golden files verify exact error messages Auto-fix tests verify safe transformations Test file renaming with git mv Test link discovery and resolution Test link updates preserve style (relative/absolute) Test anchor fragments are preserved Test external URLs are not modified Test links in code blocks are ignored Test rollback on failure Pre-commit hook tested via Git test repository Default path detection tested (docs/, documentation/, fallback) Link resolution tests: Unit tests for path resolution (relative → absolute) Unit tests for link regex patterns (inline, reference, image) Integration tests with before/after directory structures Edge case tests (external URLs, code blocks, broken links) Case-insensitive filesystem tests Keeping Linting Rules Synchronized with DocBuilder The linting system must stay synchronized with DocBuilder’s actual behavior to remain useful. As DocBuilder evolves—adding new features or changing how it processes documentation—the linter must reflect these changes.\nSynchronization Strategy 1. Shared Test Infrastructure\nLinting rules should be validated against actual DocBuilder behavior, not assumptions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // Test that linter rejects what DocBuilder would fail to build func TestLintRejectsInvalidFiles(t *testing.T) { invalidFiles := []string{ \"My Document.md\", // Spaces \"API_Guide.md\", // Uppercase \"file@special.md\", // Special chars } for _, file := range invalidFiles { // Verify linter catches it result := linter.LintFile(file) require.True(t, result.HasErrors()) // Verify DocBuilder would actually fail/warn buildResult := docbuilder.BuildWithFile(file) require.False(t, buildResult.Success()) } } 2. Integration Tests with Full Pipeline\nPeriodically run integration tests that:\nCreate test repositories with various violations Run docbuilder build on them Verify linter warnings/errors match actual build issues Catch cases where linter is too strict or too lenient Example test structure:\ntest/integration/ lint_docbuilder_sync_test.go # Tests linter matches build behavior 3. Version Alignment\nLinting rules should evolve with DocBuilder versions:\nDocBuilder Version Linter Rule Changes 1.0 - 1.5 Basic filename and frontmatter rules 1.6+ Enhanced frontmatter schema validation 2.0+ Asset transformation and link validation Future Custom Hugo module support detection Version compatibility approach:\nLinter reports its “target DocBuilder version” Warns if linting against much older/newer DocBuilder behavior Can optionally validate against multiple versions 4. Feature Detection\nWhen DocBuilder adds new features, update linter rules accordingly:\nDocBuilder Feature Linting Rule Update New frontmatter field support Add validation for new fields Asset transformation (WebP) Allow new file extensions Custom shortcodes Validate shortcode syntax Multi-language support Validate language-specific paths Repository metadata injection Validate editURL patterns 5. Documentation Cross-References\nMaintain bidirectional links between linter rules and DocBuilder documentation:\n1 2 3 4 5 6 7 8 \u003c!-- In lint error message --\u003e ERROR: Invalid filename contains uppercase letters Learn more: https://docs.docbuilder.io/reference/filename-conventions \u003c!-- In DocBuilder docs --\u003e ## Filename Conventions DocBuilder expects lowercase, hyphenated filenames. Use `docbuilder lint` to validate: https://docs.docbuilder.io/how-to/setup-linting 6. Automated Sync Checks\nAdd CI checks to prevent drift:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 name: Lint Rules Sync Check on: [pull_request] jobs: verify-sync: runs-on: ubuntu-latest steps: - name: Run sync tests run: go test ./test/integration/lint_docbuilder_sync_test.go -v - name: Check for new DocBuilder features run: | # Parse recent commits for new features # Check if corresponding lint rules exist ./scripts/check-lint-coverage.sh 7. Maintenance Workflow\nWhen DocBuilder changes:\nFeature Addition:\nUpdate linter to recognize new valid patterns Add test cases for new feature Update docs/reference/lint-rules.md Deprecation:\nLinter warns about deprecated patterns Provide migration suggestions Eventually promote warnings to errors Bug Fixes:\nIf DocBuilder now accepts something it previously rejected, update linter Add regression test Update golden test files 8. Rule Evolution Process\nDocBuilder Change → Update Lint Rule → Add Tests → Update Docs → Release Notes ↓ ↓ ↓ ↓ ↓ feat: support Add .webp to Golden test Update rule v1.7.0: WebP images allowed assets for WebP reference WebP support 9. Feedback Loop\nMonitor false positives/negatives:\nTrack GitHub issues tagged linter-false-positive or linter-missed-issue Periodic review of linter vs actual build failures User feedback in success metrics (see Success Metrics section) 10. Living Documentation\nMaintain a changelog specifically for linting rules:\n1 2 3 4 5 6 7 ## v1.7.0 (2026-01-15) - Added: Support for .webp images (DocBuilder v1.7.0) - Changed: .drawio.png now explicitly whitelisted (was implicitly allowed) - Fixed: False positive on _index.md in root directory ## v1.6.0 (2025-12-29) - Initial release: Filename and frontmatter validation Practical Example: Adding Asset Transformation Support Scenario: DocBuilder adds support for automatic WebP image conversion.\nSynchronization steps:\nDetect the change: PR adds WebP transformation to internal/docs/assets.go\nUpdate linter rules:\n1 2 3 4 5 6 7 8 9 10 11 12 13 // internal/lint/asset_rules.go func (l *Linter) validateAssetFile(file string) []Issue { ext := filepath.Ext(file) allowedExts := []string{\".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".webp\"} if !contains(allowedExts, ext) { return []Issue{{ Severity: Error, Message: fmt.Sprintf(\"Unsupported image format: %s\", ext), }} } return nil } Add tests: 1 2 3 4 5 func TestWebPAssetValidation(t *testing.T) { linter := NewLinter() result := linter.LintFile(\"diagram.webp\") require.False(t, result.HasErrors(), \"WebP should be allowed\") } Update documentation:\ndocs/reference/lint-rules.md: Add WebP to allowed asset formats Update asset rules table with WebP examples Release together: Linter v1.8.0 released alongside DocBuilder v1.8.0\nOwnership and Responsibility Core team: Maintains synchronization, reviews PRs for drift Feature developers: Update linter rules when adding DocBuilder features PR checklist: “Have you updated linting rules if applicable?” Quarterly review: Check for accumulated drift, plan alignment work Consequences Positive Early feedback: Developers catch issues before commit Consistent quality: Opinionated rules enforce best practices Better documentation: Improved structure and linking Faster CI: Fewer build failures from preventable issues Self-documenting: Error messages teach Hugo conventions Safe automation: --fix flag reduces manual renaming work Zero configuration: Intelligent defaults work out of the box (auto-detects docs/) Negative Initial friction: Existing repositories may have many violations Migration effort: Teams must fix legacy documentation Learning curve: Developers learn new rules Hook conflicts: May conflict with other pre-commit tools Mitigation Gradual rollout: Start with warnings, move to errors over time Migration guide: Document bulk-fixing existing repositories Rule documentation: Comprehensive explanation of each rule Opt-in initially: Teams adopt voluntarily before enforcement Migration Path Week 1: Soft Launch Release docbuilder lint command (warnings only) Documentation in docs/how-to/ Encourage voluntary adoption Week 2: Team Testing Select 2-3 pilot repositories Run docbuilder lint --fix to clean up Gather feedback on rules and messages Week 3: Git Hooks Publish traditional hook installer Add lefthook.yml to repository template Provide team-wide installation guide (both options) Keep as warnings (non-blocking) Month 2: CI Integration Add CI workflow to template repositories Start blocking PRs with errors (not warnings) Monitor false positives, adjust rules if needed Month 3: Full Enforcement All repositories have lint checks Warnings promoted to errors where appropriate Legacy repositories cleaned up or exempted Future Enhancements Content Linting (Phase 5) Spell checking (en-US by default, configurable) Markdown style consistency (headings, lists, code blocks) Accessibility checks (alt text, heading hierarchy) SEO recommendations (meta descriptions, keywords) Advanced Asset Handling Accessibility score for images (alt text quality) IDE Integration VS Code extension for real-time linting Language server protocol (LSP) for any editor Inline quick-fixes and refactorings Smart Fixes Automatic frontmatter generation from content Link suggestion for orphaned sections Batch rename with preview Examples Example 1: Clean Repository (Using Defaults) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ docbuilder lint Detected documentation directory: docs/ Linting documentation in: ./docs ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ ✓ docs/_index.md ✓ docs/getting-started.md ✓ docs/api/authentication.md ✓ docs/api/_index.md ✓ docs/images/architecture-diagram.drawio.png (whitelisted double extension) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Results: 5 files scanned 0 errors 0 warnings ✨ All documentation passes linting! Exit code: 0 Example 1b: No docs/ Directory Found 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ docbuilder lint No documentation directory detected (checked: docs/, documentation/) Falling back to current directory: . Linting documentation in: . ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ ✓ README.md ✓ CONTRIBUTING.md ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Results: 2 files scanned 0 errors 0 warnings Exit code: 0 $ docbuilder lint ./custom-docs Example 2: Filename Issues 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 $ docbuilder lint ./docs Linting documentation in: ./docs ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ ✗ docs/API Guide.md ERROR: Invalid filename └─ Contains uppercase letters (A, P, I, G) and space character Current: docs/API Guide.md Suggested: docs/api-guide.md Why this matters: • Spaces become %20 in URLs: /docs/API%20Guide/ • Mixed case causes issues on case-sensitive systems • Hugo expects lowercase, hyphenated slugs How to fix: 1. Manual: mv \"docs/API Guide.md\" docs/api-guide.md 2. Automatic: docbuilder lint --fix docs/ If this file is linked from other docs, those links will be automatically updated when using --fix. ✗ docs/screenshots/Login Screen.png ERROR: Invalid filename └─ Contains uppercase letters and space character Current: docs/screenshots/Login Screen.png Suggested: docs/screenshots/login-screen.png Asset files follow the same rules as markdown files. Image references in markdown will be updated automatically. ✗ docs/architecture.md.backup ERROR: Invalid double extension └─ Contains non-whitelisted double extension .md.backup Current: docs/architecture.md.backup Issue: Hugo will attempt to process this as markdown Whitelisted double extensions (allowed): • .drawio.png (Draw.io embedded PNG diagrams) • .drawio.svg (Draw.io embedded SVG diagrams) How to fix: 1. Remove backup files from docs directory 2. Use .git history or separate backup location 3. Add to .gitignore: *.backup ✓ docs/diagrams/system-flow.drawio.svg INFO: Whitelisted double extension └─ .drawio.svg is explicitly allowed for embedded diagrams ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Results: 4 files scanned 3 errors (blocks build) 0 warnings 1 info (explicitly allowed) ❌ Documentation has errors that will prevent Hugo build. Run: docbuilder lint --fix docs/ Exit code: 2 Example 3: Auto-Fix Dry Run 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ docbuilder lint --fix --dry-run ./docs Linting documentation in: ./docs (dry-run mode) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ The following changes would be made: FILE RENAMES: docs/API Guide.md → docs/api-guide.md docs/screenshots/Login Screen.png → docs/screenshots/login-screen.png CONTENT UPDATES: docs/usage.md Line 15: ![Screenshot](/docs/adr/screenshots/login screen.png) → ![Screenshot](/docs/adr/screenshots/login-screen.png) docs/index.md Line 8: [API Guide](./API Guide.md) → [API Guide](/docs/adr/api-guide) GIT OPERATIONS: git mv \"docs/API Guide.md\" \"docs/api-guide.md\" git mv \"docs/screenshots/Login Screen.png\" \"docs/screenshots/login-screen.png\" git add docs/usage.md docs/index.md ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Summary: 2 files would be renamed 2 files would have content updated 4 git operations would be performed To apply these changes: docbuilder lint --fix ./docs Exit code: 0 Example 4: Lefthook Integration 1 2 3 4 5 6 7 pre-commit: parallel: true commands: lint-docs: glob: \\\"*.{md,markdown,png,jpg,jpeg,gif,svg}\\\" run: docbuilder lint {staged_files} --quiet fail_text: \\\"Documentation linting failed. Run 'docbuilder lint --fix' to auto-fix.\\\" Usage:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ git add docs/API\\ Guide.md docs/getting-started.md $ git commit -m \"Add API documentation\" Lefthook \u003e pre-commit \u003e lint-docs: Linting 2 staged files... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ ✗ docs/API Guide.md ERROR: Invalid filename (contains spaces and uppercase) Suggested: docs/api-guide.md ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Results: 1 error, 1 passed Documentation linting failed. Run 'docbuilder lint --fix' to auto-fix. $ docbuilder lint --fix docs/ Fixed: docs/API Guide.md → docs/api-guide.md Updated 2 references in other files $ git add -A $ git commit -m \"Add API documentation\" Lefthook \u003e pre-commit \u003e lint-docs: ✓ All staged files pass linting [main a1b2c3d] Add API documentation 1 file changed, 50 insertions(+) create mode 100644 docs/api-guide.md References Hugo URL Management Hugo Content Organization Git Pre-Commit Hooks Lefthook Documentation Markdown Best Practices ADR-001: Golden Testing Strategy (for test approach) ADR-000: Uniform Error Handling (for error reporting) Implementation Checklist Phase 1: Core Linting Engine ✅ Create internal/lint package (5 files: types, rules, linter, formatter, tests) Implement filename rules with whitelisted extensions (.drawio.png, .drawio.svg) Human-readable text formatter with colorization and NO_COLOR support JSON formatter for CI/CD integration Unit tests for each rule (11 comprehensive test cases) Standard file filtering (README, CONTRIBUTING, CHANGELOG, etc.) Intelligent default path detection (docs/, documentation/, fallback to .) Phase 2: CLI Implementation ✅ Add docbuilder lint CLI command with Kong integration Exit code handling (0=clean, 1=warnings, 2=errors, 3=execution error) Output format flags: --format=text|json Verbosity control: --quiet, --verbose Color detection with NO_COLOR environment variable Duplicate error prevention (consolidated uppercase/special char reporting) Phase 3: Auto-Fix Capability (Link Resolution) Comprehensive link resolution strategy documented in ADR Phase 3a: Basic file renaming with git mv support File rename implementation Git mv integration for history preservation Dry-run mode (--fix --dry-run) Force flag for overwriting existing files Comprehensive test coverage (8 tests) Interactive confirmation prompts (deferred to Phase 3e) Detailed preview of changes (deferred to Phase 3e) Phase 3b: Link discovery and path resolution Regex patterns for inline, reference, image links Relative path resolution to absolute workspace paths Link reference tracking (source file, line number, type) External URL detection and exclusion Code block detection and exclusion Anchor fragment preservation Comprehensive test coverage (13 tests, 379 lines) Phase 3c: Link updates with atomic operations Generate replacement text preserving style Atomic file updates with rollback on failure Preserve anchor fragments (#section) in updated links Test coverage for anchor fragment preservation Test coverage for rollback mechanism Phase 3d: Edge case handling Skip external URLs (protocol detection) - already implemented in Phase 3b Ignore links in code blocks (markdown parser) - already implemented in Phase 3b Case-insensitive filesystem support Broken link detection and reporting Phase 3e: Reporting and interactive confirmation Detailed fix report with statistics Interactive confirmation showing files + links affected Dry-run preview with before/after comparison Backup creation (.docbuilder-backup-{timestamp}/) Phase 4: Git Hooks Integration Traditional pre-commit hook script (scripts/install-hooks.sh) Hook installer command: docbuilder lint install-hook Lefthook configuration example (lefthook.yml) Test with staged files workflow Phase 5: CI/CD Integration GitHub Actions workflow example (.github/workflows/lint-docs.yml) GitLab CI template (.gitlab-ci.yml) JSON output schema documentation PR comment integration examples Testing Integration tests with golden files for core lint functionality Valid scenarios: correct filenames, whitelisted extensions Invalid scenarios: mixed-case, spaces, special chars, double extensions Golden file generation with -update-golden flag Normalized path comparison for system-independence 6 comprehensive test cases covering all current rules Integration golden tests for auto-fix functionality (Phase 3) Before/after directory structures with realistic test data TestGoldenAutoFix_FileRenameWithLinkUpdates: Complete fix workflow TestGoldenAutoFix_DryRun: Dry-run mode output verification TestGoldenAutoFix_BrokenLinkDetection: Broken link reporting Sorted results for consistent comparison across runs Normalized paths (filenames only) for portability Integration tests for lint-DocBuilder sync TestLintDocBuilderSync: Full build pipeline → lint validation TestLintDocBuilderSync_FileNaming: Filename convention compliance Test repository with cross-reference links (./relative-link.md syntax) Link transformation bug fixes (strip ./ prefix in transform_links.go) Linter path resolution enhancements (Hugo site-absolute paths in fixer.go) CI workflow to detect rule drift GitHub Actions workflow: .github/workflows/detect-rule-drift.yml Weekly schedule (Sunday midnight) + manual dispatch Single theme testing (Relearn only) Artifact uploads (90-day retention) PR comment integration on drift detection Documentation docs/how-to/setup-linting.md - Setup and usage guide (completed) docs/reference/lint-rules.md - Complete rule reference (completed) docs/reference/lint-rules-changelog.md - Rule version history (completed) docs/how-to/migrate-to-linting.md - Migration guide for existing repositories (completed) docs/how-to/ci-cd-linting.md - CI/CD integration examples (completed) Future Enhancements VS Code extension for real-time linting Content linting rules (spell checking, style consistency) Advanced asset handling (accessibility checks) Success Metrics After 3 months of deployment:\n90%+ of commits pass linting without errors 50% reduction in documentation-related CI failures Positive developer feedback (survey) \u003c5% false positive rate on errors Active usage of --fix flag (telemetry) Decision Owner: [To be assigned]\nStakeholders: Development team, documentation maintainers, DevOps\nReview Date: 3 months after implementation",
    "description": "Date: 2025-12-29\nStatus Proposed\nContext DocBuilder processes documentation from multiple Git repositories, transforming markdown files into Hugo-compatible sites. Currently, developers discover issues only after committing and running builds:\nCurrent Pain Points Late feedback: Filename issues (spaces, mixed-case) discovered during Hugo build Silent failures: Invalid frontmatter causes pages to render incorrectly or not at all Broken links: Cross-references break when files are renamed without updating links Path inconsistencies: Mixed naming conventions (README.md, api-guide.md, My Document.md) in same repository Asset orphans: Images referenced but not committed, or committed but never referenced Hugo quirks: Reserved filenames (_index.md vs index.md) behave differently but look similar Impact Developers commit documentation that fails to build CI/CD pipelines fail unexpectedly Manual inspection required to diagnose issues Inconsistent documentation quality across repositories Time wasted on avoidable build failures Hugo and DocBuilder Best Practices Hugo has specific expectations:",
    "tags": [
      "Linting",
      "Validation",
      "Documentation",
      "Developer-Experience"
    ],
    "title": "ADR-005: Documentation Linting for Pre-Commit Validation",
    "uri": "/docs/adr/adr-005-documentation-linting/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Documentation for adr",
    "description": "Documentation for adr",
    "tags": [],
    "title": "adr",
    "uri": "/docs/adr/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Documentation for debugging",
    "description": "Documentation for debugging",
    "tags": [],
    "title": "debugging",
    "uri": "/docs/debugging/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Documentation for how-to",
    "description": "Documentation for how-to",
    "tags": [],
    "title": "how-to",
    "uri": "/docs/how-to/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "Index File Handling DocBuilder automatically generates index pages for repositories and sections, but also respects user-provided index files. This document explains how index files are discovered, processed, and what takes precedence when multiple options exist.\nOverview DocBuilder generates Hugo _index.md files at three levels:\nSite Level: Main landing page (content/_index.md) Repository Level: Repository overview pages (content/{repository}/_index.md) Section Level: Section overview pages (content/{repository}/{section}/_index.md) Users can provide their own index content to replace auto-generated indexes.\nTransform Pipeline Processing All index files are processed through DocBuilder’s fixed transform pipeline, which applies transformations in a specific order:\nnormalizeIndexFiles - Automatically converts README.md → _index.md for Hugo compatibility buildBaseFrontMatter - Adds default metadata (title, type, date, repository) extractIndexTitle - Extracts H1 heading as title for index pages rewriteRelativeLinks - Fixes markdown links to work in Hugo (.md → /, directory-style) rewriteImageLinks - Corrects image paths relative to content root addRepositoryMetadata - Injects repository/forge/commit information addEditLink - Generates edit URLs for source links This ensures all index files (whether user-provided or auto-generated) receive consistent processing and link rewriting.\nFor more details on the transform pipeline, see Content Transform Pipeline and ADR-003: Fixed Transform Pipeline.\nRepository-Level Indexes At the repository level (directly under the configured paths), DocBuilder supports three scenarios:\nCase 1: README.md Only When a repository contains only a README.md file at the root of the docs path:\nyour-repo/ docs/ README.md ← Automatically converted to _index.md by pipeline guide.md api/ reference.md Behavior: README.md is automatically normalized to _index.md by the normalizeIndexFiles transform. The transform pipeline then processes it like any other document (link rewriting, metadata injection, etc.).\nCase 2: No User-Provided Index When neither README.md nor index.md exists:\nyour-repo/ docs/ guide.md api/ reference.md Behavior: DocBuilder auto-generates a repository index listing sections and documents.\nCase 3: index.md Only When a repository contains an index.md file:\nyour-repo/ docs/ index.md ← Automatically converted to _index.md by pipeline guide.md api/ reference.md Behavior: index.md is automatically normalized to _index.md by the normalizeIndexFiles transform, just like README.md files.\nCase 4: Both index.md and README.md When both files exist at the repository root:\nyour-repo/ docs/ README.md ← Processed as regular document, accessible at /repo/readme/ index.md ← Normalized to _index.md (takes precedence) guide.md Behavior:\nBoth files are discovered and processed through the transform pipeline index.md is normalized to _index.md first (takes precedence for the repository index) README.md is also normalized to _index.md but is renamed to prevent collision The original README.md position in the pipeline ensures it’s accessible at /repo/readme/ Precedence Order: index.md \u003e README.md \u003e auto-generated\nImplementation Note: This precedence is handled during the generation phase, before transforms run. The pipeline only receives files that should exist in the final site.\nSection-Level Indexes Within sections (subdirectories), README.md or index.md files are recognized as section indexes:\nyour-repo/ docs/ api/ index.md ← Normalized to _index.md by pipeline endpoint-a.md endpoint-b.md guides/ README.md ← Also normalized to _index.md tutorial.md Behavior: Both README.md and index.md in a section directory are normalized to _index.md by the transform pipeline. If both exist in the same section, index.md takes precedence (same as repository-level).\nIf no user-provided index exists, the generation phase creates a section index listing all documents in that section. Generated indexes are then processed through the same transform pipeline.\nFile Naming Conventions Important notes about file naming:\nCase-Insensitive: README.md, readme.md, Readme.md are all treated the same Lowercase URLs: Files are converted to lowercase for URLs (README.md → /repo/readme/) Automatic Normalization: Both README.md and index.md are automatically converted to _index.md by the normalizeIndexFiles transform early in the pipeline Hugo Compatibility: The _index.md naming is required by Hugo for section/repository index pages Configuration Impact Forge-Discovered Repositories Repositories discovered via forge auto-discovery (GitLab, GitHub, Forgejo) default to using [\"docs\"] as their documentation paths:\n1 2 3 4 5 6 7 8 forges: - name: \"gitlab\" type: \"gitlab\" api_url: \"https://gitlab.example.com/api/v4\" auto_discover: true filtering: required_paths: [\"docs\"] # Only repos with \"docs\" folder are included For these repositories, place your index.md or README.md at:\ndocs/index.md (repository index) docs/section-name/index.md (section index) Explicitly Configured Repositories For explicitly configured repositories, you control the documentation paths:\n1 2 3 4 repositories: - url: \"https://github.com/example/repo.git\" name: \"my-repo\" paths: [\"documentation\", \"guides\"] Index files should be placed at the root of each configured path:\ndocumentation/index.md guides/index.md Front Matter in User-Provided Indexes User-provided index files can include Hugo front matter:\n1 2 3 4 5 6 7 8 9 --- title: \"Custom Repository Title\" description: \"A detailed description\" weight: 10 --- # Welcome to My Repository Custom content here... If no front matter is present, the buildBaseFrontMatter transform adds it automatically:\n1 2 3 4 title: Repository Name # Extracted from filename or H1 repository: repository-name # Source repository type: docs # Hugo content type date: 2025-12-12T15:30:00Z # Commit date or fixed epoch Additional metadata is injected by later transforms:\neditURL - Added by addEditLink transform if repository forge is configured Repository/commit info - Added by addRepositoryMetadata transform User-provided front matter takes precedence and is merged with auto-generated fields.\nIgnored Files The following files are filtered during discovery and never enter the processing pipeline:\nCONTRIBUTING.md CHANGELOG.md LICENSE.md CODE_OF_CONDUCT.md .github/ directory contents These files are typically repository-level documentation not relevant to the generated docs site and are excluded at discovery time regardless of location (root or subdirectories).\nImportant: README.md is not ignored. It can be used as a repository/section index (automatically normalized to _index.md) or as a regular document depending on the presence of index.md files.\nBest Practices Use index.md for Docs Sites: If you’re building a dedicated documentation site, use index.md for repository and section indexes. Reserve README.md for GitHub/GitLab repository overview.\nKeep README.md for Dual Purpose: If your repository README is also suitable as docs landing page, use README.md and skip index.md. Both are normalized to _index.md by the pipeline.\nSection Organization: Provide README.md or index.md files in section directories to give users context about what’s in that section. Missing indexes are auto-generated but lack custom content.\nFront Matter: Add proper front matter to control titles, descriptions, and ordering in navigation. The pipeline merges user front matter with auto-generated fields.\nRelative Links Work: Both user-provided and auto-generated indexes receive link rewriting, so you can use relative markdown links ([Guide](/docs/reference/guide)) and they’ll be converted to Hugo-compatible URLs automatically.\nExamples Example 1: Technical Documentation with Separate README my-project/ README.md ← GitHub repository overview docs/ index.md ← Docs landing page getting-started.md api/ index.md ← API section overview rest.md graphql.md Result:\nSite uses docs/index.md as repository index GitHub shows root README.md Root README.md not included in docs site Example 2: Simple Project with README as Docs my-tool/ docs/ README.md ← Repository and docs landing page installation.md usage.md Result:\ndocs/README.md becomes repository index Simple structure for small projects Example 3: Multi-Language Documentation my-app/ docs/ index.md ← Main docs index en/ index.md ← English section index guide.md fr/ index.md ← French section index guide.md Result:\nClear index pages at each level Language sections well-organized Troubleshooting Q: My index.md isn’t being used as the repository index\nA: Ensure:\nFile is at the root of your configured paths directory File is named exactly index.md or README.md (case-insensitive) File has .md or .markdown extension Repository has been rebuilt (daemon mode requires rebuild trigger) Check verbose logs for “normalizeIndexFiles” transform output Q: Links in my index files aren’t working\nA: Index files go through the same link rewriting transforms as regular documents. Use standard markdown links:\nRelative links: [Guide](/docs/reference/guide) → /repo/guide/ Section links: [API](/docs/reference/api/overview) → /repo/api/overview/ The rewriteRelativeLinks transform handles conversion automatically Q: README.md is missing from my site when I have both README.md and index.md\nA: This is expected behavior. When index.md exists at the same level, it takes precedence during the generation phase. README.md may still be accessible as a regular document at /repository/readme/ depending on how the precedence was resolved.\nQ: My section index.md isn’t showing up\nA: Check that:\nFile is in a subdirectory (section), not at repository root Section is not empty (contains other .md files) File is being discovered (check verbose logs for “Discovered file”) Transform pipeline logs show normalization: “normalizeIndexFiles: README → _index” Q: Front matter from my index file is being overwritten\nA: User-provided front matter is merged with auto-generated fields, not replaced. If you see unexpected values:\nCheck that your front matter is valid YAML Ensure the front matter fence starts at line 1 (no content before ---) User values take precedence over pipeline defaults Some fields (like editURL) are added by transforms after user front matter is parsed",
    "description": "Index File Handling DocBuilder automatically generates index pages for repositories and sections, but also respects user-provided index files. This document explains how index files are discovered, processed, and what takes precedence when multiple options exist.\nOverview DocBuilder generates Hugo _index.md files at three levels:",
    "tags": [],
    "title": "index-files",
    "uri": "/docs/reference/index-files/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e debugging",
    "content": "ISSUE-001: “Docs” Appearing as Navigation Name Instead of Repository Names Status: ✅ RESOLVED\nPriority: High\nCreated: 2025-12-19\nLast Updated: 2025-12-19\nResolved: 2025-12-19 Additional Fix: 2025-12-19 (nested docs directories)\nProblem Statement The Hugo navigation sidebar shows “Docs” as the section name instead of the repository name. This creates confusing, nested “Docs \u003e Docs \u003e Docs” hierarchies instead of meaningful navigation like “Repository-Name \u003e Documentation”.\nVisual Evidence Screenshot shows navigation structure:\n\u003e 555 Variant Interpretations Human Varde Data Structures Library Documentation \u003e Docs \u003e Docs \u003e Docs ARCHITECTURE Docs Expected structure:\n\u003e RepositoryName ARCHITECTURE [other docs] Root Cause Analysis (CONFIRMED) The Problem When a repository has a docs/ directory configured as the documentation path, the content structure becomes:\ncontent/ repository-name/ docs/ _index.md ← title: \"Docs\" (WRONG - should use repo name) file1.md file2.md This causes Hugo navigation to show:\nDocs (the docs folder becomes a section) Docs (nested somehow by Hugo) ARCHITECTURE Root Cause File: internal/hugo/pipeline/generators.go:generateSectionIndex()\nLines 135-142:\n1 2 3 4 5 6 // Generate section index // If section is a docs base directory (e.g., \"docs\"), use repository name as title title := titleCase(filepath.Base(sectionName)) if isDocsBaseSection(sectionName, docs) { title = titleCase(repo) } The logic has a isDocsBaseSection() check that SHOULD use the repo name when the section is “docs”, BUT there’s a problem with how it works.\nThe Bug: isDocsBaseSection() checks if all documents have DocsBase matching the section name. But when docs are in subdirectories like docs/architecture/file.md, the section is \"docs\" but the docsBase might be different paths, causing the check to fail.\nExample Failure Case Repository: franklin-hardpanel-mapper\nConfigured paths: [\"docs\"]\nFiles discovered:\ndocs/architecture.md → section: “docs”, DocsBase: “docs” ✓ docs/api/endpoint.md → section: “docs/api”, DocsBase: “docs” ✗ When generating section index for “docs”:\nisDocsBaseSection(\"docs\", docs) checks all docs Some docs have section “docs/api” not “docs” Check fails → uses titleCase(\"docs\") → “Docs” instead of repo name Investigation Plan Phase 1: Trace Current Behavior ✅ COMPLETED Add debug logging to generateSectionIndex() to see what title is being used Check what repoInfo.Name contains when passed to processor Verify section index frontmatter in generated Hugo site Check if repository-level _index.md is being created vs directory-level ones Findings: Section indexes were using titleCase(filepath.Base(sectionName)) which converted “docs” → “Docs”. The isDocsBaseSection() check was failing for repositories with nested docs subdirectories.\nPhase 2: Identify Title Source ✅ COMPLETED Find where section index title is determined Check if it’s using directory name vs repository name Verify Document.RepositoryName is populated correctly Check if path parsing is extracting correct segment Findings: Title determined in generators.go:generateSectionIndex() at line 138. Was using titleCase(repo) instead of repoMeta.Name, and isDocsBaseSection() logic was flawed for nested directories.\nPhase 3: Fix Implementation ✅ COMPLETED Ensure repository-level _index.md uses repo.Name as title Ensure subdirectory _index.md files use meaningful names (not “docs”) Add test case for repository section title generation Update golden tests if output format changes Actions Taken:\nAdded DocsPaths field to RepositoryInfo Replaced isDocsBaseSection() with isConfiguredDocsPath() Changed to use repoMeta.Name (preserves original capitalization) No golden test updates needed - existing tests validated the fix Phase 4: Verification ✅ COMPLETED Run local build and inspect generated _index.md files Check Hugo navigation in browser Run golden tests to ensure no regressions Document fix in this file Results:\nManual test: franklin-hardpanel-mapper/docs/_index.md → title: “Franklin Hardpanel Mapper” ✅ All 16 golden integration tests pass ✅ Full test suite passes (44 packages) ✅ Linter clean (0 issues) ✅ Code Locations Primary Files to Check internal/hugo/pipeline/transform_section_indexes.go\nRun() method - orchestrates section index generation generateSectionIndex() - creates individual _index.md files Title assignment logic internal/hugo/pipeline/document.go\nDocument.RepositoryName field RepositoryInfo structure internal/hugo/pipeline/processor.go\nHow RepositoryInfo is passed to transforms Context available to section index generator internal/hugo/content_copy_pipeline.go\nbuildRepositoryMetadata() - where repo.Name should be set Test Files test/integration/golden_test.go\nTestGolden_TwoRepos - multi-repo navigation TestGolden_SectionIndexes - section index generation test/testdata/golden/*/content-structure.golden.json\nExpected frontmatter for _index.md files Previous Fix Attempts (Document History) This Was The First Documented Attempt The issue was tracked and resolved systematically on 2025-12-19.\nPrevious undocumented attempts (inferred from code history):\nThe isDocsBaseSection() function existed, suggesting at least one prior attempt to detect and handle docs directories The logic checked all documents’ DocsBase field, which was too fragile for nested directory structures No documentation existed tracking why this approach was chosen or why it failed in production Solution Strategy The Fix (IMPLEMENTED APPROACH) Root Cause: The isDocsBaseSection() function fails when documents are nested in subdirectories under “docs”.\nSolution: Simplify the logic - ANY section that is a top-level documentation directory (matching the configured paths) should use the repository name as the title.\nImplementation:\nCheck if sectionName is in the repository’s configured paths (e.g., “docs”, “documentation”) If yes → use repository name as title If no → use humanized directory name as title Code Change Location: internal/hugo/pipeline/generators.go:generateSectionIndex()\nBefore:\n1 2 3 4 title := titleCase(filepath.Base(sectionName)) if isDocsBaseSection(sectionName, docs) { title = titleCase(repo) } After:\n1 2 3 4 title := titleCase(filepath.Base(sectionName)) if isConfiguredDocsPath(sectionName, repoMeta.DocsPaths) { title = repoMeta.Name // Use actual repository name, not directory name } New helper function:\n1 2 3 4 5 6 7 8 9 // isConfiguredDocsPath checks if the section matches a configured documentation path func isConfiguredDocsPath(sectionName string, docsPaths []string) bool { for _, path := range docsPaths { if sectionName == path || strings.HasPrefix(sectionName, path+\"/\") { return true } } return false } Why This Works Clear Identification: We know which directories are “docs roots” from configuration No Ambiguity: No need to inspect all documents’ metadata Correct Titles: Top-level docs directory gets repository name Nested Dirs: Subdirectories under docs/ get their own meaningful names Example Results Repository: franklin-hardpanel-mapper\nConfigured paths: [\"docs\"]\nGenerated sections:\ncontent/franklin-hardpanel-mapper/docs/_index.md → title: “Franklin Hardpanel Mapper” ✅ content/franklin-hardpanel-mapper/docs/api/_index.md → title: “API” ✅ content/franklin-hardpanel-mapper/docs/architecture/_index.md → title: “Architecture” ✅ Navigation shows:\n\u003e Franklin Hardpanel Mapper API Architecture NOT:\n\u003e Docs \u003e Docs API Architecture Resolution Summary Date: 2025-12-19\nImplementation (Initial Fix) Modified 3 files:\ninternal/hugo/pipeline/document.go\nAdded DocsPaths []string field to RepositoryInfo Stores all configured documentation paths from repository config internal/hugo/content_copy_pipeline.go\nPopulated DocsPaths with repo.Paths from configuration Default to []string{\"docs\"} when no paths configured internal/hugo/pipeline/generators.go\nReplaced isDocsBaseSection() with isConfiguredDocsPath() New function checks if section exactly matches a configured docs path When match found, uses repoMeta.Name (repository name) instead of titleCase(repo) or directory name Additional Fix (Nested Docs Directories) Problem Discovered: Repositories with nested directories matching the docs path name (e.g., docs/docs/docs/) still showed “Docs” in navigation.\nRoot Cause: The initial isConfiguredDocsPath() only checked for exact section match (e.g., “docs”), but didn’t handle cases where the section was “docs/docs” or “docs/docs/docs”.\nSolution: Extended isConfiguredDocsPath() to also check if the last segment of the section path matches a configured docs path name.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func isConfiguredDocsPath(sectionName string, docsPaths []string) bool { lastSegment := filepath.Base(sectionName) for _, path := range docsPaths { // Exact match (e.g., section \"docs\" matches path \"docs\") if sectionName == path { return true } // Last segment matches (e.g., section \"docs/docs\" has last segment \"docs\") if lastSegment == filepath.Base(path) { return true } } return false } Result: Any section ending with a docs path name uses the repository name as title.\nExamples:\nSection docs → title: “Repository Name” ✅ Section docs/docs → title: “Repository Name” ✅ Section docs/docs/docs → title: “Repository Name” ✅ Section docs/api → title: “API” ✅ Test Results ✅ All 16 golden tests pass ✅ Full test suite passes (44 packages) ✅ Linter clean (0 issues) ✅ Manual verification: franklin-hardpanel-mapper/docs/ → title “Franklin Hardpanel Mapper” ✅ Nested docs test: docs/docs/docs/architecture.md → all intermediate indexes use repository name Before vs After Before Fix:\n\u003e Docs ← docs directory name \u003e Docs ← nested somehow ARCHITECTURE API After Fix:\n\u003e Franklin Hardpanel Mapper ← repository name ARCHITECTURE API Why The Old Code Failed The previous isDocsBaseSection() function checked if all documents in a section had DocsBase matching the section name. This failed when:\nDocuments were in subdirectories (e.g., docs/architecture/file.md) Some sections were docs while others were docs/api The check required ALL docs to match, causing it to fail and fall back to using directory name “Docs” Why The New Code Works The new isConfiguredDocsPath() function:\nDirectly checks if section matches a configured path from repositories[].paths No need to inspect document metadata Simple, reliable, and performant Uses actual repository name from repoMeta.Name (preserves capitalization and formatting) Testing Checklist Single repository builds correctly Multiple repositories each show distinct names Nested directories show meaningful names “docs” directory doesn’t appear in navigation Golden tests pass Manual browser testing confirms navigation Existing Test Coverage TestGolden_SectionIndexes provides coverage for this issue:\nTest File: test/integration/golden_test.go (lines 170-219) Config: test/testdata/configs/section-indexes.yaml Repository: test/testdata/repos/transforms/section-indexes/ Verification: test/testdata/golden/section-indexes/content-structure.golden.json What It Tests:\nRepository named section-docs with paths: [\"docs\"] Files located in docs/getting-started/ and docs/advanced/ Generated content/section-docs/_index.md has title: \"Section Docs\" (repository name) NOT title: \"Docs\" (directory name) ✅ Golden File Verification:\n1 2 3 4 5 6 7 \"content/section-docs/_index.md\": { \"frontmatter\": { \"repository\": \"section-docs\", \"title\": \"Section Docs\", // ← Repository name, not \"Docs\" \"type\": \"docs\" } } This test verifies the fix is working correctly. The repository name is used as the title for the docs section, not the directory name.\nAdditional Test Coverage TestGolden_TwoRepos also indirectly tests this:\nMulti-repository build with separate navigation sections Each repository should show its own name, not “Docs” Verifies repository isolation and distinct naming Success Criteria Navigation shows repository names at top level No “Docs” repeated entries in navigation Each repository’s documentation is clearly separated Subdirectories use meaningful names All tests pass Notes This issue affects user experience significantly Navigation is critical for multi-repo documentation sites Solution must be robust and tested thoroughly ✅ Test coverage provided by TestGolden_SectionIndexes - verifies repository name used instead of “Docs” The golden file section-indexes/content-structure.golden.json ensures regression prevention Related Files internal/hugo/pipeline/transform_section_indexes.go - Section generation internal/hugo/pipeline/document.go - Document metadata internal/hugo/content_copy_pipeline.go - Repository metadata test/integration/golden_test.go - Integration tests",
    "description": "ISSUE-001: “Docs” Appearing as Navigation Name Instead of Repository Names Status: ✅ RESOLVED\nPriority: High\nCreated: 2025-12-19\nLast Updated: 2025-12-19\nResolved: 2025-12-19 Additional Fix: 2025-12-19 (nested docs directories)\nProblem Statement The Hugo navigation sidebar shows “Docs” as the section name instead of the repository name. This creates confusing, nested “Docs \u003e Docs \u003e Docs” hierarchies instead of meaningful navigation like “Repository-Name \u003e Documentation”.",
    "tags": [],
    "title": "issue-001-docs-navigation-naming",
    "uri": "/docs/debugging/issue-001-docs-navigation-naming/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "JSON Output Schema DocBuilder’s linter supports machine-readable JSON output for CI/CD integration, automated reporting, and tooling integration.\nUsage 1 2 3 4 5 6 7 8 # Output JSON to stdout docbuilder lint --format=json # Save to file docbuilder lint --format=json \u003e lint-report.json # Use in CI pipeline docbuilder lint --format=json | jq '.summary.errors' Schema Definition Root Object 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"version\": \"1.0\", \"timestamp\": \"2025-12-29T20:30:00Z\", \"path\": \"docs\", \"auto_detected\": true, \"summary\": { \"total_files\": 38, \"errors\": 2, \"warnings\": 5, \"passed\": 31 }, \"issues\": [ { \"file\": \"docs/API Guide.md\", \"line\": 0, \"column\": 0, \"severity\": \"error\", \"rule\": \"filename-convention\", \"message\": \"Filename contains spaces and uppercase letters\", \"suggestion\": \"docs/api-guide.md\", \"context\": { \"current\": \"API Guide.md\", \"suggested\": \"api-guide.md\", \"reason\": \"Spaces create %20 in URLs; uppercase causes case-sensitivity issues\" } } ], \"broken_links\": [ { \"source_file\": \"docs/getting-started.md\", \"line\": 45, \"link_text\": \"API Reference\", \"target\": \"./api/overview.md\", \"link_type\": \"inline\", \"error\": \"target file does not exist\" } ], \"exit_code\": 2 } Field Descriptions Root Fields Field Type Description version string Schema version (semver format) timestamp string ISO 8601 timestamp of lint execution path string Path that was linted auto_detected boolean Whether the path was auto-detected (vs. explicitly provided) summary object Summary statistics (see below) issues array List of lint issues found (see below) broken_links array List of broken links detected (see below) exit_code integer Exit code: 0 (success), 1 (warnings), 2 (errors) Summary Object Field Type Description total_files integer Total number of files scanned errors integer Number of files with errors (blocks build) warnings integer Number of files with warnings passed integer Number of files that passed all checks Invariant: errors + warnings + passed = total_files\nIssue Object Field Type Required Description file string ✅ Relative path to the file with the issue line integer ✅ Line number (0 for file-level issues) column integer ✅ Column number (0 for file-level issues) severity string ✅ \"error\" or \"warning\" rule string ✅ Rule identifier (e.g., \"filename-convention\") message string ✅ Human-readable error message suggestion string ❌ Suggested fix (if applicable) context object ❌ Additional context about the issue Broken Link Object Field Type Required Description source_file string ✅ File containing the broken link line integer ✅ Line number where link appears link_text string ✅ Display text of the link target string ✅ Target URL/path that is broken link_type string ✅ Type: \"inline\", \"reference\", \"image\" error string ✅ Description of why link is broken Rule Identifiers Current rules that may appear in the rule field:\nRule ID Description Severity filename-convention Filename violates naming conventions error missing-title Frontmatter missing required title field warning broken-links Internal link target does not exist error invalid-extension File extension not in whitelist error Examples Success (No Issues) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"version\": \"1.0\", \"timestamp\": \"2025-12-29T20:30:00Z\", \"path\": \"docs\", \"auto_detected\": true, \"summary\": { \"total_files\": 15, \"errors\": 0, \"warnings\": 0, \"passed\": 15 }, \"issues\": [], \"broken_links\": [], \"exit_code\": 0 } Errors Found 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 { \"version\": \"1.0\", \"timestamp\": \"2025-12-29T20:31:00Z\", \"path\": \"docs\", \"auto_detected\": false, \"summary\": { \"total_files\": 20, \"errors\": 3, \"warnings\": 2, \"passed\": 15 }, \"issues\": [ { \"file\": \"docs/Getting Started.md\", \"line\": 0, \"column\": 0, \"severity\": \"error\", \"rule\": \"filename-convention\", \"message\": \"Filename contains spaces\", \"suggestion\": \"docs/getting-started.md\", \"context\": { \"current\": \"Getting Started.md\", \"suggested\": \"getting-started.md\", \"reason\": \"Spaces create %20 in URLs\" } }, { \"file\": \"docs/api/_index.md\", \"line\": 0, \"column\": 0, \"severity\": \"warning\", \"rule\": \"missing-title\", \"message\": \"Frontmatter missing 'title' field\", \"context\": { \"recommendation\": \"Add title to frontmatter for better navigation\" } } ], \"broken_links\": [ { \"source_file\": \"docs/getting-started.md\", \"line\": 23, \"link_text\": \"API Reference\", \"target\": \"./api/overview.md\", \"link_type\": \"inline\", \"error\": \"target file does not exist\" } ], \"exit_code\": 2 } Parsing Examples Shell (jq) 1 2 3 4 5 6 7 8 9 10 11 # Extract error count ERRORS=$(jq -r '.summary.errors' lint-report.json) # Get all error messages jq -r '.issues[] | select(.severity==\"error\") | .message' lint-report.json # List files with issues jq -r '.issues[].file | unique' lint-report.json # Check if any broken links exist HAS_BROKEN_LINKS=$(jq '.broken_links | length \u003e 0' lint-report.json) Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import json with open('lint-report.json') as f: report = json.load(f) # Check for errors if report['summary']['errors'] \u003e 0: print(f\"❌ {report['summary']['errors']} errors found\") for issue in report['issues']: if issue['severity'] == 'error': print(f\" {issue['file']}:{issue['line']} - {issue['message']}\") exit(1) # Check for broken links if report['broken_links']: print(f\"🔗 {len(report['broken_links'])} broken links found\") for link in report['broken_links']: print(f\" {link['source_file']}:{link['line']} -\u003e {link['target']}\") JavaScript/Node.js 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); // Group issues by severity const errors = report.issues.filter(i =\u003e i.severity === 'error'); const warnings = report.issues.filter(i =\u003e i.severity === 'warning'); console.log(`Errors: ${errors.length}, Warnings: ${warnings.length}`); // Build summary for PR comment let comment = `## Documentation Lint Results\\n\\n`; comment += `📊 ${report.summary.total_files} files scanned\\n\\n`; if (errors.length \u003e 0) { comment += `### ❌ Errors (${errors.length})\\n\\n`; errors.slice(0, 5).forEach(error =\u003e { comment += `- **${error.file}**: ${error.message}\\n`; }); } Go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 package main import ( \"encoding/json\" \"fmt\" \"os\" ) type LintReport struct { Version string `json:\"version\"` Timestamp string `json:\"timestamp\"` Path string `json:\"path\"` AutoDetected bool `json:\"auto_detected\"` Summary Summary `json:\"summary\"` Issues []Issue `json:\"issues\"` BrokenLinks []Link `json:\"broken_links\"` ExitCode int `json:\"exit_code\"` } type Summary struct { TotalFiles int `json:\"total_files\"` Errors int `json:\"errors\"` Warnings int `json:\"warnings\"` Passed int `json:\"passed\"` } type Issue struct { File string `json:\"file\"` Line int `json:\"line\"` Column int `json:\"column\"` Severity string `json:\"severity\"` Rule string `json:\"rule\"` Message string `json:\"message\"` Suggestion string `json:\"suggestion,omitempty\"` Context map[string]interface{} `json:\"context,omitempty\"` } type Link struct { SourceFile string `json:\"source_file\"` Line int `json:\"line\"` LinkText string `json:\"link_text\"` Target string `json:\"target\"` LinkType string `json:\"link_type\"` Error string `json:\"error\"` } func main() { data, _ := os.ReadFile(\"lint-report.json\") var report LintReport json.Unmarshal(data, \u0026report) fmt.Printf(\"Files: %d, Errors: %d, Warnings: %d\\n\", report.Summary.TotalFiles, report.Summary.Errors, report.Summary.Warnings) } Version History v1.0 (Current) Initial JSON schema with:\nBasic issue reporting Broken link detection Summary statistics Exit code reporting Planned Enhancements Future versions may include:\nfixes_available field indicating which issues can be auto-fixed performance metrics (files/second, total time) rules_applied list of rules that were checked JUnit XML output format for better CI integration SARIF (Static Analysis Results Interchange Format) support Integration Tips CI/CD Best Practices Always capture JSON output even on failure:\n1 docbuilder lint --format=json \u003e lint-report.json || true Upload as artifact for debugging:\n1 2 3 4 artifacts: when: always paths: - lint-report.json Parse before failing pipeline to provide better feedback:\n1 2 EXIT_CODE=$(jq -r '.exit_code' lint-report.json) exit $EXIT_CODE Rate-limit PR comments to avoid spam on large changes\nCache results for incremental linting on large repositories\nTooling Integration The JSON output is designed to integrate with:\nCode review tools: GitHub Actions, GitLab CI, BitBucket Pipelines IDE extensions: VS Code, IntelliJ, Vim/NeoVim Quality dashboards: SonarQube, CodeClimate Slack/Discord bots: Automated team notifications Documentation portals: Link to specific issues in docs Schema Stability This schema follows semantic versioning:\nMajor version changes indicate breaking changes Minor version changes add new fields (backward compatible) Patch version changes are documentation/clarification only The version field in the JSON output reflects the schema version, not the DocBuilder version.",
    "description": "JSON Output Schema DocBuilder’s linter supports machine-readable JSON output for CI/CD integration, automated reporting, and tooling integration.\nUsage 1 2 3 4 5 6 7 8 # Output JSON to stdout docbuilder lint --format=json # Save to file docbuilder lint --format=json \u003e lint-report.json # Use in CI pipeline docbuilder lint --format=json | jq '.summary.errors' Schema Definition Root Object 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"version\": \"1.0\", \"timestamp\": \"2025-12-29T20:30:00Z\", \"path\": \"docs\", \"auto_detected\": true, \"summary\": { \"total_files\": 38, \"errors\": 2, \"warnings\": 5, \"passed\": 31 }, \"issues\": [ { \"file\": \"docs/API Guide.md\", \"line\": 0, \"column\": 0, \"severity\": \"error\", \"rule\": \"filename-convention\", \"message\": \"Filename contains spaces and uppercase letters\", \"suggestion\": \"docs/api-guide.md\", \"context\": { \"current\": \"API Guide.md\", \"suggested\": \"api-guide.md\", \"reason\": \"Spaces create %20 in URLs; uppercase causes case-sensitivity issues\" } } ], \"broken_links\": [ { \"source_file\": \"docs/getting-started.md\", \"line\": 45, \"link_text\": \"API Reference\", \"target\": \"./api/overview.md\", \"link_type\": \"inline\", \"error\": \"target file does not exist\" } ], \"exit_code\": 2 } Field Descriptions Root Fields Field Type Description version string Schema version (semver format) timestamp string ISO 8601 timestamp of lint execution path string Path that was linted auto_detected boolean Whether the path was auto-detected (vs. explicitly provided) summary object Summary statistics (see below) issues array List of lint issues found (see below) broken_links array List of broken links detected (see below) exit_code integer Exit code: 0 (success), 1 (warnings), 2 (errors) Summary Object Field Type Description total_files integer Total number of files scanned errors integer Number of files with errors (blocks build) warnings integer Number of files with warnings passed integer Number of files that passed all checks Invariant: errors + warnings + passed = total_files",
    "tags": [],
    "title": "lint-json-schema",
    "uri": "/docs/reference/lint-json-schema/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "PR Comment Integration Examples This document provides examples for integrating DocBuilder lint results into pull request comments across different platforms.\nTable of Contents GitHub Actions GitLab CI BitBucket Pipelines Azure DevOps Generic Webhook GitHub Actions Basic Comment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 - name: Comment PR with Lint Results if: github.event_name == 'pull_request' uses: actions/github-script@v7 with: script: | const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); let body = '## 📝 Documentation Lint Report\\n\\n'; body += `**Files scanned:** ${report.summary.total_files}\\n`; body += `**Errors:** ${report.summary.errors}\\n`; body += `**Warnings:** ${report.summary.warnings}\\n`; await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: body }); Advanced Comment with Issue Details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 - name: Advanced PR Comment if: github.event_name == 'pull_request' uses: actions/github-script@v7 with: script: | const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); // Build detailed comment let comment = '## 📝 Documentation Lint Report\\n\\n'; // Summary with status emoji if (report.summary.errors === 0 \u0026\u0026 report.summary.warnings === 0) { comment += '✅ **All documentation passes linting!**\\n\\n'; } else { if (report.summary.errors \u003e 0) { comment += `❌ **${report.summary.errors} error(s) found** - merge blocked\\n`; } if (report.summary.warnings \u003e 0) { comment += `⚠️ **${report.summary.warnings} warning(s) found** - should fix\\n`; } comment += '\\n'; } comment += `📊 **Summary:** ${report.summary.total_files} files scanned\\n\\n`; // Group issues by file if (report.issues \u0026\u0026 report.issues.length \u003e 0) { comment += '### Issues Found\\n\\n'; const byFile = {}; for (const issue of report.issues) { if (!byFile[issue.file]) byFile[issue.file] = []; byFile[issue.file].push(issue); } // Show up to 10 files with most severe issues first const files = Object.keys(byFile) .sort((a, b) =\u003e { const aErrors = byFile[a].filter(i =\u003e i.severity === 'error').length; const bErrors = byFile[b].filter(i =\u003e i.severity === 'error').length; return bErrors - aErrors; }) .slice(0, 10); for (const file of files) { const issues = byFile[file]; const errorCount = issues.filter(i =\u003e i.severity === 'error').length; const warnCount = issues.filter(i =\u003e i.severity === 'warning').length; comment += `\u003cdetails\u003e\\n`; comment += `\u003csummary\u003e\u003ccode\u003e${file}\u003c/code\u003e - `; if (errorCount \u003e 0) comment += `${errorCount} error(s) `; if (warnCount \u003e 0) comment += `${warnCount} warning(s)`; comment += `\u003c/summary\u003e\\n\\n`; for (const issue of issues.slice(0, 5)) { const emoji = issue.severity === 'error' ? '❌' : '⚠️'; const lineLink = issue.line \u003e 0 ? `[L${issue.line}](https://github.com/${context.repo.owner}/${context.repo.repo}/blob/${context.payload.pull_request.head.sha}/${issue.file}#L${issue.line})` : 'File-level'; comment += `${emoji} **${issue.rule}** (${lineLink})\\n`; comment += `\u003e ${issue.message}\\n`; if (issue.suggestion) { comment += `\u003e 💡 Suggestion: \\`${issue.suggestion}\\`\\n`; } comment += '\\n'; } if (issues.length \u003e 5) { comment += `... and ${issues.length - 5} more issue(s)\\n`; } comment += `\u003c/details\u003e\\n\\n`; } if (Object.keys(byFile).length \u003e 10) { comment += `*... and ${Object.keys(byFile).length - 10} more file(s) with issues*\\n\\n`; } } // Broken links section if (report.broken_links \u0026\u0026 report.broken_links.length \u003e 0) { comment += `### 🔗 Broken Links (${report.broken_links.length})\\n\\n`; for (const link of report.broken_links.slice(0, 10)) { const lineLink = `[${link.source_file}:${link.line}](https://github.com/${context.repo.owner}/${context.repo.repo}/blob/${context.payload.pull_request.head.sha}/${link.source_file}#L${link.line})`; comment += `- ${lineLink}: \\`${link.target}\\`\\n`; comment += ` *${link.error}*\\n`; } if (report.broken_links.length \u003e 10) { comment += `\\n*... and ${report.broken_links.length - 10} more broken link(s)*\\n`; } comment += '\\n'; } // Instructions comment += '---\\n\\n'; comment += '### How to Fix\\n\\n'; comment += '```bash\\n'; comment += '# Review all issues\\n'; comment += 'docbuilder lint\\n\\n'; comment += '# Auto-fix where possible\\n'; comment += 'docbuilder lint --fix\\n\\n'; comment += '# Preview changes without applying\\n'; comment += 'docbuilder lint --fix --dry-run\\n'; comment += '```\\n\\n'; comment += '*💡 Tip: The pre-commit hook will prevent future issues*\\n'; comment += '```bash\\n'; comment += 'docbuilder lint install-hook\\n'; comment += '```\\n'; // Post or update comment const { data: comments } = await github.rest.issues.listComments({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number }); const botComment = comments.find(c =\u003e c.user.type === 'Bot' \u0026\u0026 c.body.includes('Documentation Lint Report') ); if (botComment) { // Update existing comment await github.rest.issues.updateComment({ owner: context.repo.owner, repo: context.repo.repo, comment_id: botComment.id, body: comment }); } else { // Create new comment await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: comment }); } Minimal Comment (Errors Only) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 - name: Minimal Error Comment if: github.event_name == 'pull_request' \u0026\u0026 steps.lint.outputs.exit_code == '2' uses: actions/github-script@v7 with: script: | const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); const errors = report.issues.filter(i =\u003e i.severity === 'error'); let comment = `❌ **Documentation linting failed** - ${errors.length} error(s) found\\n\\n`; for (const error of errors.slice(0, 5)) { comment += `- \\`${error.file}\\`: ${error.message}\\n`; } if (errors.length \u003e 5) { comment += `\\n*... and ${errors.length - 5} more error(s)*\\n`; } comment += `\\nRun \\`docbuilder lint --fix\\` to resolve automatically.\\n`; await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: comment }); GitLab CI Basic MR Comment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 lint:docs:comment: stage: lint image: alpine:latest needs: - job: lint:docs artifacts: true script: - apk add --no-cache curl jq - | ERRORS=$(jq -r '.summary.errors' lint-report.json) WARNINGS=$(jq -r '.summary.warnings' lint-report.json) TOTAL=$(jq -r '.summary.total_files' lint-report.json) COMMENT=\"## 📝 Documentation Lint Report\\n\\n\" COMMENT=\"${COMMENT}**Files scanned:** ${TOTAL}\\n\" COMMENT=\"${COMMENT}**Errors:** ${ERRORS}\\n\" COMMENT=\"${COMMENT}**Warnings:** ${WARNINGS}\\n\" if [ \"${ERRORS}\" -gt 0 ]; then COMMENT=\"${COMMENT}\\n❌ Linting failed - see details in pipeline artifacts\" else COMMENT=\"${COMMENT}\\n✅ All documentation passes linting!\" fi # Post to MR curl --request POST \\ --header \"PRIVATE-TOKEN: ${GITLAB_API_TOKEN}\" \\ --header \"Content-Type: application/json\" \\ --data \"{\\\"body\\\": \\\"${COMMENT}\\\"}\" \\ \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/merge_requests/${CI_MERGE_REQUEST_IID}/notes\" rules: - if: $CI_PIPELINE_SOURCE == \"merge_request_event\" allow_failure: true Detailed GitLab Comment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 lint:docs:comment: stage: lint image: alpine:latest needs: - job: lint:docs artifacts: true script: - apk add --no-cache curl jq - | # Build detailed comment with issue grouping COMMENT=$(cat \u003c\u003c'EOF' ## 📝 Documentation Lint Report EOF ) TOTAL=$(jq -r '.summary.total_files' lint-report.json) ERRORS=$(jq -r '.summary.errors' lint-report.json) WARNINGS=$(jq -r '.summary.warnings' lint-report.json) COMMENT=\"${COMMENT}**Summary:** ${TOTAL} files scanned\\n\\n\" if [ \"${ERRORS}\" -gt 0 ]; then COMMENT=\"${COMMENT}❌ **${ERRORS} error(s)** - merge blocked\\n\" fi if [ \"${WARNINGS}\" -gt 0 ]; then COMMENT=\"${COMMENT}⚠️ **${WARNINGS} warning(s)** - should fix\\n\" fi # Add top 5 issues ISSUES=$(jq -r '.issues[:5] | .[] | \"- **\\(.rule)** in `\\(.file)` (L\\(.line)): \\(.message)\"' lint-report.json) if [ -n \"${ISSUES}\" ]; then COMMENT=\"${COMMENT}\\n### Issues\\n\\n${ISSUES}\\n\" fi COMMENT=\"${COMMENT}\\n---\\n**Fix:** \\`docbuilder lint --fix\\`\\n\" # Escape for JSON COMMENT_ESCAPED=$(echo \"${COMMENT}\" | jq -Rs .) # Post to MR curl --request POST \\ --header \"PRIVATE-TOKEN: ${GITLAB_API_TOKEN}\" \\ --header \"Content-Type: application/json\" \\ --data \"{\\\"body\\\": ${COMMENT_ESCAPED}}\" \\ \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/merge_requests/${CI_MERGE_REQUEST_IID}/notes\" rules: - if: $CI_PIPELINE_SOURCE == \"merge_request_event\" allow_failure: true BitBucket Pipelines 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 pipelines: pull-requests: '**': - step: name: Lint Documentation image: golang:1.21 script: - go install git.home.luguber.info/inful/docbuilder/cmd/docbuilder@latest - docbuilder lint --format=json \u003e lint-report.json || true # Parse results - ERRORS=$(jq -r '.summary.errors' lint-report.json) - WARNINGS=$(jq -r '.summary.warnings' lint-report.json) # Build comment - | COMMENT=\"## Documentation Lint Report\\n\\n\" COMMENT=\"${COMMENT}Errors: ${ERRORS}, Warnings: ${WARNINGS}\\n\\n\" if [ \"${ERRORS}\" -gt 0 ]; then COMMENT=\"${COMMENT}❌ Linting failed\\n\" fi # Post comment via API curl -X POST \\ -u \"${BB_AUTH_STRING}\" \\ -H \"Content-Type: application/json\" \\ -d \"{\\\"content\\\": {\\\"raw\\\": \\\"${COMMENT}\\\"}}\" \\ \"https://api.bitbucket.org/2.0/repositories/${BITBUCKET_REPO_FULL_NAME}/pullrequests/${BITBUCKET_PR_ID}/comments\" # Fail if errors found - test \"${ERRORS}\" -eq 0 artifacts: - lint-report.json Azure DevOps 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 - task: Bash@3 displayName: 'Lint Documentation' inputs: targetType: 'inline' script: | go install git.home.luguber.info/inful/docbuilder/cmd/docbuilder@latest docbuilder lint --format=json \u003e lint-report.json || true - task: Bash@3 displayName: 'Comment PR with Results' condition: eq(variables['Build.Reason'], 'PullRequest') inputs: targetType: 'inline' script: | ERRORS=$(jq -r '.summary.errors' lint-report.json) WARNINGS=$(jq -r '.summary.warnings' lint-report.json) COMMENT=\"## Documentation Lint Report\\n\\n\" COMMENT=\"${COMMENT}Errors: ${ERRORS}, Warnings: ${WARNINGS}\\n\" # Post to PR using Azure DevOps API az repos pr comment create \\ --org \"$(System.TeamFoundationCollectionUri)\" \\ --project \"$(System.TeamProject)\" \\ --pull-request-id \"$(System.PullRequest.PullRequestId)\" \\ --repository-id \"$(Build.Repository.ID)\" \\ --content \"${COMMENT}\" env: AZURE_DEVOPS_EXT_PAT: $(System.AccessToken) Generic Webhook For platforms without native integrations, post results to a webhook:\n1 2 3 4 5 6 7 8 9 10 #!/bin/bash # Run linter docbuilder lint --format=json \u003e lint-report.json # Post to webhook curl -X POST https://your-webhook.example.com/lint-results \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${WEBHOOK_TOKEN}\" \\ --data @lint-report.json Best Practices 1. Update Instead of Duplicate Always update existing comments instead of creating new ones:\n1 2 3 4 5 6 7 8 9 10 const existingComment = comments.find(c =\u003e c.user.type === 'Bot' \u0026\u0026 c.body.includes('Documentation Lint Report') ); if (existingComment) { await github.rest.issues.updateComment({...}); } else { await github.rest.issues.createComment({...}); } 2. Collapsible Sections Use \u003cdetails\u003e tags for long issue lists:\n1 2 3 4 5 6 7 8 \u003cdetails\u003e \u003csummary\u003e📄 docs/api-guide.md - 5 issues\u003c/summary\u003e - Error 1 - Error 2 ... \u003c/details\u003e 3. Link to Source Link directly to the problematic lines:\n1 const lineLink = `[L${issue.line}](https://github.com/${owner}/${repo}/blob/${sha}/${file}#L${issue.line})`; 4. Rate Limiting Avoid posting comments on every push to a PR:\n1 2 3 4 5 6 7 8 9 10 11 # Only comment once per PR - name: Check for existing comment id: check run: | EXISTING=$(gh pr view ${{ github.event.pull_request.number }} \\ --json comments --jq '.comments[] | select(.body | contains(\"Documentation Lint Report\"))') echo \"has_comment=$([[ -n \"$EXISTING\" ]] \u0026\u0026 echo true || echo false)\" \u003e\u003e $GITHUB_OUTPUT - name: Comment PR if: steps.check.outputs.has_comment == 'false' ... 5. Conditional Posting Only post when there are issues:\n1 2 3 - name: Comment PR if: steps.parse.outputs.errors != '0' || steps.parse.outputs.warnings != '0' ... 6. Clear Remediation Steps Always include actionable instructions:\n1 2 3 4 5 6 7 8 9 ### How to Fix 1. Run locally: `docbuilder lint` 2. Auto-fix: `docbuilder lint --fix` 3. Review changes and commit Or install the pre-commit hook: ```bash docbuilder lint install-hook ### 7. Status Emojis Use consistent emojis for quick visual scanning: - ✅ Success - ❌ Errors (blocking) - ⚠️ Warnings (non-blocking) - 🔗 Broken links - 💡 Suggestions - 📊 Summary stats ## Testing Comments Test your comment formatting locally before deploying: ```bash # Generate test report docbuilder lint --format=json \u003e test-report.json # Test comment generation node test-comment-script.js # Validate markdown npx markdownlint comment.md Security Considerations Token Permissions: Use minimal required permissions\nGitHub: pull-requests: write only GitLab: api scope with project access Sensitive Data: Never include secrets in comments\nSanitize file paths if they contain usernames Don’t expose internal URLs Rate Limits: Respect platform API rate limits\nCache comment existence checks Batch operations when possible Spam Prevention: Limit comment size and frequency\nCap issue display (e.g., max 10 files, 5 issues per file) Update existing comments instead of creating new ones",
    "description": "PR Comment Integration Examples This document provides examples for integrating DocBuilder lint results into pull request comments across different platforms.\nTable of Contents GitHub Actions GitLab CI BitBucket Pipelines Azure DevOps Generic Webhook GitHub Actions Basic Comment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 - name: Comment PR with Lint Results if: github.event_name == 'pull_request' uses: actions/github-script@v7 with: script: | const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); let body = '## 📝 Documentation Lint Report\\n\\n'; body += `**Files scanned:** ${report.summary.total_files}\\n`; body += `**Errors:** ${report.summary.errors}\\n`; body += `**Warnings:** ${report.summary.warnings}\\n`; await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: body }); Advanced Comment with Issue Details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 - name: Advanced PR Comment if: github.event_name == 'pull_request' uses: actions/github-script@v7 with: script: | const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); // Build detailed comment let comment = '## 📝 Documentation Lint Report\\n\\n'; // Summary with status emoji if (report.summary.errors === 0 \u0026\u0026 report.summary.warnings === 0) { comment += '✅ **All documentation passes linting!**\\n\\n'; } else { if (report.summary.errors \u003e 0) { comment += `❌ **${report.summary.errors} error(s) found** - merge blocked\\n`; } if (report.summary.warnings \u003e 0) { comment += `⚠️ **${report.summary.warnings} warning(s) found** - should fix\\n`; } comment += '\\n'; } comment += `📊 **Summary:** ${report.summary.total_files} files scanned\\n\\n`; // Group issues by file if (report.issues \u0026\u0026 report.issues.length \u003e 0) { comment += '### Issues Found\\n\\n'; const byFile = {}; for (const issue of report.issues) { if (!byFile[issue.file]) byFile[issue.file] = []; byFile[issue.file].push(issue); } // Show up to 10 files with most severe issues first const files = Object.keys(byFile) .sort((a, b) =\u003e { const aErrors = byFile[a].filter(i =\u003e i.severity === 'error').length; const bErrors = byFile[b].filter(i =\u003e i.severity === 'error').length; return bErrors - aErrors; }) .slice(0, 10); for (const file of files) { const issues = byFile[file]; const errorCount = issues.filter(i =\u003e i.severity === 'error').length; const warnCount = issues.filter(i =\u003e i.severity === 'warning').length; comment += `\u003cdetails\u003e\\n`; comment += `\u003csummary\u003e\u003ccode\u003e${file}\u003c/code\u003e - `; if (errorCount \u003e 0) comment += `${errorCount} error(s) `; if (warnCount \u003e 0) comment += `${warnCount} warning(s)`; comment += `\u003c/summary\u003e\\n\\n`; for (const issue of issues.slice(0, 5)) { const emoji = issue.severity === 'error' ? '❌' : '⚠️'; const lineLink = issue.line \u003e 0 ? `[L${issue.line}](https://github.com/${context.repo.owner}/${context.repo.repo}/blob/${context.payload.pull_request.head.sha}/${issue.file}#L${issue.line})` : 'File-level'; comment += `${emoji} **${issue.rule}** (${lineLink})\\n`; comment += `\u003e ${issue.message}\\n`; if (issue.suggestion) { comment += `\u003e 💡 Suggestion: \\`${issue.suggestion}\\`\\n`; } comment += '\\n'; } if (issues.length \u003e 5) { comment += `... and ${issues.length - 5} more issue(s)\\n`; } comment += `\u003c/details\u003e\\n\\n`; } if (Object.keys(byFile).length \u003e 10) { comment += `*... and ${Object.keys(byFile).length - 10} more file(s) with issues*\\n\\n`; } } // Broken links section if (report.broken_links \u0026\u0026 report.broken_links.length \u003e 0) { comment += `### 🔗 Broken Links (${report.broken_links.length})\\n\\n`; for (const link of report.broken_links.slice(0, 10)) { const lineLink = `[${link.source_file}:${link.line}](https://github.com/${context.repo.owner}/${context.repo.repo}/blob/${context.payload.pull_request.head.sha}/${link.source_file}#L${link.line})`; comment += `- ${lineLink}: \\`${link.target}\\`\\n`; comment += ` *${link.error}*\\n`; } if (report.broken_links.length \u003e 10) { comment += `\\n*... and ${report.broken_links.length - 10} more broken link(s)*\\n`; } comment += '\\n'; } // Instructions comment += '---\\n\\n'; comment += '### How to Fix\\n\\n'; comment += '```bash\\n'; comment += '# Review all issues\\n'; comment += 'docbuilder lint\\n\\n'; comment += '# Auto-fix where possible\\n'; comment += 'docbuilder lint --fix\\n\\n'; comment += '# Preview changes without applying\\n'; comment += 'docbuilder lint --fix --dry-run\\n'; comment += '```\\n\\n'; comment += '*💡 Tip: The pre-commit hook will prevent future issues*\\n'; comment += '```bash\\n'; comment += 'docbuilder lint install-hook\\n'; comment += '```\\n'; // Post or update comment const { data: comments } = await github.rest.issues.listComments({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number }); const botComment = comments.find(c =\u003e c.user.type === 'Bot' \u0026\u0026 c.body.includes('Documentation Lint Report') ); if (botComment) { // Update existing comment await github.rest.issues.updateComment({ owner: context.repo.owner, repo: context.repo.repo, comment_id: botComment.id, body: comment }); } else { // Create new comment await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: comment }); } Minimal Comment (Errors Only) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 - name: Minimal Error Comment if: github.event_name == 'pull_request' \u0026\u0026 steps.lint.outputs.exit_code == '2' uses: actions/github-script@v7 with: script: | const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); const errors = report.issues.filter(i =\u003e i.severity === 'error'); let comment = `❌ **Documentation linting failed** - ${errors.length} error(s) found\\n\\n`; for (const error of errors.slice(0, 5)) { comment += `- \\`${error.file}\\`: ${error.message}\\n`; } if (errors.length \u003e 5) { comment += `\\n*... and ${errors.length - 5} more error(s)*\\n`; } comment += `\\nRun \\`docbuilder lint --fix\\` to resolve automatically.\\n`; await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: comment }); GitLab CI Basic MR Comment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 lint:docs:comment: stage: lint image: alpine:latest needs: - job: lint:docs artifacts: true script: - apk add --no-cache curl jq - | ERRORS=$(jq -r '.summary.errors' lint-report.json) WARNINGS=$(jq -r '.summary.warnings' lint-report.json) TOTAL=$(jq -r '.summary.total_files' lint-report.json) COMMENT=\"## 📝 Documentation Lint Report\\n\\n\" COMMENT=\"${COMMENT}**Files scanned:** ${TOTAL}\\n\" COMMENT=\"${COMMENT}**Errors:** ${ERRORS}\\n\" COMMENT=\"${COMMENT}**Warnings:** ${WARNINGS}\\n\" if [ \"${ERRORS}\" -gt 0 ]; then COMMENT=\"${COMMENT}\\n❌ Linting failed - see details in pipeline artifacts\" else COMMENT=\"${COMMENT}\\n✅ All documentation passes linting!\" fi # Post to MR curl --request POST \\ --header \"PRIVATE-TOKEN: ${GITLAB_API_TOKEN}\" \\ --header \"Content-Type: application/json\" \\ --data \"{\\\"body\\\": \\\"${COMMENT}\\\"}\" \\ \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/merge_requests/${CI_MERGE_REQUEST_IID}/notes\" rules: - if: $CI_PIPELINE_SOURCE == \"merge_request_event\" allow_failure: true Detailed GitLab Comment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 lint:docs:comment: stage: lint image: alpine:latest needs: - job: lint:docs artifacts: true script: - apk add --no-cache curl jq - | # Build detailed comment with issue grouping COMMENT=$(cat \u003c\u003c'EOF' ## 📝 Documentation Lint Report EOF ) TOTAL=$(jq -r '.summary.total_files' lint-report.json) ERRORS=$(jq -r '.summary.errors' lint-report.json) WARNINGS=$(jq -r '.summary.warnings' lint-report.json) COMMENT=\"${COMMENT}**Summary:** ${TOTAL} files scanned\\n\\n\" if [ \"${ERRORS}\" -gt 0 ]; then COMMENT=\"${COMMENT}❌ **${ERRORS} error(s)** - merge blocked\\n\" fi if [ \"${WARNINGS}\" -gt 0 ]; then COMMENT=\"${COMMENT}⚠️ **${WARNINGS} warning(s)** - should fix\\n\" fi # Add top 5 issues ISSUES=$(jq -r '.issues[:5] | .[] | \"- **\\(.rule)** in `\\(.file)` (L\\(.line)): \\(.message)\"' lint-report.json) if [ -n \"${ISSUES}\" ]; then COMMENT=\"${COMMENT}\\n### Issues\\n\\n${ISSUES}\\n\" fi COMMENT=\"${COMMENT}\\n---\\n**Fix:** \\`docbuilder lint --fix\\`\\n\" # Escape for JSON COMMENT_ESCAPED=$(echo \"${COMMENT}\" | jq -Rs .) # Post to MR curl --request POST \\ --header \"PRIVATE-TOKEN: ${GITLAB_API_TOKEN}\" \\ --header \"Content-Type: application/json\" \\ --data \"{\\\"body\\\": ${COMMENT_ESCAPED}}\" \\ \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/merge_requests/${CI_MERGE_REQUEST_IID}/notes\" rules: - if: $CI_PIPELINE_SOURCE == \"merge_request_event\" allow_failure: true BitBucket Pipelines 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 pipelines: pull-requests: '**': - step: name: Lint Documentation image: golang:1.21 script: - go install git.home.luguber.info/inful/docbuilder/cmd/docbuilder@latest - docbuilder lint --format=json \u003e lint-report.json || true # Parse results - ERRORS=$(jq -r '.summary.errors' lint-report.json) - WARNINGS=$(jq -r '.summary.warnings' lint-report.json) # Build comment - | COMMENT=\"## Documentation Lint Report\\n\\n\" COMMENT=\"${COMMENT}Errors: ${ERRORS}, Warnings: ${WARNINGS}\\n\\n\" if [ \"${ERRORS}\" -gt 0 ]; then COMMENT=\"${COMMENT}❌ Linting failed\\n\" fi # Post comment via API curl -X POST \\ -u \"${BB_AUTH_STRING}\" \\ -H \"Content-Type: application/json\" \\ -d \"{\\\"content\\\": {\\\"raw\\\": \\\"${COMMENT}\\\"}}\" \\ \"https://api.bitbucket.org/2.0/repositories/${BITBUCKET_REPO_FULL_NAME}/pullrequests/${BITBUCKET_PR_ID}/comments\" # Fail if errors found - test \"${ERRORS}\" -eq 0 artifacts: - lint-report.json Azure DevOps 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 - task: Bash@3 displayName: 'Lint Documentation' inputs: targetType: 'inline' script: | go install git.home.luguber.info/inful/docbuilder/cmd/docbuilder@latest docbuilder lint --format=json \u003e lint-report.json || true - task: Bash@3 displayName: 'Comment PR with Results' condition: eq(variables['Build.Reason'], 'PullRequest') inputs: targetType: 'inline' script: | ERRORS=$(jq -r '.summary.errors' lint-report.json) WARNINGS=$(jq -r '.summary.warnings' lint-report.json) COMMENT=\"## Documentation Lint Report\\n\\n\" COMMENT=\"${COMMENT}Errors: ${ERRORS}, Warnings: ${WARNINGS}\\n\" # Post to PR using Azure DevOps API az repos pr comment create \\ --org \"$(System.TeamFoundationCollectionUri)\" \\ --project \"$(System.TeamProject)\" \\ --pull-request-id \"$(System.PullRequest.PullRequestId)\" \\ --repository-id \"$(Build.Repository.ID)\" \\ --content \"${COMMENT}\" env: AZURE_DEVOPS_EXT_PAT: $(System.AccessToken) Generic Webhook For platforms without native integrations, post results to a webhook:",
    "tags": [],
    "title": "pr-comment-integration",
    "uri": "/docs/how-to/pr-comment-integration/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Documentation for refactoring",
    "description": "Documentation for refactoring",
    "tags": [],
    "title": "refactoring",
    "uri": "/docs/refactoring/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Documentation for reference",
    "description": "Documentation for reference",
    "tags": [],
    "title": "reference",
    "uri": "/docs/reference/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Generated documentation",
    "description": "Generated documentation",
    "tags": [],
    "title": "Test",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Documentation for testing",
    "description": "Documentation for testing",
    "tags": [],
    "title": "testing",
    "uri": "/docs/testing/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Documentation for tutorials",
    "description": "Documentation for tutorials",
    "tags": [],
    "title": "tutorials",
    "uri": "/docs/tutorials/index.html"
  },
  {
    "breadcrumb": "Test",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Code-Quality",
    "uri": "/categories/code-quality/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Complexity",
    "uri": "/tags/complexity/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e refactoring",
    "content": "Overview The nestif linter has identified 42 instances of complex nested blocks across the codebase. This document outlines a systematic plan to reduce this complexity through refactoring.\nComplexity Distribution Critical (≥15): 3 instances High (10-14): 9 instances Medium (7-9): 13 instances Low (5-6): 17 instances Priority Areas for Refactoring Priority 1: Critical Complexity (≥15) 1. internal/hugo/modules.go:33 (complexity: 20) Function: Module initialization logic Issue: Deep nesting in module setup and Go module management Refactoring Strategy:\nExtract module existence check into separate function Extract Go module initialization into dedicated method Use early returns to reduce nesting depth Consider builder pattern for module configuration 2. internal/daemon/http_server.go:337 (complexity: 19) Function: HTTP server static file serving logic Issue: Complex nested conditionals for path resolution and file serving Refactoring Strategy:\nExtract path resolution logic into separate function Extract content type detection into dedicated method Use table-driven approach for MIME type mapping Consider strategy pattern for different file type handling 3. internal/hugo/classification.go:104 (complexity: 14) Function: Error classification logic Issue: Multiple nested conditions for error type detection Refactoring Strategy:\nUse type switches instead of nested if statements Extract error pattern matching into separate functions Consider error classifier interface with multiple implementations Priority 2: High Complexity (10-14) 4. internal/daemon/build_queue.go:291 (complexity: 13) Function: Event emission in build queue Issue: Complex nested logic for event processing Refactoring Strategy:\nExtract event building into separate function Use builder pattern for event construction Separate event emission logic from conditional checks 5. internal/daemon/status.go:140 (complexity: 13) Function: Status projection logic Issue: Deep nesting in status data aggregation Refactoring Strategy:\nExtract projection building into dedicated method Use early returns for nil checks Consider separate status aggregator component 6. internal/hugo/indexes.go:284 (complexity: 13) Function: Index file content parsing Issue: Complex frontmatter parsing logic with nested conditions Refactoring Strategy:\nExtract frontmatter parsing into separate parser struct Use state machine pattern for parsing logic Separate validation from parsing 7. internal/lint/fixer.go:192 (complexity: 13) Function: Filename fix logic Issue: Complex nested conditions for fix application Refactoring Strategy:\nExtract fix validation into separate function Use command pattern for different fix types Separate dry-run logic from actual fix application 8. internal/auth/manager_test.go:106 (complexity: 12) Function: Test assertion logic Issue: Complex nested test validations Refactoring Strategy:\nExtract assertion logic into helper functions Use table-driven test structure Create dedicated assertion helpers Priority 3: Medium Complexity (7-9) Files with complexity 7-9 should be addressed after Priority 1 and 2:\ninternal/lint/fixer.go (multiple instances at lines 640, 764, 925) internal/lint/formatter.go:101 internal/hugo/pipeline/transform_links.go (lines 174, 249) internal/hugo/renderer.go:77 internal/server/handlers/webhook.go (lines 107, 147) General Refactoring Strategies:\nExtract nested loops into separate functions Use early returns to reduce nesting Apply guard clauses for precondition checks Extract complex boolean expressions into named functions Priority 4: Low Complexity (5-6) 17 instances with complexity 5-6. These should be addressed as part of regular maintenance:\nFocus on readability improvements Apply guard clauses Use early returns Extract small helper functions Common Refactoring Patterns Pattern 1: Extract Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Before (complexity: 8) if condition1 { if condition2 { if condition3 { // deep logic } } } // After (complexity: 3) if condition1 { handleCondition1() } func handleCondition1() { if condition2 { handleCondition2() } } Pattern 2: Early Returns (Guard Clauses) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Before (complexity: 7) if condition1 { if condition2 { // logic } else { return err } } // After (complexity: 3) if !condition1 { return nil } if !condition2 { return err } // logic Pattern 3: Replace Type Assertions with Type Switch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Before (complexity: 9) if errors.As(err, \u0026typeA) { if typeA.Field == value { // handle typeA } } else if errors.As(err, \u0026typeB) { // handle typeB } // After (complexity: 4) switch e := err.(type) { case *TypeA: return handleTypeA(e) case *TypeB: return handleTypeB(e) } Pattern 4: Strategy Pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Before (complexity: 10) if fileType == \"css\" { if condition { // css specific logic } } else if fileType == \"js\" { if condition { // js specific logic } } // After (complexity: 3) handler := fileHandlers[fileType] if handler != nil { return handler.Handle(file) } Implementation Plan Phase 1: Critical Issues (Week 1) Refactor internal/hugo/modules.go (complexity: 20) Refactor internal/daemon/http_server.go (complexity: 19) Refactor internal/hugo/classification.go (complexity: 14) Run tests to ensure no regressions Update documentation if APIs change Phase 2: High Complexity Issues (Week 2) Refactor daemon build queue and status (complexity: 13) Refactor hugo indexes (complexity: 13) Refactor lint fixer core logic (complexity: 13) Refactor auth manager tests (complexity: 12) Run full test suite Phase 3: Medium Complexity Issues (Week 3) Refactor lint fixer link parsing (multiple instances) Refactor hugo pipeline transforms Refactor server webhook handlers Run integration tests Phase 4: Low Complexity Issues (Week 4) Address remaining instances (complexity 5-6) Final test suite run Update architectural documentation Success Criteria Zero Critical Issues: No functions with complexity ≥ 15 Minimal High Issues: Fewer than 5 functions with complexity 10-14 Test Coverage Maintained: No reduction in test coverage No Regressions: All existing tests pass Documentation Updated: Reflect any architectural changes Tracking Create tracking issues for each priority area:\nIssue #1: [Refactor] Reduce nested complexity in hugo modules (P1) Issue #2: [Refactor] Reduce nested complexity in daemon http server (P1) Issue #3: [Refactor] Reduce nested complexity in hugo error classification (P1) Issue #4: [Refactor] Reduce nested complexity in daemon build queue (P2) Issue #5: [Refactor] Reduce nested complexity in lint fixer (P2) Notes Each refactoring should be done in a separate PR Include before/after complexity metrics in PR description Maintain backward compatibility where possible Update unit tests to reflect new structure Consider adding integration tests for refactored areas References Effective Go - Control Structures Go Code Review Comments - Indent Error Flow Martin Fowler - Refactoring Catalog",
    "description": "Overview The nestif linter has identified 42 instances of complex nested blocks across the codebase. This document outlines a systematic plan to reduce this complexity through refactoring.\nComplexity Distribution Critical (≥15): 3 instances High (10-14): 9 instances Medium (7-9): 13 instances Low (5-6): 17 instances Priority Areas for Refactoring Priority 1: Critical Complexity (≥15) 1. internal/hugo/modules.go:33 (complexity: 20) Function: Module initialization logic Issue: Deep nesting in module setup and Go module management Refactoring Strategy:",
    "tags": [
      "Nestif",
      "Complexity",
      "Technical-Debt"
    ],
    "title": "Nested Complexity Reduction Plan",
    "uri": "/docs/refactoring/nestif-complexity-reduction/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Nestif",
    "uri": "/tags/nestif/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Refactoring",
    "uri": "/categories/refactoring/index.html"
  },
  {
    "breadcrumb": "Test",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Technical-Debt",
    "uri": "/tags/technical-debt/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Architecture-Decisions",
    "uri": "/categories/architecture-decisions/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Automation",
    "uri": "/tags/automation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Changelog",
    "uri": "/tags/changelog/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Ci-Cd",
    "uri": "/tags/ci-cd/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Cleanup",
    "uri": "/tags/cleanup/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Developer-Experience",
    "uri": "/tags/developer-experience/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Documentation",
    "uri": "/tags/documentation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Git-Hooks",
    "uri": "/tags/git-hooks/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Github-Actions",
    "uri": "/tags/github-actions/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Gitlab-Ci",
    "uri": "/tags/gitlab-ci/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e testing",
    "content": "Golden Test Implementation Summary Overview This document describes the comprehensive golden test framework for DocBuilder’s integration testing. The framework verifies end-to-end functionality including Git cloning, document discovery, Hugo generation, and configuration output.\nImplementation Status ✅ Foundation (Complete) Infrastructure: Test helpers, golden config loading, build execution Files: test/integration/helpers.go (437 lines) - Core test utilities test/integration/golden_test.go (813 lines) - Test implementations test/testdata/golden/ (12 directories) - Golden file storage test/testdata/configs/ (14 files) - Test configurations Key Functions: loadGoldenConfig() - Load and validate test configs setupTestRepo() - Create temporary git repositories for testing verifyHugoConfig() - Compare Hugo YAML configuration verifyContentStructure() - Check content files and front matter parseFrontMatter() - Extract and validate YAML front matter ✅ Core Test Coverage (Complete - 16 tests) Content Transformation Tests TestGolden_FrontmatterInjection - Front matter generation with editURL TestGolden_ImagePaths - Image path transformation TestGolden_SectionIndexes - Section index page generation TestGolden_MenuGeneration - Automatic menu generation Repository Tests TestGolden_TwoRepos - Multi-repository aggregation TestGolden_ConflictingPaths - Path conflict resolution TestGolden_CrossRepoLinks - Cross-repository linking Edge Case Tests TestGolden_EmptyDocs - Repository with no markdown files TestGolden_OnlyReadme - Only README.md (should be skipped) TestGolden_MalformedFrontmatter - Invalid YAML front matter TestGolden_DeepNesting - Deep directory nesting (10+ levels) TestGolden_UnicodeNames - Unicode filenames and paths TestGolden_SpecialChars - Special characters in filenames Error Handling Tests TestGolden_Error_InvalidRepository - Invalid repository URL handling TestGolden_Error_InvalidConfig - Empty/minimal configuration TestGolden_Warning_NoGitCommit - Repository without commits Note: All tests use the Relearn theme. Error tests use relearn-basic.yaml as a base configuration.\nTest Results Total Tests: 16 Status: All Passing ✅ Test Execution Time: ~1.1s (without Hugo rendering) Error Case Tests: 3 tests with graceful degradation verification Theme Support: Relearn only Golden File Structure Each golden directory contains verification artifacts:\ntest/testdata/golden/\u003ctest-name\u003e/ ├── hugo-config.golden.yaml # Expected Hugo configuration ├── content-structure.golden.json # Content files and front matter └── rendered-samples.golden.json # HTML verification samples (optional) Available Golden Directories All golden directories are actively used for Relearn theme testing:\nconflicting-paths/ - Path conflict scenarios cross-repo-links/ - Inter-repository linking deep-nesting/ - Deep directory hierarchies empty-docs/ - Empty repository handling frontmatter-injection/ - Front matter generation image-paths/ - Image path transformation malformed-frontmatter/ - Invalid YAML handling menu-generation/ - Automatic menu creation only-readme/ - README-only repositories section-indexes/ - Section index pages special-chars/ - Special character filenames two-repos/ - Multi-repository builds unicode-names/ - Unicode filename support Key Technical Decisions 1. Build Service Integration Choice: Use build.BuildService for test execution Rationale: Tests verify the actual production pipeline, not mocked components Implementation: Each test creates a BuildService with real HugoGenerator 2. Error Handling Philosophy Choice: Verify graceful degradation, not hard failures Rationale: Build service logs errors but continues when possible Implementation: Check RepositoriesSkipped counter rather than expecting errors 3. Golden File Format Hugo Config: YAML files with normalized timestamps Content Structure: JSON with file paths, front matter, and content hashes Rationale: Structured formats allow precise, diffable comparisons 4. Test Organization Pattern: One test per scenario with descriptive names Naming: TestGolden_{Feature} for discoverability Isolation: Each test uses t.TempDir() for complete isolation No HTML Rendering: Tests verify Hugo config/content, not rendered HTML (too slow) Usage Running Tests 1 2 3 4 5 6 7 8 9 10 11 # Run all golden tests go test ./test/integration -run=TestGolden -v # Run specific test go test ./test/integration -run=TestGolden_FrontmatterInjection -v # Update golden files (when changes are expected) go test ./test/integration -run=TestGolden_FrontmatterInjection -update-golden # Run in short mode (skips golden tests) go test ./test/integration -short Adding New Tests Create test repository structure in test/testdata/repos/\u003ccategory\u003e/\u003ctest-name\u003e/\nAdd markdown files, images, and other assets Structure should reflect a real documentation repository Create test configuration in test/testdata/configs/\u003ctest-name\u003e.yaml\nDefine repository URLs (will be replaced by setupTestRepo) Configure Hugo with Relearn theme and parameters Set output directory (will be replaced by t.TempDir()) Add test function to test/integration/golden_test.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func TestGolden_YourFeature(t *testing.T) { if testing.Short() { t.Skip(\"Skipping golden test in short mode\") } // Setup test repo repoPath := setupTestRepo(t, \"../../test/testdata/repos/\u003cpath\u003e\") // Load and configure cfg := loadGoldenConfig(t, \"../../test/testdata/configs/your-feature.yaml\") cfg.Repositories[0].URL = repoPath outputDir := t.TempDir() cfg.Output.Directory = outputDir // Run build svc := build.NewBuildService(). WithHugoGeneratorFactory(func(cfgAny any, outDir string) build.HugoGenerator { return hugo.NewGenerator(cfgAny.(*config.Config), outDir) }) result, err := svc.Run(context.Background(), build.BuildRequest{ Config: cfg, OutputDir: outputDir, }) require.NoError(t, err) require.Equal(t, build.BuildStatusSuccess, result.Status) // Verify outputs goldenDir := \"../../test/testdata/golden/your-feature\" verifyHugoConfig(t, outputDir, goldenDir+\"/hugo-config.golden.yaml\", *updateGolden) verifyContentStructure(t, outputDir, goldenDir+\"/content-structure.golden.json\", *updateGolden) } Generate golden files with -update-golden flag\n1 go test ./test/integration -run=TestGolden_YourFeature -update-golden Review golden files - Don’t blindly accept generated output\nCheck hugo-config.golden.yaml for correct configuration Verify content-structure.golden.json has expected files and front matter Commit golden files with your test Verify test passes without -update-golden\n1 go test ./test/integration -run=TestGolden_YourFeature -v Future Enhancements Potential Additions Relearn Theme Features: Tests for Relearn-specific features (tabs, attachments, shortcodes) Performance Benchmarks: Track build time trends with testing.B Incremental Build Testing: Verify cache behavior and rebuild optimization Plugin System Testing: Verify transform plugins work correctly Build Report Validation: Parse and verify build-report.json accuracy HTML Rendering Tests: Add optional Hugo build + HTML verification (currently skipped for speed) CI Integration Golden tests run on every PR Flag golden file changes for manual review Generate test coverage reports Detect regressions in test execution time Helper Functions Reference Test Repository Setup setupTestRepo(t, path) - Creates temporary git repository from testdata Copies files from source path Initializes git repository Creates initial commit Returns temporary directory path Configuration Loading loadGoldenConfig(t, path) - Loads YAML test configuration Validates required fields Returns *config.Config Verification Functions verifyHugoConfig(t, outputDir, goldenPath, update) - Compares Hugo YAML\nNormalizes dynamic fields (build_date, timestamps) Updates golden file if update=true Asserts YAML equality verifyContentStructure(t, outputDir, goldenPath, update) - Verifies content files\nWalks content directory Extracts front matter from each file Computes content hashes Compares structure and metadata parseFrontMatter(data) - Extracts YAML front matter\nReturns front matter map and remaining content Handles files without front matter Build Execution Tests use build.BuildService directly:\n1 2 3 4 5 6 7 8 9 svc := build.NewBuildService(). WithHugoGeneratorFactory(func(cfgAny any, outDir string) build.HugoGenerator { return hugo.NewGenerator(cfgAny.(*config.Config), outDir) }) result, err := svc.Run(context.Background(), build.BuildRequest{ Config: cfg, OutputDir: outputDir, }) Related Documentation Test Architecture CI/CD Setup Configuration Reference Style Guide Maintenance Notes Golden File Review Never blindly regenerate golden files - always review changes Golden files represent expected behavior, not actual output Changes to golden files should be justified in PR descriptions Hugo config changes should be theme-compatible Test Design Guidelines Keep tests fast (target \u003c 2s for full suite) Don’t run Hugo build in tests (too slow, config is sufficient) Use t.TempDir() for complete test isolation Test one scenario per test function Error tests should verify graceful degradation, not hard failures File Organization test/ ├── integration/ │ ├── golden_test.go # Test implementations (813 lines) │ ├── helpers.go # Test utilities (437 lines) │ └── README.md # Integration test overview ├── testdata/ │ ├── configs/ # YAML test configurations (14 files) │ ├── golden/ # Expected outputs (12 directories) │ └── repos/ # Test repository structures │ ├── multi-repo/ # Multi-repository scenarios │ └── transforms/ # Content transformation tests Common Pitfalls Dynamic timestamps: Use verifyHugoConfig which normalizes build_date Absolute paths: Golden files should not contain temp directory paths Git state: Use setupTestRepo to ensure consistent git history Configuration placeholders: Test configs use PLACEHOLDER for dynamic values",
    "description": "Golden Test Implementation Summary Overview This document describes the comprehensive golden test framework for DocBuilder’s integration testing. The framework verifies end-to-end functionality including Git cloning, document discovery, Hugo generation, and configuration output.\nImplementation Status ✅ Foundation (Complete) Infrastructure: Test helpers, golden config loading, build execution Files: test/integration/helpers.go (437 lines) - Core test utilities test/integration/golden_test.go (813 lines) - Test implementations test/testdata/golden/ (12 directories) - Golden file storage test/testdata/configs/ (14 files) - Test configurations Key Functions: loadGoldenConfig() - Load and validate test configs setupTestRepo() - Create temporary git repositories for testing verifyHugoConfig() - Compare Hugo YAML configuration verifyContentStructure() - Check content files and front matter parseFrontMatter() - Extract and validate YAML front matter ✅ Core Test Coverage (Complete - 16 tests) Content Transformation Tests TestGolden_FrontmatterInjection - Front matter generation with editURL TestGolden_ImagePaths - Image path transformation TestGolden_SectionIndexes - Section index page generation TestGolden_MenuGeneration - Automatic menu generation Repository Tests TestGolden_TwoRepos - Multi-repository aggregation TestGolden_ConflictingPaths - Path conflict resolution TestGolden_CrossRepoLinks - Cross-repository linking Edge Case Tests TestGolden_EmptyDocs - Repository with no markdown files TestGolden_OnlyReadme - Only README.md (should be skipped) TestGolden_MalformedFrontmatter - Invalid YAML front matter TestGolden_DeepNesting - Deep directory nesting (10+ levels) TestGolden_UnicodeNames - Unicode filenames and paths TestGolden_SpecialChars - Special characters in filenames Error Handling Tests TestGolden_Error_InvalidRepository - Invalid repository URL handling TestGolden_Error_InvalidConfig - Empty/minimal configuration TestGolden_Warning_NoGitCommit - Repository without commits Note: All tests use the Relearn theme. Error tests use relearn-basic.yaml as a base configuration.",
    "tags": [
      "Golden-Tests",
      "Implementation",
      "Testing-Strategy"
    ],
    "title": "Golden Test Implementation",
    "uri": "/docs/testing/golden-test-implementation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Golden-Tests",
    "uri": "/tags/golden-tests/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "CI/CD Linting Integration This guide shows how to integrate documentation linting into your CI/CD pipeline for automated validation.\nOverview CI/CD linting provides:\nAutomated validation: Catch issues before merge Consistent enforcement: All PRs validated equally Visible feedback: Clear error messages in PR comments Quality gates: Block merges if docs fail validation Supported Platforms GitHub Actions GitLab CI Jenkins CircleCI Generic CI GitHub Actions Basic Workflow Create .github/workflows/lint-docs.yml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 name: Lint Documentation on: pull_request: paths: - 'docs/**' - '**.md' push: branches: - main jobs: lint: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v4 - name: Setup Go uses: actions/setup-go@v5 with: go-version: '1.21' - name: Install DocBuilder run: | go install github.com/your-org/docbuilder/cmd/docbuilder@latest echo \"$(go env GOPATH)/bin\" \u003e\u003e $GITHUB_PATH - name: Lint Documentation run: | docbuilder lint --format=json \u003e lint-report.json docbuilder lint # Human-readable output - name: Upload Lint Report if: always() uses: actions/upload-artifact@v3 with: name: lint-report path: lint-report.json retention-days: 30 Features:\n✅ Runs on PRs affecting docs ✅ Installs DocBuilder from source ✅ Generates both JSON and text reports ✅ Uploads artifacts for later review Advanced: PR Comments Post lint results directly on PR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 name: Lint Documentation with PR Comments on: pull_request: paths: - 'docs/**' - '**.md' jobs: lint: runs-on: ubuntu-latest permissions: contents: read pull-requests: write # Required for PR comments steps: - uses: actions/checkout@v4 - name: Setup Go uses: actions/setup-go@v5 with: go-version: '1.21' - name: Install DocBuilder run: | go install github.com/your-org/docbuilder/cmd/docbuilder@latest echo \"$(go env GOPATH)/bin\" \u003e\u003e $GITHUB_PATH - name: Lint Documentation id: lint run: | set +e # Don't fail on linting errors docbuilder lint --format=json \u003e lint-report.json LINT_EXIT=$? echo \"exit_code=$LINT_EXIT\" \u003e\u003e $GITHUB_OUTPUT # Generate summary ERROR_COUNT=$(jq '[.issues[] | select(.severity==\"error\")] | length' lint-report.json) WARNING_COUNT=$(jq '[.issues[] | select(.severity==\"warning\")] | length' lint-report.json) echo \"errors=$ERROR_COUNT\" \u003e\u003e $GITHUB_OUTPUT echo \"warnings=$WARNING_COUNT\" \u003e\u003e $GITHUB_OUTPUT exit $LINT_EXIT - name: Comment PR - Success if: steps.lint.outputs.exit_code == '0' uses: actions/github-script@v6 with: script: | github.rest.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: '✅ **Documentation linting passed!**\\n\\nAll files meet linting standards.' }) - name: Comment PR - Warnings if: steps.lint.outputs.exit_code == '1' uses: actions/github-script@v6 with: script: | const warnings = ${{ steps.lint.outputs.warnings }}; github.rest.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: `⚠️ **Documentation has ${warnings} warning(s)**\\n\\nConsider fixing before merge. Run \\`docbuilder lint --fix\\` locally.` }) - name: Comment PR - Errors if: steps.lint.outputs.exit_code == '2' uses: actions/github-script@v6 with: script: | const fs = require('fs'); const report = JSON.parse(fs.readFileSync('lint-report.json', 'utf8')); const errors = report.issues.filter(i =\u003e i.severity === 'error'); let comment = `❌ **Documentation linting failed with ${errors.length} error(s)**\\n\\n`; comment += '### Errors\\n\\n'; errors.slice(0, 10).forEach(issue =\u003e { comment += `- **${issue.file}**\\n`; comment += ` \\`${issue.message}\\`\\n\\n`; }); if (errors.length \u003e 10) { comment += `\\n_...and ${errors.length - 10} more errors. Download full report from artifacts._\\n`; } comment += '\\n**How to fix**:\\n'; comment += '```bash\\n'; comment += 'docbuilder lint --fix\\n'; comment += 'git add -A\\n'; comment += 'git commit -m \"docs: fix linting issues\"\\n'; comment += '```'; github.rest.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: comment }) - name: Upload Report if: always() uses: actions/upload-artifact@v3 with: name: lint-report path: lint-report.json - name: Fail if errors if: steps.lint.outputs.exit_code == '2' run: exit 1 Features:\n✅ Posts summary comment on PR ✅ Shows first 10 errors inline ✅ Provides fix instructions ✅ Fails workflow if errors found ✅ Allows warnings without blocking Auto-Fix on Push Automatically fix issues and commit:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 name: Auto-Fix Documentation on: push: branches: - main paths: - 'docs/**' - '**.md' jobs: auto-fix: runs-on: ubuntu-latest permissions: contents: write # Required to push commits steps: - uses: actions/checkout@v4 with: token: ${{ secrets.GITHUB_TOKEN }} - name: Setup Go uses: actions/setup-go@v5 with: go-version: '1.21' - name: Install DocBuilder run: | go install github.com/your-org/docbuilder/cmd/docbuilder@latest echo \"$(go env GOPATH)/bin\" \u003e\u003e $GITHUB_PATH - name: Auto-Fix Issues run: | docbuilder lint --fix --yes - name: Commit Fixes run: | git config user.name \"docbuilder-bot\" git config user.email \"bot@example.com\" if [[ -n $(git status -s) ]]; then git add -A git commit -m \"docs: auto-fix linting issues [skip ci]\" git push else echo \"No fixes needed\" fi ⚠️ Warning: Auto-fix on push can create unexpected commits. Consider using only for specific branches or requiring review.\nLint Only Changed Files Optimize CI by linting only changed files:\n1 2 3 4 5 6 7 8 9 10 11 12 - name: Get Changed Files id: changed-files run: | git fetch origin ${{ github.base_ref }} CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep -E '\\.(md|markdown)$' || echo \"\") echo \"files=$CHANGED\" \u003e\u003e $GITHUB_OUTPUT echo \"$CHANGED\" \u003e changed_files.txt - name: Lint Changed Files if: steps.changed-files.outputs.files != '' run: | cat changed_files.txt | xargs docbuilder lint GitLab CI Basic Pipeline Add to .gitlab-ci.yml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 stages: - test lint-docs: stage: test image: golang:1.21 before_script: - go install github.com/your-org/docbuilder/cmd/docbuilder@latest - export PATH=$PATH:$(go env GOPATH)/bin script: - docbuilder lint --format=json | tee lint-report.json - docbuilder lint # Human-readable output artifacts: when: always paths: - lint-report.json reports: junit: lint-report.json expire_in: 30 days only: changes: - docs/** - '**/*.md' MR Comments with API Post lint results to merge request:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 lint-docs-with-comments: stage: test image: golang:1.21 before_script: - go install github.com/your-org/docbuilder/cmd/docbuilder@latest - export PATH=$PATH:$(go env GOPATH)/bin - apt-get update \u0026\u0026 apt-get install -y jq curl script: - | set +e docbuilder lint --format=json \u003e lint-report.json LINT_EXIT=$? ERROR_COUNT=$(jq '[.issues[] | select(.severity==\"error\")] | length' lint-report.json) WARNING_COUNT=$(jq '[.issues[] | select(.severity==\"warning\")] | length' lint-report.json) if [ \"$LINT_EXIT\" -eq 2 ]; then STATUS=\"❌ **Linting failed** with $ERROR_COUNT error(s)\" elif [ \"$LINT_EXIT\" -eq 1 ]; then STATUS=\"⚠️ **Linting passed** with $WARNING_COUNT warning(s)\" else STATUS=\"✅ **Linting passed**\" fi COMMENT=\"$STATUS\\n\\nRun \\`docbuilder lint --fix\\` to auto-fix issues.\" # Post comment using GitLab API curl --request POST \\ --header \"PRIVATE-TOKEN: $CI_JOB_TOKEN\" \\ --header \"Content-Type: application/json\" \\ --data \"{\\\"body\\\": \\\"$COMMENT\\\"}\" \\ \"$CI_API_V4_URL/projects/$CI_PROJECT_ID/merge_requests/$CI_MERGE_REQUEST_IID/notes\" exit $LINT_EXIT artifacts: when: always paths: - lint-report.json only: - merge_requests Auto-Fix on Main 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 auto-fix-docs: stage: fix image: golang:1.21 before_script: - go install github.com/your-org/docbuilder/cmd/docbuilder@latest - export PATH=$PATH:$(go env GOPATH)/bin - git config user.name \"DocBuilder Bot\" - git config user.email \"bot@example.com\" script: - docbuilder lint --fix --yes - | if [[ -n $(git status -s) ]]; then git add -A git commit -m \"docs: auto-fix linting issues [skip ci]\" git push \"https://oauth2:${CI_JOB_TOKEN}@${CI_SERVER_HOST}/${CI_PROJECT_PATH}.git\" HEAD:main fi only: - main when: on_success Jenkins Pipeline Configuration Create Jenkinsfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 pipeline { agent any environment { GOPATH = \"${WORKSPACE}/go\" PATH = \"${PATH}:${GOPATH}/bin\" } stages { stage('Setup') { steps { sh 'go install github.com/your-org/docbuilder/cmd/docbuilder@latest' } } stage('Lint Documentation') { steps { script { def lintStatus = sh( script: 'docbuilder lint --format=json \u003e lint-report.json \u0026\u0026 docbuilder lint', returnStatus: true ) archiveArtifacts artifacts: 'lint-report.json', allowEmptyArchive: false if (lintStatus == 2) { error(\"Documentation linting failed with errors\") } else if (lintStatus == 1) { unstable(\"Documentation has warnings\") } } } } } post { always { publishHTML([ allowMissing: false, alwaysLinkToLastBuild: true, keepAll: true, reportDir: '.', reportFiles: 'lint-report.json', reportName: 'Lint Report' ]) } } } CircleCI Configuration Create .circleci/config.yml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 version: 2.1 jobs: lint-docs: docker: - image: cimg/go:1.21 steps: - checkout - restore_cache: keys: - go-mod-v1-{{ checksum \"go.sum\" }} - run: name: Install DocBuilder command: | go install github.com/your-org/docbuilder/cmd/docbuilder@latest - save_cache: key: go-mod-v1-{{ checksum \"go.sum\" }} paths: - \"/home/circleci/go/pkg/mod\" - run: name: Lint Documentation command: | docbuilder lint --format=json \u003e lint-report.json docbuilder lint - store_artifacts: path: lint-report.json destination: lint-report - store_test_results: path: lint-report.json workflows: version: 2 lint: jobs: - lint-docs: filters: branches: ignore: - gh-pages Generic CI Systems Docker-Based Approach For any CI that supports Docker:\n1 2 3 4 5 6 7 8 9 10 # Dockerfile.lint FROM golang:1.21-alpine RUN apk add --no-cache git RUN go install github.com/your-org/docbuilder/cmd/docbuilder@latest WORKDIR /workspace ENTRYPOINT [\"docbuilder\", \"lint\"] Build image:\n1 docker build -f Dockerfile.lint -t docbuilder-lint:latest . Use in CI:\n1 2 # Generic CI script docker run --rm -v $(pwd):/workspace docbuilder-lint:latest --format=json \u003e lint-report.json Shell Script Approach For CI without Docker support:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #!/bin/bash # lint-docs.sh set -e # Install Go if needed if ! command -v go \u0026\u003e /dev/null; then echo \"Installing Go...\" wget https://go.dev/dl/go1.21.0.linux-amd64.tar.gz tar -C /usr/local -xzf go1.21.0.linux-amd64.tar.gz export PATH=$PATH:/usr/local/go/bin fi # Install DocBuilder echo \"Installing DocBuilder...\" go install github.com/your-org/docbuilder/cmd/docbuilder@latest export PATH=$PATH:$(go env GOPATH)/bin # Run linting echo \"Linting documentation...\" docbuilder lint --format=json \u003e lint-report.json docbuilder lint # Check exit code LINT_EXIT=$? if [ $LINT_EXIT -eq 2 ]; then echo \"❌ Linting failed with errors\" exit 1 elif [ $LINT_EXIT -eq 1 ]; then echo \"⚠️ Linting passed with warnings\" exit 0 else echo \"✅ Linting passed\" exit 0 fi Advanced Patterns Parallel Linting Lint multiple directories in parallel:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # GitHub Actions jobs: lint-api-docs: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - # ... setup steps - run: docbuilder lint docs/api/ lint-guides: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - # ... setup steps - run: docbuilder lint docs/guides/ lint-reference: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - # ... setup steps - run: docbuilder lint docs/reference/ Conditional Enforcement Different rules for different branches:\n1 2 3 4 5 6 7 - name: Lint (Strict on Main) if: github.ref == 'refs/heads/main' run: docbuilder lint # Fail on any error - name: Lint (Relaxed on Feature Branches) if: github.ref != 'refs/heads/main' run: docbuilder lint || true # Don't block Scheduled Deep Scans Run comprehensive linting weekly:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 name: Weekly Documentation Audit on: schedule: - cron: '0 2 * * 0' # Sunday 2am jobs: audit: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - # ... setup steps - name: Full Lint run: | docbuilder lint --format=json \u003e weekly-audit.json - name: Generate Report run: | jq -r '.issues[] | \"\\(.severity): \\(.file) - \\(.message)\"' weekly-audit.json \u003e weekly-report.txt - name: Email Report uses: dawidd6/action-send-mail@v3 with: server_address: smtp.example.com server_port: 465 username: ${{ secrets.MAIL_USERNAME }} password: ${{ secrets.MAIL_PASSWORD }} subject: Weekly Documentation Audit to: docs-team@example.com from: CI Bot attachments: weekly-report.txt Performance Optimization Cache Dependencies 1 2 3 4 5 6 7 8 9 10 # GitHub Actions - name: Cache Go modules uses: actions/cache@v3 with: path: | ~/.cache/go-build ~/go/pkg/mod key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }} restore-keys: | ${{ runner.os }}-go- Incremental Linting Only lint changed files:\n1 2 3 4 5 6 7 8 9 10 11 12 - name: Get changed markdown files id: changed-files uses: tj-actions/changed-files@v39 with: files: | **/*.md **/*.markdown - name: Lint changed files if: steps.changed-files.outputs.any_changed == 'true' run: | echo \"${{ steps.changed-files.outputs.all_changed_files }}\" | xargs docbuilder lint Matrix Builds Test against multiple DocBuilder versions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 jobs: lint: strategy: matrix: docbuilder-version: ['latest', 'v1.0.0', 'v1.1.0'] runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Install DocBuilder ${{ matrix.docbuilder-version }} run: | go install github.com/your-org/docbuilder/cmd/docbuilder@${{ matrix.docbuilder-version }} - name: Lint run: docbuilder lint Monitoring and Metrics Track Lint Success Rate 1 2 3 4 5 6 7 8 9 10 11 12 - name: Record Metrics if: always() run: | LINT_EXIT=$? curl -X POST https://metrics.example.com/api/lint \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"repo\\\": \\\"${{ github.repository }}\\\", \\\"pr\\\": \\\"${{ github.event.pull_request.number }}\\\", \\\"exit_code\\\": $LINT_EXIT, \\\"timestamp\\\": \\\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\" }\" Dashboard Integration Export metrics to dashboard tools:\n1 2 3 4 5 6 7 8 - name: Export to DataDog if: always() run: | ERROR_COUNT=$(jq '[.issues[] | select(.severity==\"error\")] | length' lint-report.json) WARNING_COUNT=$(jq '[.issues[] | select(.severity==\"warning\")] | length' lint-report.json) echo \"lint.errors:$ERROR_COUNT|g|#repo:${{ github.repository }}\" | nc -u -w1 datadog-agent 8125 echo \"lint.warnings:$WARNING_COUNT|g|#repo:${{ github.repository }}\" | nc -u -w1 datadog-agent 8125 Troubleshooting CI Issue: CI Timeout Solution: Lint only changed files or increase timeout\n1 2 3 - name: Lint with timeout timeout-minutes: 10 run: docbuilder lint Issue: False Positives in CI Solution: Ensure consistent environment\n1 2 3 4 5 - name: Normalize line endings run: git config core.autocrlf false - name: Lint run: docbuilder lint Issue: Secrets in Error Messages Solution: Sanitize output\n1 2 3 - name: Lint run: | docbuilder lint 2\u003e\u00261 | sed 's/${{ secrets.TOKEN }}/***REDACTED***/g' Best Practices Start permissive: Allow warnings initially, tighten later Fast feedback: Lint only changed files in PRs Clear messages: Use PR comments for actionable feedback Auto-fix carefully: Only on trusted branches Monitor trends: Track lint success rates over time Document exceptions: Explain any --no-verify usage Version pin: Use specific DocBuilder version in CI Next Steps Review Lint Rules Reference for complete rule list See Setup Linting for local development See Migration Guide for adopting in existing repos Read ADR-005 for design decisions CI Integration Checklist:\nBasic workflow runs on PRs Artifacts uploaded for review PR comments provide feedback Failures block merge Warnings don’t block (optional) Team notified of new checks Documentation updated",
    "description": "CI/CD Linting Integration This guide shows how to integrate documentation linting into your CI/CD pipeline for automated validation.\nOverview CI/CD linting provides:\nAutomated validation: Catch issues before merge Consistent enforcement: All PRs validated equally Visible feedback: Clear error messages in PR comments Quality gates: Block merges if docs fail validation Supported Platforms GitHub Actions GitLab CI Jenkins CircleCI Generic CI GitHub Actions Basic Workflow Create .github/workflows/lint-docs.yml:",
    "tags": [
      "Ci-Cd",
      "Linting",
      "Automation",
      "Github-Actions",
      "Gitlab-Ci"
    ],
    "title": "How To: CI/CD Linting Integration",
    "uri": "/docs/how-to/ci-cd-linting/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Migrate Existing Repository to Linting This guide walks you through adopting documentation linting in an existing repository with legacy content.\nOverview Existing documentation repositories often have:\nMixed filename conventions (spaces, uppercase, underscores) Inconsistent directory structures Broken or stale links Orphaned assets This guide provides a step-by-step process to clean up existing content and adopt linting practices.\nMigration Strategy Phase 1: Assessment (Day 1) Run initial scan to understand current state.\nStep 1: Run Discovery Scan 1 2 cd your-repository docbuilder lint --format=json \u003e lint-assessment.json This generates a complete report of all issues without blocking.\nStep 2: Analyze Results 1 2 3 4 5 6 7 8 # View human-readable summary docbuilder lint # Count issues by type jq '.issues | group_by(.rule) | map({rule: .[0].rule, count: length})' lint-assessment.json # Find most problematic directories jq -r '.issues[].file' lint-assessment.json | xargs -n1 dirname | sort | uniq -c | sort -rn Expected output:\nResults: 147 files scanned 89 errors (blocks build) 23 warnings (should fix) Most common issues: • 45 files with spaces in filename • 32 files with uppercase letters • 12 files with invalid double extensions Step 3: Categorize Work Create fix plan based on severity:\nPriority Issue Type Count Effort P0 Spaces in filenames 45 Auto-fix P0 Uppercase letters 32 Auto-fix P0 Special characters 7 Auto-fix P1 Double extensions 12 Manual review P2 Missing section indexes 8 Manual creation P3 Orphaned assets 15 Manual cleanup Phase 2: Automated Cleanup (Day 1-2) Use auto-fix to handle the majority of issues.\nStep 1: Preview Auto-Fix 1 2 3 4 5 # Dry-run to see what would change docbuilder lint --fix --dry-run \u003e fix-preview.txt # Review the preview less fix-preview.txt Example preview:\nDRY RUN: No changes will be applied FILE RENAMES: docs/API Guide.md → docs/api-guide.md docs/User Manual.md → docs/user-manual.md docs/Getting Started.md → docs/getting-started.md images/Screenshot 2024.png → images/screenshot-2024.png CONTENT UPDATES: docs/index.md Line 15: [API Guide](./API Guide.md) → [API Guide](/docs/how-to/api-guide) Line 23: ![Screenshot](/docs/how-to/images/screenshot 2024.png) → ![Screenshot](/docs/how-to/images/screenshot-2024.png) docs/tutorials/quickstart.md Line 8: See [Getting Started](../Getting Started.md) → See [Getting Started](/docs/getting-started) Summary: 45 files would be renamed 87 links would be updated across 23 files Step 2: Apply Auto-Fix 1 2 3 4 # Interactive mode (recommended first time) docbuilder lint --fix # Confirm when prompted Expected output:\nFound 45 files with naming issues: Files to rename: 45 Links to update: 87 links in 23 files This will: ✓ Rename 45 files using git mv (preserves history) ✓ Update 87 internal links ✓ Create backup: .docbuilder-backup-20251229-143052/ Proceed with fixes? [y/N]: y Fixing issues... ✓ Renamed 45 files ✓ Updated 87 links in 23 files ✓ All changes applied successfully Run 'git status' to review changes. Step 3: Review Changes 1 2 3 4 5 6 7 8 9 # Check Git status git status # Review specific changes git diff docs/index.md git diff --name-status # Verify no broken links find docs/ -name \"*.md\" -exec grep -l \"API Guide.md\" {} \\; # Should be empty Step 4: Test Build 1 2 3 4 5 6 # Verify Hugo build works cd your-repository docbuilder build -c config.yaml -o /tmp/test-build # Or if using Hugo directly cd docs \u0026\u0026 hugo server Visit http://localhost:1313 and verify:\n✅ All pages render correctly ✅ Navigation works ✅ Links work ✅ Images display Step 5: Commit Automated Fixes 1 2 3 4 5 6 7 8 git add -A git commit -m \"docs: normalize filenames for linting compliance - Rename files to lowercase with hyphens (45 files) - Update internal links to renamed files (87 links) - Preserve Git history with git mv Generated by: docbuilder lint --fix\" Phase 3: Manual Cleanup (Day 2-3) Handle issues that require manual intervention.\nIssue: Invalid Double Extensions Example: api-guide.md.backup, notes.markdown.old\nResolution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Find all double extension files find docs/ -name \"*.md.*\" -o -name \"*.markdown.*\" # Review and remove backup files rm docs/api-guide.md.backup rm docs/notes.markdown.old # Add to .gitignore echo \"*.backup\" \u003e\u003e .gitignore echo \"*.old\" \u003e\u003e .gitignore echo \"*.tmp\" \u003e\u003e .gitignore git add -A git commit -m \"docs: remove backup files and update .gitignore\" Issue: Reserved Filenames Example: docs/tags.md, docs/categories.md\nResolution:\n1 2 3 4 5 6 7 8 9 10 # Rename with prefix git mv docs/tags.md docs/content-tags.md git mv docs/categories.md docs/content-categories.md # Update any links grep -r \"tags.md\" docs/ --include=\"*.md\" # Manually update found references git add -A git commit -m \"docs: rename reserved filenames to avoid Hugo conflicts\" Issue: Missing Section Indexes Example: docs/api/ directory has no _index.md\nResolution:\nCreate docs/api/_index.md:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- title: \"API Documentation\" weight: 2 description: \"Complete API reference and guides\" --- # API Documentation This section contains comprehensive API documentation including: - Authentication and authorization - REST API endpoints - GraphQL schema - SDKs and client libraries 1 2 git add docs/api/_index.md git commit -m \"docs: add missing section index for API documentation\" Issue: Broken Internal Links Example: Links to moved or deleted files\nResolution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Find all broken links (requires build or manual check) docbuilder lint # Will report broken links # For each broken link, either: # 1. Fix the target path # 2. Remove the link # 3. Create the missing target file # Example fix vim docs/index.md # Change: [Old Guide](/docs/how-to/removed-doc) # To: [New Guide](/docs/how-to/current-doc) git add docs/index.md git commit -m \"docs: fix broken link to renamed documentation\" Issue: Orphaned Assets Example: Images not referenced by any documentation\nResolution:\n1 2 3 4 5 6 7 8 # Find orphaned images (requires custom script or manual review) # For each orphaned file, either: # 1. Reference it in documentation # 2. Remove if truly unused # Example removal git rm docs/images/old-screenshot.png git commit -m \"docs: remove orphaned screenshot\" Phase 4: Git Hooks Setup (Day 3) Prevent future issues by installing pre-commit hooks.\nOption A: Lefthook (Recommended) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Install lefthook brew install lefthook # macOS # or go install github.com/evilmartians/lefthook@latest # Create lefthook.yml cat \u003e lefthook.yml \u003c\u003c 'EOF' pre-commit: parallel: true commands: lint-docs: glob: \"*.{md,markdown,png,jpg,jpeg,gif,svg}\" run: docbuilder lint {staged_files} --quiet stage_fixed: true EOF # Install hooks lefthook install # Test echo \"test\" \u003e\u003e docs/Test File.md # Invalid filename git add docs/Test\\ File.md git commit -m \"test\" # Should fail Option B: Traditional Hook 1 2 3 4 5 6 7 # Install pre-commit hook docbuilder lint install-hook # Test echo \"test\" \u003e\u003e docs/Invalid\\ Name.md git add docs/Invalid\\ Name.md git commit -m \"test\" # Should fail Commit hook configuration:\n1 2 git add lefthook.yml # or .git/hooks/pre-commit git commit -m \"chore: add documentation linting pre-commit hook\" Phase 5: Team Rollout (Week 1-2) Communicate changes and help team adopt.\nStep 1: Announce Migration Email template:\nSubject: Documentation Linting Now Active Hi team, We've cleaned up our documentation and enabled automatic linting to maintain quality going forward. What changed: • All docs now use lowercase, hyphen-separated filenames • Git hooks validate documentation before commit • CI/CD pipeline checks will catch any issues What you need to do: 1. Pull latest changes: git pull 2. Install hooks: lefthook install (or docbuilder lint install-hook) 3. Test: lefthook run pre-commit --verbose Resources: • Setup guide: docs/how-to/setup-linting.md • Rule reference: docs/reference/lint-rules.md • Questions: #docs-tooling Slack channel Thanks! Step 2: Setup Guide for Team Create docs/CONTRIBUTING.md (if not exists):\n1 2 3 4 5 6 7 8 9 10 11 # Contributing to Documentation ## Setup (One-Time) 1. Install lefthook: - macOS: `brew install lefthook` - Linux: `go install github.com/evilmartians/lefthook@latest` 2. Install hooks: ```bash lefthook install Verify: 1 lefthook run pre-commit --verbose Writing Documentation Filename Rules ✅ Do: Use lowercase with hyphens\napi-guide.md getting-started.md user-manual.md ❌ Don’t: Use spaces, uppercase, or special characters\nAPI Guide.md # Uppercase getting_started.md # Underscore user manual.md # Spaces Auto-Fix If linting fails on commit:\n1 2 3 4 docbuilder lint --fix --dry-run # Preview docbuilder lint --fix # Apply git add -A git commit -m \"your message\" Bypassing Hooks Only in emergencies:\n1 git commit --no-verify -m \"emergency fix\" ```bash git add docs/CONTRIBUTING.md git commit -m \"docs: add documentation contribution guidelines\" Step 3: Monitor Adoption First week: Track metrics\n1 2 3 4 5 6 # Count commits that passed linting git log --since=\"1 week ago\" --grep=\"docs:\" --oneline | wc -l # Check for --no-verify usage git log --since=\"1 week ago\" --all --source --full-history \\ --grep=\"no-verify\" | wc -l Adjust if needed:\nToo many complaints: Review if rules too strict People bypassing: Improve auto-fix or documentation Confusion: Hold Q\u0026A session Phase 6: CI/CD Integration (Week 2) Add automated validation to CI/CD pipeline.\nGitHub Actions Create .github/workflows/lint-docs.yml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 name: Lint Documentation on: pull_request: paths: - 'docs/**' - '**.md' jobs: lint: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Setup Go uses: actions/setup-go@v5 with: go-version: '1.21' - name: Install DocBuilder run: go install github.com/your-org/docbuilder/cmd/docbuilder@latest - name: Lint Documentation run: | docbuilder lint --format=json \u003e lint-report.json docbuilder lint # Human-readable output - name: Upload Report if: always() uses: actions/upload-artifact@v3 with: name: lint-report path: lint-report.json 1 2 git add .github/workflows/lint-docs.yml git commit -m \"ci: add documentation linting workflow\" GitLab CI Add to .gitlab-ci.yml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 lint-docs: stage: test image: golang:1.21 script: - go install github.com/your-org/docbuilder/cmd/docbuilder@latest - docbuilder lint --format=json | tee lint-report.json - docbuilder lint # Human-readable artifacts: when: always paths: - lint-report.json reports: junit: lint-report.json only: changes: - docs/** - '**/*.md' 1 2 git add .gitlab-ci.yml git commit -m \"ci: add documentation linting to CI pipeline\" Common Migration Scenarios Scenario 1: Large Repository (1000+ Docs) Challenge: Too many files to review manually\nSolution: Incremental approach\n1 2 3 4 5 6 7 8 9 10 # Fix one directory at a time docbuilder lint --fix docs/api/ git add docs/api/ git commit -m \"docs(api): normalize filenames\" docbuilder lint --fix docs/guides/ git add docs/guides/ git commit -m \"docs(guides): normalize filenames\" # Continue for each top-level directory Scenario 2: Active Development Challenge: Many concurrent PRs\nSolution: Coordinate freeze window\n1 2 3 4 5 1. Announce: \"Docs cleanup Friday 2-4pm, merge PRs before then\" 2. Friday 2pm: Merge all pending docs PRs 3. Friday 2-4pm: Run migration, commit changes 4. Friday 4pm: Team pulls latest, installs hooks 5. Monday: CI enforcement active Scenario 3: Multi-Repo Organization Challenge: Dozens of repositories\nSolution: Batch automation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash # migrate-all-repos.sh REPOS=( \"project-docs\" \"api-docs\" \"platform-docs\" ) for repo in \"${REPOS[@]}\"; do echo \"Migrating $repo...\" cd \"$repo\" # Run migration docbuilder lint --fix --yes # Commit if changes if [[ -n $(git status -s) ]]; then git add -A git commit -m \"docs: normalize filenames for linting compliance\" git push fi cd .. done Scenario 4: External Contributors Challenge: Contributors don’t have DocBuilder installed\nSolution: CI-only mode\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # .github/workflows/lint-docs.yml - name: Auto-fix on PR if: github.event_name == 'pull_request' run: | docbuilder lint --fix --yes # Commit fixes if needed if [[ -n $(git status -s) ]]; then git config user.name \"docbuilder-bot\" git config user.email \"bot@example.com\" git add -A git commit -m \"docs: auto-fix linting issues\" git push fi Rollback Plan If migration causes issues:\nQuick Rollback 1 2 3 4 5 6 7 8 9 10 11 # Revert the migration commit git revert \u003cmigration-commit-sha\u003e git push # Uninstall hooks rm .git/hooks/pre-commit # or lefthook uninstall # Notify team echo \"Linting temporarily disabled, investigating issues\" Gradual Rollback 1 2 3 4 5 6 7 8 9 10 # Change hooks to non-blocking # In lefthook.yml: pre-commit: commands: lint-docs: run: docbuilder lint {staged_files} --quiet || true # Don't block # In .github/workflows/lint-docs.yml: - name: Lint Documentation continue-on-error: true # Don't fail CI Success Metrics Track these metrics over 30 days:\nMetric Target Actual Commits passing linting \u003e90% ___ CI failures from docs \u003c5% ___ –no-verify usage \u003c10% ___ Time to fix issues \u003c5 min avg ___ Team satisfaction \u003e4/5 ___ Survey questions (after 2 weeks):\nHow easy was linting setup? (1-5) Do error messages help you fix issues? (1-5) Has linting improved docs quality? (Yes/No) Suggestions for improvement? (Open) Troubleshooting Migration Issue: Too Many Errors to Fix Solution: Use gradual enforcement\n1 2 3 4 5 6 7 # Week 1: Warnings only # In CI config, add: || true # Week 2: Block new files only # Custom script to check only changed files # Week 3: Full enforcement Issue: Team Resistance Solution: Show value\nDemonstrate auto-fix saving time Show before/after build reliability Highlight caught issues (broken links, etc.) Issue: Performance Issues Solution: Optimize linting scope\n1 2 3 4 5 6 7 8 9 # Only lint changed files in CI - name: Get changed files run: | git diff --name-only origin/main...HEAD \u003e changed_files.txt grep -E '\\.(md|markdown)$' changed_files.txt || true - name: Lint changed files only run: | cat changed_files.txt | xargs docbuilder lint Next Steps After successful migration:\n✅ All error-level issues resolved ✅ Team has hooks installed ✅ CI/CD enforcing linting 📖 Review Lint Rules Reference 📖 Consider CI/CD Integration enhancements 📊 Track metrics and iterate Additional Resources Setup Linting - Detailed setup guide Lint Rules Reference - All rules documented CI/CD Integration - Automated validation ADR-005: Documentation Linting - Design decisions Migration Duration: 1-2 weeks for most repositories\nEffort: ~1 day hands-on, rest is coordination\nROI: Improved docs quality, fewer build failures, better developer experience",
    "description": "Migrate Existing Repository to Linting This guide walks you through adopting documentation linting in an existing repository with legacy content.\nOverview Existing documentation repositories often have:\nMixed filename conventions (spaces, uppercase, underscores) Inconsistent directory structures Broken or stale links Orphaned assets This guide provides a step-by-step process to clean up existing content and adopt linting practices.",
    "tags": [
      "Linting",
      "Migration",
      "Cleanup"
    ],
    "title": "How To: Migrate Existing Repository to Linting",
    "uri": "/docs/how-to/migrate-to-linting/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Setup Documentation Linting This guide explains how to set up documentation linting in your repository to catch issues before commit and during CI/CD.\nOverview DocBuilder’s linting system validates documentation files against Hugo and DocBuilder best practices, catching common issues like:\nInvalid filenames (spaces, uppercase, special characters) Malformed frontmatter Broken internal links Invalid double extensions (except whitelisted .drawio.png, .drawio.svg) Linting can run:\nManually: docbuilder lint command Pre-commit: Git hooks (lefthook or traditional) CI/CD: GitHub Actions, GitLab CI, etc. Prerequisites DocBuilder installed (go install or download binary) Git repository with documentation (usually in docs/ or documentation/) Manual Linting Basic Usage Run linting from your repository root:\n1 2 3 4 5 6 7 8 # Auto-detects docs/ or documentation/ directory docbuilder lint # Lint specific directory docbuilder lint ./docs # Lint current directory explicitly docbuilder lint . Understanding Output Linting documentation in: ./docs ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ ✗ docs/API Guide.md ERROR: Invalid filename └─ Contains uppercase letters and spaces Current: docs/API Guide.md Suggested: docs/api-guide.md ✓ docs/getting-started.md ✓ docs/images/architecture.drawio.png (whitelisted) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Results: 3 files scanned 1 error (blocks build) 0 warnings Exit Codes 0: No issues (clean) 1: Warnings present (should fix) 2: Errors found (blocks build) 3: Execution error (filesystem access, etc.) Auto-Fix Automatically fix common issues:\n1 2 3 4 5 6 7 8 9 10 11 # Dry-run mode (preview changes) docbuilder lint --fix --dry-run # Interactive mode (prompts for confirmation) docbuilder lint --fix # Non-interactive mode (CI/automation) docbuilder lint --fix --yes # Force overwrite existing files docbuilder lint --fix --force Auto-fix will:\nRename files to lowercase with hyphens Update all internal links to renamed files Preserve Git history with git mv Show detailed report of changes Output Formats 1 2 3 4 5 6 7 8 9 10 11 # Human-readable (default) docbuilder lint # JSON for CI/CD integration docbuilder lint --format=json \u003e lint-report.json # Quiet mode (errors only) docbuilder lint --quiet # Verbose mode (detailed output) docbuilder lint --verbose Git Hooks Integration Option 1: Lefthook (Recommended) Lefthook is a fast, modern Git hooks manager with better performance and easier configuration.\nInstallation macOS:\n1 brew install lefthook Linux:\n1 2 3 4 5 6 # Using Go go install github.com/evilmartians/lefthook@latest # Or download binary from releases curl -1sLf 'https://dl.cloudsmith.io/public/evilmartians/lefthook/setup.deb.sh' | sudo -E bash sudo apt install lefthook Windows:\n1 scoop install lefthook Configuration Create lefthook.yml in your repository root:\n1 2 3 4 5 6 7 8 # lefthook.yml pre-commit: parallel: true commands: lint-docs: glob: \"*.{md,markdown,png,jpg,jpeg,gif,svg}\" run: docbuilder lint {staged_files} --quiet stage_fixed: true # Auto-stage files fixed with --fix Setup in Repository 1 2 3 4 5 # Install hooks (one-time per clone) lefthook install # Verify installation lefthook run pre-commit --verbose With Auto-Fix To automatically fix issues on commit:\n1 2 3 4 5 6 7 8 # lefthook.yml pre-commit: parallel: true commands: lint-docs: glob: \"*.{md,markdown,png,jpg,jpeg,gif,svg}\" run: docbuilder lint {staged_files} --fix --yes --quiet stage_fixed: true Workflow 1 2 3 4 5 6 7 8 9 10 11 12 # Make changes to documentation vim docs/API Guide.md # Commit triggers automatic linting git add docs/ git commit -m \"Add API documentation\" # Lefthook runs automatically Lefthook \u003e pre-commit \u003e lint-docs: ✓ All staged files pass linting [main a1b2c3d] Add API documentation Option 2: Traditional Pre-Commit Hook For repositories that prefer traditional Git hooks without additional tools.\nInstallation 1 2 3 4 5 6 # Automated installation docbuilder lint install-hook # Manual installation curl -o .git/hooks/pre-commit https://raw.githubusercontent.com/your-org/docbuilder/main/scripts/pre-commit-hook.sh chmod +x .git/hooks/pre-commit Manual Hook Setup Create .git/hooks/pre-commit:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/sh # DocBuilder documentation linting pre-commit hook # Only lint staged markdown and asset files STAGED_FILES=$(git diff --cached --name-only --diff-filter=ACM | grep -E '\\.(md|markdown|png|jpg|jpeg|gif|svg)$') if [ -n \"$STAGED_FILES\" ]; then echo \"Linting staged documentation files...\" docbuilder lint $STAGED_FILES --quiet LINT_EXIT=$? if [ $LINT_EXIT -eq 2 ]; then echo \"\" echo \"❌ Lint errors found. Commit blocked.\" echo \"Fix errors or run: docbuilder lint --fix\" exit 1 elif [ $LINT_EXIT -eq 1 ]; then echo \"\" echo \"⚠️ Lint warnings present. Consider fixing before commit.\" echo \"To auto-fix: docbuilder lint --fix\" # Allow commit but show warning exit 0 fi fi exit 0 Make executable:\n1 chmod +x .git/hooks/pre-commit Comparison: Lefthook vs Traditional Hooks Feature Lefthook Traditional Hook Installation Single command: lefthook install Manual script creation Configuration Checked into repo (lefthook.yml) Not in repo (.git/hooks/) Performance Parallel execution Sequential Portability Works across team (committed config) Each clone needs manual setup Auto-staging Built-in (stage_fixed: true) Manual implementation Maintenance Easy to update (edit lefthook.yml) Edit script in each clone Dependencies Requires lefthook binary Only Git Recommendation: Use lefthook for better developer experience and maintainability.\nSkipping Hooks (When Needed) Sometimes you need to commit without running hooks:\n1 2 3 4 5 # Skip all hooks git commit --no-verify -m \"Emergency hotfix\" # With lefthook, skip specific commands LEFTHOOK_EXCLUDE=lint-docs git commit -m \"Skip linting\" ⚠️ Warning: Skipping linting may cause CI failures. Use sparingly.\nTeam Adoption Gradual Rollout Week 1: Introduce linting\n1 2 3 4 # Team members run manually docbuilder lint docbuilder lint --fix --dry-run # Show what would change docbuilder lint --fix # Apply fixes Week 2: Add lefthook configuration\n1 2 3 4 5 # Start with warnings only (non-blocking) pre-commit: commands: lint-docs: run: docbuilder lint {staged_files} --quiet || true # Don't block commits Week 3: Enable enforcement\n1 2 3 4 5 # Remove || true to block commits with errors pre-commit: commands: lint-docs: run: docbuilder lint {staged_files} --quiet Team Setup Guide Share this with your team:\n1 2 3 4 5 6 7 8 9 10 11 ## Documentation Linting Setup (One-Time) 1. Install lefthook: - macOS: `brew install lefthook` - Linux: `go install github.com/evilmartians/lefthook@latest` - Windows: `scoop install lefthook` 2. Install hooks in your clone: ```bash cd your-repo lefthook install Verify it works:\n1 lefthook run pre-commit --verbose Clean up existing docs (one-time):\n1 2 3 4 docbuilder lint --fix --dry-run # Preview changes docbuilder lint --fix # Apply fixes git add -A git commit -m \"docs: normalize filenames for linting\" That’s it! Linting now runs automatically on every commit.\n## Troubleshooting ### Issue: \"docbuilder: command not found\" **Solution**: Add DocBuilder to PATH: ```bash # Add to ~/.bashrc or ~/.zshrc export PATH=\"$PATH:$(go env GOPATH)/bin\" # Or install globally sudo cp $(which docbuilder) /usr/local/bin/ Issue: Hook doesn’t run on commit Lefthook:\n1 2 3 4 5 # Reinstall hooks lefthook install # Check status lefthook run pre-commit --verbose Traditional hook:\n1 2 3 # Verify hook exists and is executable ls -la .git/hooks/pre-commit chmod +x .git/hooks/pre-commit Issue: Too many warnings/errors in existing repo Solution: Bulk fix existing issues:\n1 2 3 4 5 6 7 8 9 # Preview all fixes docbuilder lint --fix --dry-run # Review changes and apply docbuilder lint --fix # Commit cleanup git add -A git commit -m \"docs: normalize filenames and fix linting issues\" Issue: Lefthook slows down commits Solution: Optimize configuration:\n1 2 3 4 5 6 7 8 9 pre-commit: parallel: true # Run commands in parallel commands: lint-docs: glob: \"*.{md,markdown}\" # Only markdown files run: docbuilder lint {staged_files} --quiet skip: - merge # Skip during merge commits - rebase # Skip during rebase Issue: Need to commit despite linting errors 1 2 3 4 5 # Emergency bypass (use sparingly) git commit --no-verify -m \"Hotfix: critical issue\" # Or with lefthook LEFTHOOK=0 git commit -m \"Skip all hooks\" Next Steps See Lint Rules Reference for complete rule documentation See CI/CD Linting Integration for automated validation See Migration Guide for cleaning up existing repositories Additional Resources Lefthook Documentation Git Hooks Tutorial Hugo URL Management ADR-005: Documentation Linting",
    "description": "Setup Documentation Linting This guide explains how to set up documentation linting in your repository to catch issues before commit and during CI/CD.\nOverview DocBuilder’s linting system validates documentation files against Hugo and DocBuilder best practices, catching common issues like:\nInvalid filenames (spaces, uppercase, special characters) Malformed frontmatter Broken internal links Invalid double extensions (except whitelisted .drawio.png, .drawio.svg) Linting can run:",
    "tags": [
      "Linting",
      "Validation",
      "Git-Hooks",
      "Developer-Experience"
    ],
    "title": "How To: Setup Documentation Linting",
    "uri": "/docs/how-to/setup-linting/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: How-To",
    "uri": "/categories/how-to/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Implementation",
    "uri": "/tags/implementation/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "This document tracks changes to linting rules across DocBuilder versions, helping teams understand when rules were added, changed, or deprecated.\nVersion History v1.0.0 (2025-12-29) - Initial Release Initial linting implementation with core validation rules.\nFilename Rules (Errors) Added:\n✅ Uppercase letter detection ✅ Space character detection ✅ Special character detection (not [a-z0-9-_.]) ✅ Leading/trailing hyphen and underscore detection ✅ Invalid double extension detection ✅ Reserved filename detection (tags.md, categories.md) Whitelisted Extensions:\n✅ .drawio.png - Draw.io embedded PNG diagrams ✅ .drawio.svg - Draw.io embedded SVG diagrams Standard File Exclusions:\n✅ README.md ✅ CONTRIBUTING.md ✅ CHANGELOG.md ✅ LICENSE.md ✅ CODE_OF_CONDUCT.md Auto-Fix Capabilities Added:\n✅ File renaming with git mv support ✅ Lowercase conversion ✅ Space to hyphen replacement ✅ Special character to hyphen replacement ✅ Leading/trailing character stripping ✅ Internal link resolution and updates ✅ Anchor fragment preservation in links ✅ Reference-style link updates ✅ Image link updates ✅ Dry-run mode (--fix --dry-run) ✅ Interactive confirmation prompts ✅ Detailed fix reports Link Resolution Added:\n✅ Relative path resolution (e.g., ./file.md, ../dir/file.md) ✅ Hugo site-absolute path support (e.g., /docs/file) ✅ Inline link detection and updates ✅ Reference-style link detection and updates ✅ Image link detection and updates ✅ Anchor fragment preservation (#section) ✅ External URL exclusion (don’t modify https://...) ✅ Code block exclusion (don’t modify links in code) ✅ Broken link detection and reporting CLI Features Added:\n✅ docbuilder lint [path] command ✅ Intelligent path detection (docs/, documentation/, fallback to .) ✅ Exit codes: 0 (clean), 1 (warnings), 2 (errors), 3 (failure) ✅ Output formats: --format=text|json ✅ Verbosity: --quiet, --verbose ✅ Color support with NO_COLOR detection ✅ --fix flag for auto-fixing ✅ --fix --dry-run for preview ✅ --fix --yes for non-interactive mode Git Hooks Added:\n✅ Traditional pre-commit hook script ✅ docbuilder lint install-hook command ✅ Lefthook configuration example ✅ Staged file filtering CI/CD Integration Added:\n✅ GitHub Actions workflow example ✅ GitLab CI template ✅ JSON output schema ✅ PR comment integration examples Testing Added:\n✅ Golden test framework for lint validation ✅ Auto-fix integration tests ✅ Link resolution test suite ✅ Lint-DocBuilder sync tests ✅ Rule drift detection CI workflow Bug Fixes Fixed:\n✅ Link transformation preserves ./ prefix (Issue: ./file.md became /repo/./file) ✅ Linter now resolves Hugo site-absolute paths (/docs/file) ✅ Linter tries .md and .markdown extensions when validating links Future Planned Changes v1.1.0 (Planned) Content Rules (Errors) - In Development:\nMalformed frontmatter YAML detection Missing frontmatter closing marker detection Invalid frontmatter key detection (duplicates) Broken internal link detection (enhanced) Image reference validation Structure Rules (Warnings) - Planned:\nMissing section index detection (_index.md) Deep directory nesting detection (\u003e4 levels) Orphaned asset detection (unreferenced images) Mixed naming style detection in same directory Asset Rules (Warnings/Info) - Planned:\nImage filename validation (same as markdown) Absolute URL to internal asset detection Large binary file detection (\u003e5 MB) v1.2.0+ (Future) Advanced Features - Roadmap:\nFrontmatter schema validation (structured fields) Content linting (spell checking, grammar) Markdown style consistency (headings, lists) Accessibility checks (alt text, heading hierarchy) SEO recommendations (meta descriptions) VS Code extension integration Language Server Protocol (LSP) support Migration Guide Upgrading to v1.0.0 Initial adoption for existing repositories:\nRun discovery:\n1 docbuilder lint --format=json \u003e lint-issues.json Review issues:\n1 docbuilder lint Preview fixes:\n1 docbuilder lint --fix --dry-run Apply fixes:\n1 docbuilder lint --fix Commit cleanup:\n1 2 git add -A git commit -m \"docs: normalize filenames for linting compliance\" Breaking Changes: None (initial release)\nDeprecations: None (initial release)\nRule Severity Changes No severity changes in v1.0.0 (initial release).\nFuture considerations:\nWarnings may be promoted to errors based on usage data New rules start as warnings before becoming errors Breaking changes announced 1 version ahead Compatibility Matrix DocBuilder Version Linter Version Go Version Hugo Version 1.0.0 1.0.0 1.21+ 0.112+ Rule Statistics v1.0.0 Coverage Category Rules Errors Warnings Info Filename 6 6 0 0 Content 0 0 0 0 Structure 0 0 0 0 Assets 1 0 0 1 Total 7 6 0 1 Auto-fixable: 5 / 6 error rules (83%)\nFeedback and Evolution How Rules Are Added Proposal: Issue or discussion in DocBuilder repository Review: Core team evaluates against best practices Implementation: Add to linter with tests Documentation: Update this changelog and rule reference Release: Included in next version with migration notes Reporting Issues If you encounter:\nFalse positives: Rule incorrectly flags valid content False negatives: Rule misses actual issues Unclear messages: Error/warning text confusing Please file an issue with:\nLinter version: docbuilder --version Minimal reproduction case Expected vs actual behavior Suggested improvement Requesting New Rules When requesting new rules, include:\nUse case: What problem does this solve? Examples: Show invalid patterns to detect Rationale: Why is this a best practice? Severity: Should it be error, warning, or info? Auto-fix: Can it be automatically fixed? Version Compatibility Semantic Versioning Linter rules follow semantic versioning:\nMajor (x.0.0): Breaking changes, rule removals Minor (1.x.0): New rules, new features Patch (1.0.x): Bug fixes, message improvements Backward Compatibility Error rules: Never removed without deprecation period Warning rules: May be promoted to errors with notice Auto-fix behavior: Changes announced in advance Exit codes: Stable contract across versions Sync with DocBuilder Linting rules stay synchronized with DocBuilder behavior:\nDocBuilder Change Linter Update New Hugo feature support Add validation rules Deprecate functionality Add deprecation warnings Bug fix in processing Update corresponding rule New file type support Extend validation Sync verification: Weekly CI job tests linter against actual DocBuilder builds.\nSee Also Lint Rules Reference - Complete rule documentation Setup Linting - Installation and usage Migration Guide - Adopt linting in existing repos ADR-005: Documentation Linting - Design decisions Last Updated: 2025-12-29\nLinter Version: 1.0.0\nDocBuilder Version: 1.0.0",
    "description": "This document tracks changes to linting rules across DocBuilder versions, helping teams understand when rules were added, changed, or deprecated.\nVersion History v1.0.0 (2025-12-29) - Initial Release Initial linting implementation with core validation rules.\nFilename Rules (Errors) Added:",
    "tags": [
      "Linting",
      "Changelog",
      "Versions"
    ],
    "title": "Lint Rules Changelog",
    "uri": "/docs/reference/lint-rules-changelog/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "Complete reference for all documentation linting rules enforced by DocBuilder.\nOverview DocBuilder’s linter enforces opinionated, non-configurable rules based on Hugo and DocBuilder best practices. Rules are classified by severity:\nSeverity Impact Build Behavior Error Blocks build Exit code 2, CI fails Warning Should fix Exit code 1, CI passes Info Informational Exit code 0, CI passes Rule Categories Filename Rules (Errors) Content Rules (Errors) Structure Rules (Warnings) Asset Rules (Warnings/Info) Filename Rules All filename rules are Errors that block the build.\nRule: Uppercase Letters Pattern: [A-Z] in filename\nRationale:\nFilenames become URL slugs in Hugo Uppercase causes case-sensitivity issues across platforms (macOS/Windows case-insensitive, Linux case-sensitive) Creates inconsistent URLs: API-Guide.md → /API-Guide/ vs expected /api-guide/ Examples:\n❌ Invalid: API-Guide.md MyDocument.md UserManual.md README_FIRST.md ✅ Valid: api-guide.md my-document.md user-manual.md readme-first.md Auto-fix: Converts to lowercase: API-Guide.md → api-guide.md\nError Message:\nERROR: Invalid filename File: docs/API-Guide.md Issue: Contains uppercase letters (A, P, I, G) Current: docs/API-Guide.md Suggested: docs/api-guide.md Why: Uppercase letters cause case-sensitivity issues across platforms and create inconsistent URLs. Fix: docbuilder lint --fix docs/ Rule: Spaces in Filename Pattern: [ ] (space character) in filename\nRationale:\nSpaces become %20 in URLs: My Doc.md → /my%20doc/ Breaks some link parsers and shell commands Poor user experience with encoded URLs Examples:\n❌ Invalid: My Document.md API Guide.md User Manual v2.md Architecture Diagram.png ✅ Valid: my-document.md api-guide.md user-manual-v2.md architecture-diagram.png Auto-fix: Replaces spaces with hyphens: My Document.md → my-document.md\nError Message:\nERROR: Invalid filename File: docs/My Document.md Issue: Contains space characters Current: docs/My Document.md Suggested: docs/my-document.md Why: Spaces create problematic URLs (/my%20document/) and may break cross-references. Fix: docbuilder lint --fix docs/ Rule: Special Characters Pattern: Any character not in [a-z0-9-_.]\nRationale:\nSpecial characters unsupported by Hugo’s URL generation May require shell escaping in commands Can break on different filesystems Invalid Characters: @, #, $, %, \u0026, *, (, ), [, ], {, }, ;, :, ', \", \u003c, \u003e, ?, |, \\, /, ~, `\nExamples:\n❌ Invalid: file@name.md doc#tag.md guide(final).md config[prod].md notes\u0026ideas.md ✅ Valid: file-name.md doc-tag.md guide-final.md config-prod.md notes-ideas.md Auto-fix: Replaces special characters with hyphens: file@name.md → file-name.md\nError Message:\nERROR: Invalid filename File: docs/config[prod].md Issue: Contains special characters: [ ] Current: docs/config[prod].md Suggested: docs/config-prod.md Why: Special characters are unsupported by Hugo and may cause issues on different filesystems. Fix: docbuilder lint --fix docs/ Rule: Leading/Trailing Hyphens or Underscores Pattern: Filename starts or ends with - or _\nRationale:\nCreates malformed URLs: /-docs/ or /_temp/ May be interpreted as hidden files on Unix systems Poor aesthetics and confusing navigation Examples:\n❌ Invalid: -draft.md _temporary.md config-.md notes_.md ✅ Valid: draft.md temporary.md config.md notes.md Auto-fix: Strips leading/trailing hyphens and underscores: -draft.md → draft.md\nError Message:\nERROR: Invalid filename File: docs/-draft.md Issue: Leading hyphen Current: docs/-draft.md Suggested: docs/draft.md Why: Leading/trailing hyphens create malformed URLs and may be interpreted as hidden files. Fix: docbuilder lint --fix docs/ Rule: Invalid Double Extensions Pattern: Multiple file extensions (e.g., .md.backup, .markdown.old)\nRationale:\nHugo attempts to process files with .md or .markdown anywhere in the extension Backup files, temp files cause build errors Whitelisted exceptions exist for embedded diagram formats Whitelisted Double Extensions:\n.drawio.png - Draw.io embedded PNG diagrams (editable) .drawio.svg - Draw.io embedded SVG diagrams (editable) Examples:\n❌ Invalid: api-guide.md.backup notes.markdown.old config.md.txt draft.md.2024-12-29 ✅ Valid (single extension): api-guide.md notes.markdown config.txt draft.md ✅ Valid (whitelisted): architecture.drawio.png flowchart.drawio.svg Auto-fix: Not automatically fixable (requires manual intervention)\nError Message:\nERROR: Invalid double extension File: docs/api-guide.md.backup Issue: Contains non-whitelisted double extension .md.backup Current: docs/api-guide.md.backup Hugo will attempt to process this as markdown, causing build errors. Whitelisted double extensions (allowed): • .drawio.png (Draw.io embedded PNG diagrams) • .drawio.svg (Draw.io embedded SVG diagrams) How to fix: 1. Remove backup files from docs directory 2. Use Git history or separate backup location 3. Add to .gitignore: *.backup Rule: Reserved Names Pattern: Filenames that conflict with Hugo taxonomy URLs\nReserved Names: tags.md, categories.md (without namespace prefix)\nRationale:\nHugo reserves /tags/ and /categories/ URLs for taxonomy listings Direct files cause URL conflicts and build errors Examples:\n❌ Invalid: tags.md categories.md ✅ Valid (with prefix): content-tags.md doc-categories.md using-tags.md about-categories.md Auto-fix: Adds prefix: tags.md → content-tags.md\nError Message:\nERROR: Reserved filename File: docs/tags.md Issue: Conflicts with Hugo taxonomy URL Current: docs/tags.md Suggested: docs/content-tags.md Why: Hugo reserves /tags/ for taxonomy listings. Using tags.md creates URL conflicts. Fix: docbuilder lint --fix docs/ Content Rules Content rules validate the internal structure of markdown files.\nRule: Malformed Frontmatter YAML Pattern: Invalid YAML syntax in frontmatter block\nRationale:\nHugo silently skips pages with invalid frontmatter Page won’t render but no error is shown Causes missing documentation without obvious cause Examples:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ❌ Invalid: --- title: API Guide date: 2025-12-29 invalid key without colon --- ❌ Invalid (indentation): --- title: API Guide date: 2025-12-29 --- ❌ Invalid (missing closing): --- title: API Guide date: 2025-12-29 1 2 3 4 5 6 7 8 ✅ Valid: --- title: \"API Guide\" date: 2025-12-29 tags: - api - reference --- Auto-fix: Not automatically fixable (requires manual correction)\nError Message:\nERROR: Invalid frontmatter File: docs/api-guide.md Line: 4 Issue: YAML parsing failed: mapping values are not allowed here --- title: API Guide date: 2025-12-29 invalid key without colon --- Frontmatter must be valid YAML enclosed by --- markers. Check for proper indentation and key: value format. Rule: Missing Frontmatter Closing Pattern: Frontmatter starts with --- but missing closing ---\nRationale:\nHugo treats entire file as frontmatter No content rendered Page appears blank Examples:\n1 2 3 4 5 6 7 ❌ Invalid: --- title: API Guide date: 2025-12-29 This is the content... 1 2 3 4 5 6 7 8 ✅ Valid: --- title: API Guide date: 2025-12-29 --- This is the content... Auto-fix: Not automatically fixable (ambiguous where frontmatter should end)\nError Message:\nERROR: Incomplete frontmatter File: docs/api-guide.md Issue: Missing closing --- marker Frontmatter started at line 1 but never closed. Hugo will treat the entire file as frontmatter, resulting in a blank page. Add --- after frontmatter block before content. Rule: Broken Internal Links Pattern: Link to non-existent local file\nRationale:\nBroken links lead to 404 errors in production Poor user experience Indicates stale or incorrect documentation Examples:\n1 2 3 4 ❌ Invalid: [API Guide](/docs/reference/api-guide) # File doesn't exist [Configuration](/docs/config/settings) # File doesn't exist ![Diagram](/docs/reference/images/flow.png) # Image doesn't exist 1 2 3 4 ✅ Valid: [API Guide](/docs/reference/getting-started) # File exists [Configuration](/docs/config/) # File exists ![Diagram](/docs/reference/images/architecture.png) # Image exists Auto-fix: Not automatically fixable (can’t determine intent)\nError Message:\nERROR: Broken internal link File: docs/index.md Line: 15 Issue: Link target does not exist [API Guide](/docs/reference/api-guide) ^^^^^^^^^^^^^^^^^ Target file not found: docs/api-guide.md Possible fixes: • Create the missing file • Update link to correct path • Remove broken link Structure Rules Structure rules are Warnings that don’t block builds but should be addressed.\nRule: Missing Section Index Pattern: Directory contains .md files but no _index.md\nRationale:\nSection won’t appear in navigation sidebar Directory appears empty in site structure No landing page for the section Examples:\n❌ Missing _index.md: docs/ api/ authentication.md authorization.md # No _index.md ✅ Has _index.md: docs/ api/ _index.md ← Section landing page authentication.md authorization.md Auto-fix: Can generate basic _index.md with --fix --generate-indexes\nWarning Message:\nWARNING: Missing section index Directory: docs/api/ Issue: Contains 5 markdown files but no _index.md Impact: Section will not appear in navigation sidebar Create _index.md to define this section: --- title: \"API Documentation\" weight: 2 --- This section contains API guides and references. Rule: Deep Nesting Pattern: Directory structure exceeds 4 levels deep\nRationale:\nPoor navigation UX (too many clicks) Overly complex information architecture Consider flattening or reorganizing Examples:\n❌ Too deep (5 levels): docs/guides/advanced/api/rest/authentication.md ⚠️ Consider flattening: docs/guides/api-rest-authentication.md docs/api/rest-authentication.md Auto-fix: Not automatically fixable (requires architectural decision)\nWarning Message:\nWARNING: Deep directory nesting File: docs/guides/advanced/api/rest/authentication.md Depth: 5 levels Consider flattening directory structure for better UX. Deep nesting makes navigation difficult. Suggested: docs/api/rest-authentication.md Rule: Orphaned Assets Pattern: Image file not referenced by any markdown file\nRationale:\nBloats repository size May be leftover from deletions Indicates maintenance needed Examples:\n❌ Orphaned: docs/ images/ old-screenshot.png # Not referenced anywhere Auto-fix: Can list orphans with --fix --remove-orphans (prompts for confirmation)\nWarning Message:\nWARNING: Orphaned asset File: docs/images/old-screenshot.png Issue: Not referenced by any markdown file Size: 2.4 MB This file may be unused and safe to remove. To remove: docbuilder lint --fix --remove-orphans Rule: Mixed Naming Styles Pattern: Same directory has different naming conventions\nRationale:\nInconsistent developer experience Harder to remember filenames Looks unprofessional Examples:\n❌ Mixed styles: docs/ getting_started.md # snake_case api-guide.md # kebab-case UserManual.md # PascalCase CHANGELOG.md # SCREAMING_CASE ✅ Consistent: docs/ getting-started.md api-guide.md user-manual.md changelog.md Auto-fix: Normalizes to kebab-case: getting_started.md → getting-started.md\nWarning Message:\nWARNING: Mixed filename styles Directory: docs/ Issue: Files use different naming conventions Files: • getting_started.md (snake_case) • api-guide.md (kebab-case) • UserManual.md (PascalCase) Recommended: Use consistent kebab-case naming Fix: docbuilder lint --fix docs/ Asset Rules Asset rules apply to images and other non-markdown files.\nRule: Image Filename Issues Severity: Warning\nPattern: Same filename rules as markdown (spaces, uppercase, special chars)\nRationale:\nSame URL and filesystem concerns as markdown Image paths must be exact matches (case-sensitive) Examples:\n❌ Warning: Screenshot 2024.png Company_Logo.svg Diagram(final).png ✅ Valid: screenshot-2024.png company-logo.svg diagram-final.png Auto-fix: Applies same transformations as markdown files\nWarning Message:\nWARNING: Invalid image filename File: docs/images/Screenshot 2024.png Issue: Contains spaces and uppercase letters Current: docs/images/Screenshot 2024.png Suggested: docs/images/screenshot-2024.png Asset files follow the same rules as markdown files. Image references in markdown will be updated automatically. Fix: docbuilder lint --fix docs/ Rule: Whitelisted Double Extensions Severity: Info (explicitly allowed)\nPattern: .drawio.png or .drawio.svg files\nRationale:\nDraw.io exports editable diagrams as embedded format PNG/SVG contains both image and diagram source data Allows round-trip editing without separate source files Examples:\n✅ Explicitly allowed: architecture.drawio.png flowchart.drawio.svg system-design.drawio.png Info Message:\nINFO: Whitelisted double extension File: docs/diagrams/architecture.drawio.png This double extension is explicitly whitelisted for Draw.io embedded diagrams (editable format). Rule: Absolute URLs to Internal Assets Severity: Warning\nPattern: ![](https://your-domain.com/images/logo.png) pointing to same repository\nRationale:\nBreaks in local development Not portable across deployments Should use relative paths Examples:\n❌ Absolute (internal): ![Logo](https://docs.example.com/images/logo.png) ✅ Relative: ![Logo](/docs/reference/images/logo.png) ![Logo](/docs/reference/../assets/logo.png) Auto-fix: Not automatically fixable (requires manual verification)\nWarning Message:\nWARNING: Absolute URL to internal asset File: docs/index.md Line: 8 Issue: Using absolute URL for same-repository asset ![Logo](https://docs.example.com/images/logo.png) This breaks in local development and isn't portable. Use relative path instead: ![Logo](/docs/reference/images/logo.png) Rule: Large Binary Files Severity: Warning\nPattern: Image/asset file exceeds 5 MB\nRationale:\nSlows Git operations (clone, fetch, pull) Poor repository performance Consider external hosting or optimization Examples:\n⚠️ Large files: screenshot-4k.png (12 MB) demo-video.gif (25 MB) presentation.pdf (18 MB) Auto-fix: Not automatically fixable (requires manual optimization)\nWarning Message:\nWARNING: Large binary file File: docs/images/screenshot-4k.png Size: 12.4 MB Large files slow Git performance and bloat repository. Consider: • Optimize/compress image (use tools like tinypng.com) • Host externally (CDN, cloud storage) • Split into multiple smaller images • Use thumbnail with link to full size Standard File Exclusions These files are automatically excluded from linting (never reported as errors):\nREADME.md (root or in subdirectories) CONTRIBUTING.md CHANGELOG.md LICENSE.md CODE_OF_CONDUCT.md Files in .git/ directory Files matching .gitignore patterns Rationale: These are standard repository files, not documentation content processed by Hugo.\nRule Evolution Linting rules evolve with DocBuilder versions. See Lint Rules Changelog for version history.\nDocBuilder Version Rule Changes 1.0 - 1.5 Initial filename and frontmatter rules 1.6+ Enhanced frontmatter schema validation (future) 2.0+ Asset transformation rules (future) Exit Codes Summary Code Meaning Condition 0 Success No issues found (clean) 1 Warnings Warnings present, no errors 2 Errors Errors found (blocks build) 3 Failure Linter execution error Auto-Fix Capabilities Rule Auto-Fixable Method Uppercase letters ✅ Yes Convert to lowercase Spaces ✅ Yes Replace with hyphens Special characters ✅ Yes Replace with hyphens Leading/trailing hyphens ✅ Yes Strip Double extensions ❌ No Manual removal Reserved names ✅ Yes Add prefix Malformed frontmatter ❌ No Manual correction Broken links ❌ No* Manual fix (*Can detect only) Missing section index ⚠️ Partial Generate basic _index.md Mixed naming styles ✅ Yes Normalize to kebab-case Image filename issues ✅ Yes Same as markdown files Large binary files ❌ No Manual optimization See Also Setup Linting - Installation and usage guide CI/CD Integration - Automated validation Migration Guide - Cleaning up existing repositories ADR-005: Documentation Linting - Architecture decision",
    "description": "Complete reference for all documentation linting rules enforced by DocBuilder.\nOverview DocBuilder’s linter enforces opinionated, non-configurable rules based on Hugo and DocBuilder best practices. Rules are classified by severity:\nSeverity Impact Build Behavior Error Blocks build Exit code 2, CI fails Warning Should fix Exit code 1, CI passes Info Informational Exit code 0, CI passes Rule Categories Filename Rules (Errors) Content Rules (Errors) Structure Rules (Warnings) Asset Rules (Warnings/Info) Filename Rules All filename rules are Errors that block the build.",
    "tags": [
      "Linting",
      "Validation",
      "Rules"
    ],
    "title": "Lint Rules Reference",
    "uri": "/docs/reference/lint-rules/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Linting",
    "uri": "/tags/linting/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Migration",
    "uri": "/tags/migration/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Reference",
    "uri": "/categories/reference/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Rules",
    "uri": "/tags/rules/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Testing",
    "uri": "/categories/testing/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Testing-Strategy",
    "uri": "/tags/testing-strategy/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Validation",
    "uri": "/tags/validation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Versions",
    "uri": "/tags/versions/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Adr",
    "uri": "/tags/adr/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e adr",
    "content": "Status PROPOSED - Draft for discussion\nContext DocBuilder aims to enable markdown files to work seamlessly both in their source forge (GitHub, GitLab, Forgejo) and in the rendered Hugo documentation site. However, forges support forge-specific markdown extensions that create references to forge resources:\nGitLab-Specific Markdown (GLFM) GitLab provides extensive reference syntax (GitLab Flavored Markdown):\nReference Type Syntax Cross-Project Example Issue #123, GL-123, [issue:123] namespace/project#123 #42 → Issue link Merge Request !123 namespace/project!123 !17 → MR link Snippet $123 namespace/project$123 $5 → Snippet link Epic \u0026123, [epic:123] group/subgroup\u0026123 \u00268 → Epic link User @username n/a @alice → User profile Label ~bug, ~\"feature request\" namespace/project~bug ~priority::high Milestone %v1.0 namespace/project%v1.0 %release-1.0 Commit 9ba12248 namespace/project@9ba12248 Short SHA link Alert ^alert#123 namespace/project^alert#123 Alert reference Contact [contact:test@example.com] n/a CRM contact GitHub-Specific Markdown (GFM) GitHub also has reference syntax:\nReference Type Syntax Cross-Repo Example Issue/PR #123 owner/repo#123 #42 User @username n/a @octocat Team @org/team n/a @github/docs Commit SHA owner/repo@SHA a1b2c3d Forgejo/Gitea Markdown Similar to GitHub with some extensions:\nReference Type Syntax Cross-Repo Example Issue/PR #123 owner/repo#123 #42 User @username n/a @alice Commit SHA owner/repo@SHA abc123 The Problem Multi-Forge Environments:\nDocBuilder may aggregate docs from multiple forge instances (e.g., gitlab-main, gitlab-secondary, github-public) Each document knows which forge instance it came from via page.File.Forge (the forge identifier) All references in a document refer to that same forge instance Simple references (#123) and cross-project references (other/repo#123) both refer to the document’s source forge Rendering Challenges:\nForge-specific syntax (#123, !456) is not standard markdown References must be converted to standard markdown links for Hugo Must preserve readability in both source forge and rendered docs User Goals:\nWrite markdown once, works in both forge and docs site Reference issues, PRs, users, etc. naturally Support cross-repository references Handle multiple forge instances correctly Decision Implement a multi-stage forge-specific markdown transform in the content pipeline with the following components:\n1. Reference Transforms Stage: StageTransform\nStrategy: Build one focused transformer at a time, test thoroughly, then move to the next.\nImplementation Approach: One transformer per reference type per forge (detects pattern and builds URL in single pass)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 // GitLabIssueReferenceTransform handles GitLab issue references (#123) // Detects patterns and builds URLs in a single pass type GitLabIssueReferenceTransform struct { cache ReferenceCache } func (t *GitLabIssueReferenceTransform) CanTransform(page *ContentPage, ctx *TransformContext) bool { forgeConfig := ctx.Generator.GetConfig().GetForgeByName(page.File.Forge) return forgeConfig != nil \u0026\u0026 forgeConfig.Type == config.ForgeGitLab } func (t *GitLabIssueReferenceTransform) Transform(page *ContentPage, ctx *TransformContext) (*TransformationResult, error) { forgeConfig := ctx.Generator.GetConfig().GetForgeByName(page.File.Forge) // Detect issue patterns: #123, [issue:123], GL-123 issuePattern := regexp.MustCompile(`(?:^|[\\s(])(?:#(\\d+)|GL-(\\d+)|\\[issue:(\\d+)\\])(?:[\\s.,;:!?)]|$)`) // Find all matches with their positions matches := issuePattern.FindAllStringSubmatchIndex(page.Content, -1) if len(matches) == 0 { return NewTransformationResult().SetSuccess(), nil } // Process matches in reverse order to maintain string positions var replacements []replacement for _, match := range matches { issueNum := extractNumber(page.Content[match[0]:match[1]]) // helper to extract number // Build URL (check cache first) cacheKey := fmt.Sprintf(\"gitlab:issue:%s:%d\", page.File.Repository, issueNum) url := t.cache.Get(cacheKey) if url == \"\" { url = fmt.Sprintf(\"%s/%s/-/issues/%d\", forgeConfig.BaseURL, page.File.Repository, issueNum) t.cache.Set(cacheKey, url) } // Create markdown link replacement := fmt.Sprintf(\"[#%d](/docs/adr/%s)\", issueNum, url) replacements = append(replacements, replacement{ start: match[0], end: match[1], text: replacement, }) } // Apply replacements in reverse order page.Content = applyReplacements(page.Content, replacements) return NewTransformationResult().SetSuccess(), nil } // GitLabMergeRequestReferenceDetector handles only GitLab MR references (!123) type GitLabMergeRequestReferenceDetector struct {} func (t *GitLabMergeRequestReferenceDetector) CanTransform(page *ContentPage, ctx *TransformContext) bool { forgeConfig := ctx.Generator.GetConfig().GetForgeByName(page.File.Forge) return forgeConfig != nil \u0026\u0026 forgeConfig.Type == config.ForgeGitLab } // GitLabLabelReferenceDetector handles only GitLab label references (~label) type GitLabLabelReferenceDetector struct {} // ... similar structure // GitHubIssueReferenceDetector handles GitHub issue/PR references (#123) type GitHubIssueReferenceDetector struct {} // GitHubUserReferenceDetector handles GitHub user mentions (@username) type GitHubUserReferenceDetector struct {} // GitHubTeamReferenceDetector handles GitHub team mentions (@org/team) type GitHubTeamReferenceDetector struct {} // ... and so on for each reference type Pros:\nVery focused, single-purpose transformers (single responsibility) Easy to test each reference type independently Simple to add new reference types Single pass per reference type (efficient) Easy to understand what each transformer does Direct pattern → URL conversion (no intermediate state) Cons:\nMany transformers (could be 20+ total) More CanTransform() checks in pipeline (minimal performance impact) Some duplication of pattern matching logic (mitigated by helper utilities) Pipeline Registration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func defaultTransforms(cfg *config.Config) []FileTransform { cache := NewReferenceCache() // Shared cache instance return []FileTransform{ parseFrontMatter, normalizeIndexFiles, // ... existing transforms // GitLab reference transforms (detect + build URL in one pass) NewGitLabIssueReferenceTransform(cache), NewGitLabMergeRequestReferenceTransform(cache), NewGitLabLabelReferenceTransform(cache), NewGitLabMilestoneReferenceTransform(cache), NewGitLabSnippetReferenceTransform(cache), NewGitLabEpicReferenceTransform(cache), NewGitLabUserReferenceTransform(cache), // GitHub reference transforms NewGitHubIssueReferenceTransform(cache), NewGitHubUserReferenceTransform(cache), NewGitHubTeamReferenceTransform(cache), // Forgejo reference transforms NewForgejoIssueReferenceTransform(cache), NewForgejoUserReferenceTransform(cache), serializeDocument, } } No Configuration Required:\nTransforms are always active for documents from matching forge types. The CanTransform() guard ensures each transformer only processes appropriate documents.\nTesting Benefits:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func TestGitLabIssueReferenceTransform_SimpleReference(t *testing.T) { cache := NewMockCache() transform := NewGitLabIssueReferenceTransform(cache) page := \u0026models.ContentPage{ Content: \"Fixed in #123\", File: docs.DocFile{Forge: \"my-gitlab\", Repository: \"org/repo\"}, } result, err := transform.Transform(page, ctx) require.NoError(t, err) // Verify content was replaced with markdown link assert.Equal(t, \"Fixed in [#123](https://gitlab.com/org/repo/-/issues/123)\", page.Content) } func TestGitLabIssueReferenceTransform_CacheHit(t *testing.T) { cache := NewMockCache() cache.Set(\"gitlab:issue:org/repo:456\", \"https://cached-url.com\") transform := NewGitLabIssueReferenceTransform(cache) page := \u0026models.ContentPage{ Content: \"See GL-456\", File: docs.DocFile{Forge: \"my-gitlab\", Repository: \"org/repo\"}, } result, err := transform.Transform(page, ctx) require.NoError(t, err) // Verify cached URL was used in replacement assert.Equal(t, \"See [GL-456](https://cached-url.com)\", page.Content) } // Each reference type gets focused, isolated tests for pattern detection, URL building, AND content replacement Build incrementally: Start with GitLab issue transform (#123), test thoroughly, then add GitLab merge requests (!123), then move to other forges and reference types.\nShared Helper Utilities 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Helper type for managing replacements type replacement struct { start int end int text string } // applyReplacements applies text replacements in reverse order to maintain positions func applyReplacements(content string, replacements []replacement) string { // Sort by position (descending) to apply from end to start sort.Slice(replacements, func(i, j int) bool { return replacements[i].start \u003e replacements[j].start }) for _, r := range replacements { content = content[:r.start] + r.text + content[r.end:] } return content } Rendering Strategy: Each transform converts references to standard markdown links:\n1 2 3 4 5 \u003c!-- Source in GitLab --\u003e See issue #123 for details. \u003c!-- After GitLabIssueReferenceTransform --\u003e See issue [#123](https://gitlab.com/org/repo/-/issues/123) for details. Why standard markdown links:\nSimple, no Hugo shortcodes needed Works in all markdown renderers Preserves functionality (clickable links in both forge and docs) Easy to test and verify 2. Configuration Schema Per-Forge Settings:\n1 2 3 4 5 forges: - name: gitlab-main type: gitlab base_url: https://gitlab.com api_url: https://gitlab.com/api/v4 No Additional Configuration Required:\nForge reference processing works automatically without user configuration. The cache layer uses sensible defaults (24h TTL) and automatically selects NATS KV when available in daemon mode, gracefully degrading to in-memory cache otherwise.\n3. Implementation Stages Phase 1: Basic References (Minimal Viable)\nImplement transforms for basic patterns: #123, !123 Each transform detects, builds URL, and replaces content Cache URL building results with 24h TTL Add metrics for reference frequency Phase 2: Advanced Features\nCross-project references Label and milestone support Epic support (GitLab) Snippet support Consequences Positive Dual Compatibility: Markdown works in both forge and docs Rich References: Support forge-native syntax naturally Multi-Forge: Handle multiple forge instances correctly Extensible: Easy to add new reference types Graceful Degradation: Failures preserve original text Testable: Each reference type can be tested independently (Option B) Simple: No configuration needed, transforms run when applicable Negative Complexity: Adds transforms to pipeline Maintenance: Must track forge markdown spec changes Future API Work: May need forge API for validation/metadata (not in initial scope) Risks \u0026 Mitigations Risk Mitigation Performance impact Cache URL building results, CanTransform() guards prevent unnecessary work Forge spec changes Version detection, graceful fallbacks Pattern ambiguity Well-tested regex patterns, explicit word boundaries Alternatives Considered Alternative 1: No Processing (Status Quo) Approach: Leave forge references as-is, let them break in rendered docs.\nRejected because:\nPoor user experience in docs Defeats dual-compatibility goal No better than current state Alternative 2: Hugo-Only Processing Approach: Use Hugo’s markdown processing hooks.\nRejected because:\nLocks us into Hugo implementation Can’t reuse with other static generators Less control over processing Alternative 3: Client-Side JavaScript Approach: Detect and resolve references in browser.\nRejected because:\nDoesn’t work in static exports Requires forge API access from client Performance issues SEO problems Examples Example 1: Simple Issue Reference Source (in GitLab):\n1 The authentication bug was fixed in #123. After Processing:\n1 The authentication bug was fixed in [#123](https://gitlab.com/myorg/api-docs/-/issues/123). Frontmatter Addition:\n1 2 3 4 5 forge_references: - type: issue id: 123 url: https://gitlab.com/myorg/api-docs/-/issues/123 resolved: true Example 2: Cross-Project Reference Source (in GitLab):\n1 See the design in myorg/design-system#45. After Processing:\n1 See the design in {{\u003c forge-ref type=\"issue\" project=\"myorg/design-system\" id=\"45\" \u003e}}myorg/design-system#45{{\u003c /forge-ref \u003e}}. Example 3: Multiple Forge Types Configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 forges: - name: gitlab-main type: gitlab base_url: https://gitlab.com - name: github-oss type: github base_url: https://github.com repositories: - url: https://gitlab.com/myorg/internal-docs forge: gitlab-main - url: https://github.com/myorg/public-docs forge: github-oss Source (in gitlab-main repo):\n1 2 Internal issue: #100 Public discussion: myorg/public-docs#50 After Processing:\n1 2 Internal issue: [#100](https://gitlab.com/myorg/internal-docs/-/issues/100) Public discussion: [myorg/public-docs#50](https://github.com/myorg/public-docs/issues/50) Implementation Plan Phase 1: Foundation (Week 1) Create shared helper utilities (applyReplacements, pattern extraction) Create ReferenceCache interface and NATS implementation Add cache factory with graceful degradation logic Unit tests for helper utilities and cache layer Phase 2: GitLab Issue References (Week 1-2) Implement GitLabIssueReferenceTransform (single transform: detect + build URL + replace) Pattern matching for #123, GL-123, [issue:123] URL building with cache integration Unit tests (pattern detection, URL building, content replacement) Golden test with GitLab repo containing issue references Test thoroughly before moving to next reference type Phase 3: GitLab Merge Requests (Week 2-3) Implement GitLabMergeRequestReferenceTransform (!123 syntax) Pattern matching for !123 and cross-project namespace/project!123 URL building for merge requests Unit tests for MR-specific patterns Update golden tests to include MR references Verify no regressions in issue transform Phase 4: GitHub Support (Week 3-4) Implement GitHubIssueReferenceTransform (#123 for issues and PRs) Pattern matching for same-repo and cross-repo references URL building for GitHub issues/PRs Unit tests for GitHub-specific patterns Golden test with GitHub repo Test multi-forge scenarios (GitLab + GitHub repos) Phase 5: Additional Reference Types (Week 4-5) GitLab: Labels (~label), Milestones (%v1.0), Users (@username) GitHub: Users (@username), Teams (@org/team) Forgejo: Issues (#123), Users (@username) Build one transform at a time, test thoroughly Integration tests with all reference types combined Phase 6: Cross-Project References (Week 5-6) Implement cross-project pattern detection (namespace/project#123) Update all transforms to handle cross-project syntax Test cross-project URL building Golden tests with cross-project references Performance optimization (regex compilation, caching efficiency) Error handling improvements Phase 7: Advanced Features (Week 6-7) GitLab Snippets ($123), Epics (\u0026123), Alerts (^alert#123) Commit SHA references (all forges) Quoted labels (~\"feature request\") Documentation and user guide Migration guide for existing deployments Phase 8: Optional Enhancements (Week 7-8) API-based validation (optional, disabled by default) Fetch issue/MR titles for richer link text Metrics for reference processing (count by type, cache hit rate) Advanced caching strategies (pre-warming, TTL tuning) Open Questions Forge Type Detection: Do we need to detect forge type from content?\nAnswer: No! We already know from page.File.Forge. Each document tracks its source forge. Caching Strategy: Use NATS KV (like link verification) or local cache?\nAnswer: Use NATS KV automatically when available (daemon mode) with 24h default TTL. Degrade gracefully to in-memory cache if NATS unavailable. Log when degradation occurs. No user configuration needed. Private References: How to handle references to private issues?\nRecommendation: Skip resolution, preserve original text, log warning Reference Validation: Should we validate that references exist?\nRecommendation: Optional validation (off by default), log warnings Shortcode Library: Which Hugo theme should host shortcodes?\nRecommendation: Include in all themes, theme-agnostic design API Authentication: Use same auth as git operations?\nRecommendation: Yes, reuse existing forge auth config References GitLab Flavored Markdown GitHub Flavored Markdown Forgejo Markdown ADR-002: In-Memory Content Pipeline ADR-003: Fixed Transform Pipeline Decision Log 2025-12-18: Initial proposal created with forge-specific markdown support concept 2025-12-18: Clarified to use existing page.File.Forge metadata (no forge type detection needed) 2025-12-18: Adopted per-reference-type transformers with CanTransform() guards for maximum modularity 2025-12-18: Removed all user configuration (no enable/disable flags) - well-tested transforms always run 2025-12-18: Simplified caching to automatic NATS KV with graceful degradation (24h default TTL, no user config) 2025-12-18: Removed TransformerConfiguration - transformers are stateless or hold only cache reference 2025-12-18: Combined detection and resolution into single-pass transforms (no separate renderer stage) 2025-12-18: Transforms modify page.Content directly with standard markdown links (no intermediate metadata) 2025-12-18: Finalized incremental implementation strategy: build one transform, test thoroughly, then next TBD: Team review and feedback TBD: Implementation start date",
    "description": "Status PROPOSED - Draft for discussion\nContext DocBuilder aims to enable markdown files to work seamlessly both in their source forge (GitHub, GitLab, Forgejo) and in the rendered Hugo documentation site. However, forges support forge-specific markdown extensions that create references to forge resources:",
    "tags": [
      "Adr",
      "Markdown",
      "Forges",
      "Content-Processing"
    ],
    "title": "ADR-004: Forge-Specific Markdown Support",
    "uri": "/docs/adr/adr-004-forge-specific-markdown/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Architecture",
    "uri": "/categories/architecture/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Content-Processing",
    "uri": "/tags/content-processing/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Forges",
    "uri": "/tags/forges/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Markdown",
    "uri": "/tags/markdown/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Architecture",
    "uri": "/tags/architecture/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Webhooks allow DocBuilder to automatically rebuild documentation when changes are pushed to your repositories. This guide shows you how to configure webhooks for GitHub, GitLab, and Forgejo.\nOverview When configured, DocBuilder:\nReceives webhook events from your forge (GitHub/GitLab/Forgejo) Validates the webhook signature for security Parses the event to extract repository and branch information Triggers a targeted rebuild for only the affected repository Returns an acknowledgment with the build job ID Important: Webhook-triggered builds only refetch and rebuild the specific repository mentioned in the webhook event, not all configured repositories. This provides fast, efficient updates.\nConfiguration 0. Understanding Port Isolation Important: DocBuilder runs webhooks on a separate HTTP server and port from your documentation. This means:\nDocumentation is served on docs_port (default: 8080) Webhooks are received on webhook_port (default: 8081) No collision is possible - they’re completely isolated servers Example URLs:\nDocumentation: http://your-server:8080/docs/guide/ Webhooks: http://your-server:8081/webhooks/github See Webhook and Documentation Isolation for detailed architecture information.\n1. Add Webhook Configuration to Your Forge In your config.yaml, add webhook configuration to each forge:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 forges: - name: github type: github base_url: \"https://github.com\" api_url: \"https://api.github.com\" auth: type: token token: \"${GITHUB_TOKEN}\" webhook: secret: \"${GITHUB_WEBHOOK_SECRET}\" # Shared secret for signature validation path: \"/webhooks/github\" # Endpoint path (optional, default shown) events: # Events to listen for - push - repository - name: gitlab type: gitlab base_url: \"https://gitlab.com\" api_url: \"https://gitlab.com/api/v4\" auth: type: token token: \"${GITLAB_TOKEN}\" webhook: secret: \"${GITLAB_WEBHOOK_SECRET}\" path: \"/webhooks/gitlab\" events: - push - tag_push - name: forgejo type: forgejo base_url: \"https://git.home.luguber.info\" api_url: \"https://git.home.luguber.info/api/v1\" auth: type: token token: \"${FORGEJO_TOKEN}\" webhook: secret: \"${FORGEJO_WEBHOOK_SECRET}\" path: \"/webhooks/forgejo\" events: - push 2. Configure Daemon HTTP Ports DocBuilder runs four separate HTTP servers on different ports:\n1 2 3 4 5 6 daemon: http: docs_port: 8080 # Documentation server (public) webhook_port: 8081 # Webhook receiver (forge access only) admin_port: 8082 # Admin API (internal only) livereload_port: 8083 # Live reload SSE (optional) Port Separation Benefits:\n✅ Webhooks cannot interfere with documentation serving ✅ Different firewall rules for each service ✅ Separate access controls (public docs, restricted webhooks) ✅ Independent scaling and monitoring Port Configuration Rules:\nAll ports must be unique (daemon fails to start if duplicates detected) Use sequential ports for clarity (8080, 8081, 8082, 8083) Ports can be customized but must remain distinct 3. Set Environment Variables Create a .env file or .env.local file with your webhook secrets:\n1 2 3 4 5 GITHUB_WEBHOOK_SECRET=your-github-webhook-secret-here GITLAB_WEBHOOK_SECRET=your-gitlab-webhook-secret-here FORGEJO_WEBHOOK_SECRET=your-forgejo-webhook-secret-here Security Note: Use strong, randomly generated secrets. You can generate one with:\n1 openssl rand -hex 32 Forge-Specific Setup GitHub Go to your repository settings → Webhooks → Add webhook Set Payload URL to: http://your-docbuilder-host:8081/webhooks/github Set Content type to: application/json Set Secret to the same value as GITHUB_WEBHOOK_SECRET Select events: Push events (for code pushes) Repository events (for repo changes) Ensure Active is checked Click Add webhook Test: Push a commit to your repository and check the webhook delivery in GitHub settings.\nGitLab Go to your project settings → Webhooks Set URL to: http://your-docbuilder-host:8081/webhooks/gitlab Set Secret token to the same value as GITLAB_WEBHOOK_SECRET Select trigger events: Push events Tag push events (optional) Uncheck SSL verification if using HTTP (not recommended for production) Click Add webhook Test: Click “Test” next to your webhook and select “Push events”.\nForgejo (Gitea) Go to your repository settings → Webhooks → Add webhook → Gitea Set Target URL to: http://your-docbuilder-host:8081/webhooks/forgejo Set HTTP Method to: POST Set POST Content Type to: application/json Set Secret to the same value as FORGEJO_WEBHOOK_SECRET Select trigger events: Push Repository (optional) Ensure Active is checked Click Add webhook Test: Push a commit and check the webhook deliveries in Forgejo.\nWebhook Endpoints DocBuilder provides these webhook endpoints:\nEndpoint Forge Signature Header Event Header /webhooks/github GitHub X-Hub-Signature-256 X-GitHub-Event /webhooks/gitlab GitLab X-Gitlab-Token X-Gitlab-Event /webhooks/forgejo Forgejo X-Hub-Signature-256 X-Forgejo-Event or X-Gitea-Event /webhook Generic Auto-detected Auto-detected Webhook Flow sequenceDiagram participant Forge as GitHub/GitLab/Forgejo participant DocBuilder as DocBuilder Daemon participant BuildQueue as Build Queue participant Hugo as Hugo Generator Forge-\u003e\u003eDocBuilder: POST /webhooks/github Note over DocBuilder: Validate signature DocBuilder-\u003e\u003eDocBuilder: Parse webhook event Note over DocBuilder: Extract repo + branch DocBuilder-\u003e\u003eBuildQueue: Enqueue webhook build BuildQueue-\u003e\u003eHugo: Build specific repository Hugo--\u003e\u003eBuildQueue: Build complete BuildQueue--\u003e\u003eDocBuilder: Job finished DocBuilder-\u003e\u003eForge: 202 Accepted (job_id) Webhook Response Successful webhook receives return:\n1 2 3 4 5 6 7 { \"status\": \"received\", \"timestamp\": \"2025-12-17T10:30:00Z\", \"event\": \"push\", \"source\": \"github\", \"build_job_id\": \"webhook-1734433800\" } If no matching repository is found, the webhook is still acknowledged but no build is triggered.\nVerification Check Webhook Logs Monitor DocBuilder daemon logs for webhook events:\n1 2 3 4 5 INFO Webhook signature validated forge=github INFO Webhook matched repository repo=docbuilder full_name=inful/docbuilder branch=main INFO Webhook build triggered job_id=webhook-1734433800 repo=inful/docbuilder branch=main target_count=1 WARN No matching repositories found for webhook repo_full_name=unknown/repo branch=main Check Build Queue Check the build queue via the admin API:\n1 curl http://localhost:8082/api/queue/status Look for jobs with type: \"webhook\" and verify the repositories field contains only the triggered repository.\nTroubleshooting Signature Validation Failed WARN Webhook signature validation failed forge=github event=push Solution: Ensure the webhook.secret in your config matches the secret configured in your forge.\nNo Matching Repository WARN No matching repositories found for webhook repo_full_name=owner/repo branch=main Solution:\nVerify the repository is configured in your config.yaml Check that the repository URL matches the webhook source Ensure the branch matches if you have branch-specific configs Webhook Not Received Check:\nFirewall allows connections to webhook_port (default 8081) DocBuilder daemon is running Forge can reach your DocBuilder instance (check forge webhook delivery logs) Security Considerations Port Isolation (Primary Security) Webhooks run on a separate HTTP server (port 8081 by default), completely isolated from documentation (port 8080). This provides:\nZero collision risk between webhooks and documentation Independent access control per service Network-level separation via firewall rules Access Control Best Practices Always use webhook secrets for signature validation Use HTTPS in production to encrypt webhook payloads Restrict webhook port access using firewall rules: 1 2 3 4 5 6 7 8 9 10 # Allow docs publicly iptables -A INPUT -p tcp --dport 8080 -j ACCEPT # Allow webhooks only from forge IPs iptables -A INPUT -p tcp --dport 8081 -s \u003cgithub-ip-range\u003e -j ACCEPT iptables -A INPUT -p tcp --dport 8081 -j DROP # Block others # Allow admin only from internal network iptables -A INPUT -p tcp --dport 8082 -s 10.0.0.0/8 -j ACCEPT iptables -A INPUT -p tcp --dport 8082 -j DROP Rotate webhook secrets periodically Monitor webhook logs for unusual activity Use reverse proxy for additional isolation (subdomains recommended) Advanced Configuration Custom Webhook Port 1 2 3 daemon: http: webhook_port: 9000 # Custom port Update your forge webhook URLs accordingly.\nEvent Filtering Configure which events trigger builds:\n1 2 3 4 5 webhook: events: - push # Code pushes - tag_push # Tag creation (GitLab/Forgejo) - repository # Repository events (create, delete, rename) Note: Currently, DocBuilder acknowledges all configured events but only triggers builds for push events affecting configured repositories.\nRelated Documentation Webhook and Documentation Isolation - Architecture and collision prevention Getting Started - Introduction to DocBuilder Configuration Reference - Complete configuration guide",
    "description": "Webhooks allow DocBuilder to automatically rebuild documentation when changes are pushed to your repositories. This guide shows you how to configure webhooks for GitHub, GitLab, and Forgejo.\nOverview When configured, DocBuilder:\nReceives webhook events from your forge (GitHub/GitLab/Forgejo) Validates the webhook signature for security Parses the event to extract repository and branch information Triggers a targeted rebuild for only the affected repository Returns an acknowledgment with the build job ID Important: Webhook-triggered builds only refetch and rebuild the specific repository mentioned in the webhook event, not all configured repositories. This provides fast, efficient updates.",
    "tags": [
      "Webhooks",
      "Automation",
      "Ci-Cd"
    ],
    "title": "Configure Webhooks for Automatic Rebuilds",
    "uri": "/docs/how-to/configure-webhooks/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Explanation",
    "uri": "/categories/explanation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Security",
    "uri": "/tags/security/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "This document explains how DocBuilder prevents webhook endpoints from colliding with documentation content.\nArchitecture: Multi-Server Design DocBuilder uses a defense-in-depth approach with multiple isolated HTTP servers:\n┌─────────────────────────────────────────────────────────┐ │ DocBuilder Daemon │ ├─────────────────────────────────────────────────────────┤ │ │ │ ┌────────────────┐ ┌────────────────┐ │ │ │ Docs Server │ │ Webhook Server │ │ │ │ Port: 8080 │ │ Port: 8081 │ │ │ ├────────────────┤ ├────────────────┤ │ │ │ GET / │ │ POST /webhooks/│ │ │ │ GET /docs/* │ │ github │ │ │ │ GET /search/ │ │ POST /webhooks/│ │ │ │ index.json│ │ gitlab │ │ │ └────────────────┘ │ POST /webhooks/│ │ │ │ forgejo │ │ │ ┌────────────────┐ └────────────────┘ │ │ │ Admin Server │ │ │ │ Port: 8082 │ ┌────────────────┐ │ │ ├────────────────┤ │ LiveReload │ │ │ │ GET /health │ │ Port: 8083 │ │ │ │ GET /ready │ │ (optional) │ │ │ │ GET /metrics │ ├────────────────┤ │ │ │ POST /api/ │ │ GET /sse │ │ │ │ build/ │ └────────────────┘ │ │ │ trigger │ │ │ └────────────────┘ │ └─────────────────────────────────────────────────────────┘ Port Allocation Server Default Port Purpose Collision Risk Docs 8080 Serves Hugo-generated documentation ❌ None - separate server Webhook 8081 Receives forge webhooks ❌ None - separate server Admin 8082 Administrative API, health checks ❌ None - separate server LiveReload 8083 Server-Sent Events for live reload ❌ None - separate server Defense-in-Depth Layers Layer 1: Port Isolation (Primary Defense) Each server runs on a separate TCP port with its own http.Server instance and request multiplexer (http.ServeMux). This provides complete isolation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // In internal/daemon/http_server.go func (s *HTTPServer) Start(ctx context.Context) error { // Bind separate ports docsListener := net.Listen(\"tcp\", \":8080\") webhookListener := net.Listen(\"tcp\", \":8081\") adminListener := net.Listen(\"tcp\", \":8082\") // Start independent servers s.docsServer = \u0026http.Server{Handler: docsHandler} s.webhookServer = \u0026http.Server{Handler: webhookHandler} s.adminServer = \u0026http.Server{Handler: adminHandler} go s.docsServer.Serve(docsListener) go s.webhookServer.Serve(webhookListener) go s.adminServer.Serve(adminListener) } Collision Probability: 0% - Requests to different ports go to completely different HTTP servers.\nLayer 2: Path Prefixing (Secondary Defense) Even if servers were combined (they’re not), webhook paths use reserved prefixes:\n/webhooks/github /webhooks/gitlab /webhooks/forgejo /webhook (generic) These paths are unlikely to exist in Hugo documentation because:\nHugo content typically lives in /docs/, /blog/, etc. The /webhooks/ prefix is API-specific, not documentation content Hugo wouldn’t generate these exact paths without explicit configuration Layer 3: HTTP Method Filtering (Tertiary Defense) Webhook handlers only accept POST requests:\n1 2 3 4 5 6 7 func (h *WebhookHandlers) HandleGitHubWebhook(w http.ResponseWriter, r *http.Request) { if r.Method != http.MethodPost { // Return 405 Method Not Allowed return } // Process webhook... } Documentation requests use GET, so even if a collision occurred:\nGET /webhooks/github → Documentation server (404 or docs file) POST /webhooks/github → Webhook server (webhook handler) Layer 4: Configuration Validation (Preventive) Port binding validation happens at startup:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Pre-bind all ports before starting any servers binds := []preBind{ {name: \"docs\", port: config.Daemon.HTTP.DocsPort}, {name: \"webhook\", port: config.Daemon.HTTP.WebhookPort}, {name: \"admin\", port: config.Daemon.HTTP.AdminPort}, } for _, bind := range binds { ln, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", bind.port)) if err != nil { return fmt.Errorf(\"%s port %d: %w\", bind.name, bind.port, err) } bind.ln = ln } If any port is already in use or if two services try to use the same port, the daemon fails to start with a clear error message.\nAdditional Safeguards 1. Port Conflict Detection DocBuilder validates that all configured ports are unique:\n1 2 3 4 5 6 daemon: http: docs_port: 8080 webhook_port: 8081 # Must differ from docs_port admin_port: 8082 # Must differ from both above livereload_port: 8083 # Must differ from all above If you accidentally configure the same port twice, the daemon will fail to start.\n2. Firewall Recommendations For production deployments, use firewall rules to restrict access:\n1 2 3 4 5 6 7 8 9 iptables -A INPUT -p tcp --dport 8080 -j ACCEPT iptables -A INPUT -p tcp --dport 8081 -s 140.82.112.0/20 -j ACCEPT # GitHub iptables -A INPUT -p tcp --dport 8081 -s 192.30.252.0/22 -j ACCEPT # GitHub iptables -A INPUT -p tcp --dport 8082 -s 10.0.0.0/8 -j ACCEPT iptables -A INPUT -p tcp --dport 8081 -j DROP iptables -A INPUT -p tcp --dport 8082 -j DROP 3. Reverse Proxy Path Segregation When using a reverse proxy (nginx, Traefik, Caddy), use different subdomains or paths:\nOption A: Subdomain Separation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 server { server_name docs.example.com; location / { proxy_pass http://localhost:8080; } } server { server_name webhooks.example.com; location / { proxy_pass http://localhost:8081; } } server { server_name admin.example.com; allow 10.0.0.0/8; deny all; location / { proxy_pass http://localhost:8082; } } Option B: Path Separation (Less Recommended)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 server { server_name example.com; # Documentation at root location / { proxy_pass http://localhost:8080; } # Webhooks at /api/webhooks location /api/webhooks/ { proxy_pass http://localhost:8081/webhooks/; } # Admin at /api/admin location /api/admin/ { allow 10.0.0.0/8; deny all; proxy_pass http://localhost:8082/api/; } } 4. Content Security Policy For defense in depth, the docs server could set CSP headers to prevent accidental form submissions to webhook paths:\n1 Content-Security-Policy: form-action 'self'; frame-ancestors 'none' This prevents JavaScript on the docs site from submitting forms to webhook endpoints.\nAttack Vectors (and Why They’re Mitigated) ❌ Path Traversal Attack Scenario: Attacker tries GET /webhooks/../../../etc/passwd\nMitigation:\nHTTP path normalization happens before routing Webhook server only handles /webhooks/*, not arbitrary paths Different port means request wouldn’t reach docs server anyway ❌ Documentation Collision Scenario: Hugo generates a page at /webhooks/github.html\nMitigation:\nWebhook server is on port 8081, docs on 8080 Even if page exists on docs server, webhook POST goes to webhook server HTTP method differs (GET vs POST) ❌ Port Confusion Scenario: User configures same port for docs and webhooks\nMitigation:\nStartup validation fails with clear error Daemon refuses to start Operator must fix configuration ❌ Webhook Forgery via Docs Scenario: Attacker embeds JavaScript in docs to forge webhooks\nMitigation:\nDifferent origins (port 8080 vs 8081) trigger CORS Webhook signature validation prevents unsigned requests Same-origin policy blocks cross-port requests Testing Collision Prevention Manual Testing 1 2 3 4 5 6 7 8 9 ./docbuilder daemon curl http://localhost:8080/ curl http://localhost:8081/webhooks/github curl -X POST http://localhost:8081/webhooks/github curl -X POST http://localhost:8080/webhooks/github Port Conflict Testing 1 2 3 nc -l 8081 \u0026 ./docbuilder daemon Configuration Best Practices ✅ Recommended: Default Ports 1 2 3 4 5 6 daemon: http: docs_port: 8080 # Standard HTTP alternative port webhook_port: 8081 # Sequential, clearly webhook-related admin_port: 8082 # Sequential, clearly admin-related livereload_port: 8083 # Sequential, optional feature ✅ Recommended: Custom Ports with Separation 1 2 3 4 5 6 daemon: http: docs_port: 3000 # Custom docs port webhook_port: 3001 # Different from docs admin_port: 3002 # Different from both livereload_port: 3003 # Different from all ❌ Never: Same Ports 1 2 3 4 5 daemon: http: docs_port: 8080 webhook_port: 8080 # ❌ WILL FAIL TO START admin_port: 8080 # ❌ WILL FAIL TO START ⚠️ Caution: Non-Sequential Ports 1 2 3 4 5 daemon: http: docs_port: 8080 webhook_port: 9443 # ⚠️ Works but non-obvious relationship admin_port: 3000 # ⚠️ Works but confusing Monitoring and Validation Startup Validation Watch daemon logs for port binding confirmation:\nINFO HTTP servers binding to ports docs_port=8080 webhook_port=8081 admin_port=8082 INFO Documentation server started on :8080 INFO Webhook server started on :8081 INFO Admin server started on :8082 If you see errors:\nERROR http startup failed: webhook port 8081: address already in use This indicates a port conflict that must be resolved before the daemon can start.\nRuntime Health Checks 1 2 3 4 curl -f http://localhost:8080/health # Docs server curl -f http://localhost:8082/health # Admin server netstat -an | grep :8081 Summary DocBuilder prevents webhook/documentation collisions through:\n✅ Port Isolation - Separate HTTP servers on different ports (primary defense) ✅ Path Prefixing - Reserved /webhooks/* prefix (secondary defense) ✅ Method Filtering - POST-only webhooks vs GET documentation (tertiary defense) ✅ Startup Validation - Fail fast if ports conflict (preventive) ✅ Firewall Rules - Network-level access control (optional) ✅ Reverse Proxy - Subdomain/path segregation (optional) Collision Risk: Effectively 0% with default configuration.\nRelated Documentation Configure Webhooks - Webhook setup guide Getting Started - Introduction to DocBuilder",
    "description": "This document explains how DocBuilder prevents webhook endpoints from colliding with documentation content.\nArchitecture: Multi-Server Design DocBuilder uses a defense-in-depth approach with multiple isolated HTTP servers:\n┌─────────────────────────────────────────────────────────┐ │ DocBuilder Daemon │ ├─────────────────────────────────────────────────────────┤ │ │ │ ┌────────────────┐ ┌────────────────┐ │ │ │ Docs Server │ │ Webhook Server │ │ │ │ Port: 8080 │ │ Port: 8081 │ │ │ ├────────────────┤ ├────────────────┤ │ │ │ GET / │ │ POST /webhooks/│ │ │ │ GET /docs/* │ │ github │ │ │ │ GET /search/ │ │ POST /webhooks/│ │ │ │ index.json│ │ gitlab │ │ │ └────────────────┘ │ POST /webhooks/│ │ │ │ forgejo │ │ │ ┌────────────────┐ └────────────────┘ │ │ │ Admin Server │ │ │ │ Port: 8082 │ ┌────────────────┐ │ │ ├────────────────┤ │ LiveReload │ │ │ │ GET /health │ │ Port: 8083 │ │ │ │ GET /ready │ │ (optional) │ │ │ │ GET /metrics │ ├────────────────┤ │ │ │ POST /api/ │ │ GET /sse │ │ │ │ build/ │ └────────────────┘ │ │ │ trigger │ │ │ └────────────────┘ │ └─────────────────────────────────────────────────────────┘ Port Allocation Server Default Port Purpose Collision Risk Docs 8080 Serves Hugo-generated documentation ❌ None - separate server Webhook 8081 Receives forge webhooks ❌ None - separate server Admin 8082 Administrative API, health checks ❌ None - separate server LiveReload 8083 Server-Sent Events for live reload ❌ None - separate server Defense-in-Depth Layers Layer 1: Port Isolation (Primary Defense) Each server runs on a separate TCP port with its own http.Server instance and request multiplexer (http.ServeMux). This provides complete isolation:",
    "tags": [
      "Architecture",
      "Webhooks",
      "Security"
    ],
    "title": "Webhook and Documentation Isolation Strategy",
    "uri": "/docs/explanation/webhook-documentation-isolation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Webhooks",
    "uri": "/tags/webhooks/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Pipeline",
    "uri": "/tags/pipeline/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Simplification",
    "uri": "/tags/simplification/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Transforms",
    "uri": "/tags/transforms/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "This document provides visual representations of DocBuilder’s architecture using ASCII diagrams and Mermaid notation.\nLast Updated: December 16, 2025 - Reflects ADR-003 fixed transform pipeline implementation.\nThis document provides visual representations of DocBuilder’s architecture using ASCII diagrams and Mermaid notation.\nTable of Contents High-Level System Architecture Pipeline Flow Package Dependencies Data Flow Component Interactions State Machine Diagrams High-Level System Architecture Layer View ┌──────────────────────────────────────────────────────────────────┐ │ COMMAND LAYER │ │ (cmd/docbuilder/commands/) │ │ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────────────┐ │ │ │ Build │ │ Daemon │ │ Preview │ │ Discover │ │ │ │ (Kong) │ │ (Watch) │ │ (Live) │ │ (Analysis) │ │ │ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────────┬─────────┘ │ │ │ │ │ │ │ └───────┼─────────────┼─────────────┼─────────────────┼────────────┘ │ │ │ │ └─────────────┴─────────────┴─────────────────┘ │ ┌─────────────────────▼───────────────────────────────────────────┐ │ SERVICE LAYER │ │ (internal/build, internal/daemon) │ │ │ │ ┌────────────────┐ ┌─────────────────┐ ┌──────────────────┐ │ │ │ BuildService │ │ DaemonService │ │ DiscoveryService │ │ │ │ │ │ │ │ │ │ │ │ - Run() │ │ - Start() │ │ - Discover() │ │ │ │ - Validate() │ │ - Stop() │ │ - Report() │ │ │ └────────┬───────┘ └────────┬────────┘ └────────┬─────────┘ │ │ │ │ │ │ └───────────┼───────────────────┼────────────────────┼────────────┘ │ │ │ └───────────────────┴────────────────────┘ │ ┌───────────────────────────────▼─────────────────────────────────────┐ │ PROCESSING LAYER │ │ (internal/hugo, internal/docs, internal/hugo/pipeline) │ │ │ │ ┌──────────────────────────────────────────────────────────────┐ │ │ │ Hugo Generator │ │ │ │ │ │ │ │ ┌─────────────┐ ┌─────────────┐ ┌─────────────────────┐ │ │ │ │ │ Pipeline │ │ Theme │ │ Report Builder │ │ │ │ │ │ Processor │ │ System │ │ │ │ │ │ │ └──────┬──────┘ └──────┬──────┘ └──────────┬──────────┘ │ │ │ └─────────┼───────────────┼───────────────────┼────────────────┘ │ │ │ │ │ │ │ ┌─────────▼───────────────▼───────────────────▼──────────────┐ │ │ │ Fixed Transform Pipeline │ │ │ │ (internal/hugo/pipeline/) │ │ │ │ │ │ │ │ 1. parseFrontMatter - Extract YAML │ │ │ │ 2. normalizeIndexFiles - README → _index │ │ │ │ 3. buildBaseFrontMatter - Add defaults │ │ │ │ 4. extractIndexTitle - H1 extraction │ │ │ │ 5. stripHeading - Remove H1 │ │ │ │ 6. rewriteRelativeLinks - Fix .md links │ │ │ │ 7. rewriteImageLinks - Fix image paths │ │ │ │ 8. generateFromKeywords - Create from @keywords │ │ │ │ 9. addRepositoryMetadata - Inject repo info │ │ │ │ 10. addEditLink - Generate editURL │ │ │ │ 11. serializeDocument - Output YAML + content │ │ │ └──────────────────────────────────────────────────────────┘ │ └─────────────────────────────────┬───────────────────────────────────┘ │ ┌─────────────────────────────────▼────────────────────────────────┐ │ DOMAIN LAYER │ │ (internal/config, internal/state, internal/docs) │ │ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────────────┐ │ │ │ Config │ │ State │ │ DocFile │ │ Repository │ │ │ │ │ │ │ │ │ │ │ │ │ │ - Hugo │ │ - Git │ │ - Path │ │ - URL │ │ │ │ - Build │ │ - Docs │ │ - Trans │ │ - Branch │ │ │ │ - Forge │ │ - Build │ │ forms │ │ - Auth │ │ │ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────────┬─────────┘ │ └───────┼─────────────┼─────────────┼─────────────────┼────────────┘ │ │ │ │ └─────────────┴─────────────┴─────────────────┘ │ ┌─────────────────────▼───────────────────────────────────────────┐ │ INFRASTRUCTURE LAYER │ │ (internal/git, internal/forge, internal/workspace) │ │ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌─────────────────┐ │ │ │ Git │ │ Forge │ │ Event │ │ Workspace │ │ │ │ Client │ │ Clients │ │ Store │ │ Manager │ │ │ │ │ │ │ │ │ │ │ │ │ │ - Clone │ │ - GitHub │ │ - Append │ │ - Create() │ │ │ │ - Update │ │ - GitLab │ │ - Query │ │ - Cleanup() │ │ │ │ - Auth │ │ - Forgejo│ │ │ │ │ │ │ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────────┬────────┘ │ │ │ │ │ │ │ │ └─────────────┴─────────────┴─────────────────┘ │ │ │ │ │ ┌────────▼────────┐ │ │ │ Foundation │ │ │ │ Errors │ │ │ │ │ │ │ │ - ClassifiedErr │ │ │ │ - Categories │ │ │ │ - Retry Logic │ │ │ └─────────────────┘ │ └─────────────────────────────────────────────────────────────────┘ Pipeline Flow Sequential Stage Execution graph TD A[Build Request] --\u003e B[PrepareOutput Stage] B --\u003e C[CloneRepos Stage] C --\u003e D[DiscoverDocs Stage] D --\u003e E[GenerateConfig Stage] E --\u003e F[Layouts Stage] F --\u003e G[CopyContent Stage] G --\u003e H[Indexes Stage] H --\u003e I{Render Mode?} I --\u003e|always| J[RunHugo Stage] I --\u003e|auto| K{Has Hugo?} K --\u003e|yes| J K --\u003e|no| L[Skip Hugo] I --\u003e|never| L J --\u003e M[Build Complete] L --\u003e M M --\u003e N[Generate Report] N --\u003e O[Emit Events] O --\u003e P[Return Result] style B fill:#e1f5ff style C fill:#e1f5ff style D fill:#e1f5ff style E fill:#e1f5ff style F fill:#e1f5ff style G fill:#e1f5ff style H fill:#e1f5ff style J fill:#e1f5ff style M fill:#c8e6c9 style P fill:#c8e6c9 Stage Detail: CloneRepos CloneRepos Stage │ ├─ For each repository config: │ │ │ ├─ 1. Authenticate │ │ ├─ SSH Key │ │ ├─ Token │ │ └─ Basic Auth │ │ │ ├─ 2. Check Incremental │ │ ├─ Compare HEAD ref │ │ ├─ Check doc hash │ │ └─ Skip if unchanged │ │ │ ├─ 3. Clone or Update │ │ ├─ git clone (first time) │ │ └─ git pull (update) │ │ │ ├─ 4. Read HEAD │ │ └─ Store ref in state │ │ │ └─ 5. Emit Event │ ├─ RepositoryCloned │ └─ RepositoryUpdated │ └─ Update GitState Stage Detail: CopyContent CopyContent Stage │ ├─ For each DocFile: │ │ │ ├─ Fixed Transform Pipeline (11 sequential transforms) │ │ │ │ │ ├─ 1. Parse Front Matter │ │ │ ├─ Extract YAML header │ │ │ └─ Parse markdown content │ │ │ │ │ ├─ 2. Normalize Index Files │ │ │ └─ README.md → _index.md │ │ │ │ │ ├─ 3. Build Base Front Matter │ │ │ ├─ Add repository metadata │ │ │ ├─ Add section/path info │ │ │ ├─ Add forge info │ │ │ └─ Add date/timestamp │ │ │ │ │ ├─ 4. Extract Index Title │ │ │ ├─ Find first H1 heading │ │ │ └─ Set as page title │ │ │ │ │ ├─ 5. Strip Heading │ │ │ └─ Remove first H1 from content │ │ │ │ │ ├─ 6. Rewrite Relative Links │ │ │ ├─ Fix .md → / conversions │ │ │ └─ Resolve relative paths │ │ │ │ │ ├─ 7. Rewrite Image Links │ │ │ └─ Fix image path references │ │ │ │ │ ├─ 8. Generate from Keywords │ │ │ └─ Process @keywords directives │ │ │ │ │ ├─ 9. Add Repository Metadata │ │ │ └─ Inject repo context │ │ │ │ │ ├─ 10. Add Edit Link │ │ │ ├─ Check forge capabilities │ │ │ ├─ Build edit URL │ │ │ └─ Add to front matter │ │ │ │ │ └─ 11. Serialize Document │ │ ├─ Generate YAML │ │ └─ Combine with content │ │ │ └─ Write to content/ │ └─ Create target file │ └─ Update DocsState Package Dependencies Dependency Graph ┌──────────────────┐ │ cmd/docbuilder/ │ │ commands/ │ └────────┬─────────┘ │ ▼ ┌───────────────────┐ │ internal/build/ │ │ internal/daemon/ │ └────────┬──────────┘ │ ├────────────────────────────────┐ │ │ ▼ ▼ ┌──────────────┐ ┌──────────────┐ │ config/ │ │ state/ │ └──────┬───────┘ └──────┬───────┘ │ │ ├────────────────────────────────┤ │ │ ▼ ▼ ┌────────────────────────────────────────────────┐ │ Domain Layer │ │ ┌─────────┐ ┌──────────────┐ ┌───────────┐ │ │ │ docs/ │ │ hugo/ │ │ forge/ │ │ │ └────┬────┘ └──────┬───────┘ └─────┬─────┘ │ │ │ │ │ │ │ │ ┌────▼─────────┐ │ │ │ │ │ pipeline/ │ │ │ │ │ │ (transforms)│ │ │ │ │ └──────────────┘ │ │ └───────┼────────────────┼──────────────┼────────┘ │ │ │ └────────────────┴──────────────┘ │ ┌────────────────┴─────────────┐ │ │ ▼ ▼ ┌──────────────┐ ┌──────────────┐ │ git/ │ │ workspace/ │ └──────┬───────┘ └──────┬───────┘ │ │ └─────────────┬─────────────┘ │ ▼ ┌──────────────┐ │ foundation/ │ │ errors/ │ └──────────────┘ Import Rules Layer Dependencies (must respect):\ncommands → services → domain → infrastructure ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✓ Package Rules:\n✅ cmd/docbuilder/commands/ can import internal/build/, internal/daemon/ ✅ internal/build/ can import internal/hugo/, internal/docs/, internal/git/ ✅ internal/hugo/ can import internal/hugo/pipeline/ ✅ internal/hugo/pipeline/ contains transform implementations ✅ internal/docs/ can import internal/config/ ✅ All packages can import internal/foundation/ ❌ internal/config/ cannot import internal/build/ ❌ internal/git/ cannot import internal/build/ ❌ internal/foundation/ cannot import application packages Data Flow Configuration Loading sequenceDiagram participant User participant CLI participant Config participant ENV participant Validator participant TypedConfig User-\u003e\u003eCLI: docbuilder build -c config.yaml CLI-\u003e\u003eConfig: Load(configPath) Config-\u003e\u003eENV: Read .env files ENV--\u003e\u003eConfig: Environment variables Config-\u003e\u003eConfig: Parse YAML Config-\u003e\u003eConfig: Expand ${VAR} references Config-\u003e\u003eConfig: Apply defaults Config-\u003e\u003eValidator: ValidateConfig() Validator-\u003e\u003eTypedConfig: HugoConfig.Validate() TypedConfig--\u003e\u003eValidator: ValidationResult Validator-\u003e\u003eTypedConfig: DaemonConfig.Validate() TypedConfig--\u003e\u003eValidator: ValidationResult Validator--\u003e\u003eConfig: Validation complete Config--\u003e\u003eCLI: Validated Config CLI-\u003e\u003eCLI: Start build Build Execution sequenceDiagram participant CLI participant BuildService participant Pipeline participant Git participant Docs participant Hugo participant EventStore CLI-\u003e\u003eBuildService: Build(config) BuildService-\u003e\u003ePipeline: Run(stages) Pipeline-\u003e\u003eEventStore: Emit BuildStarted Pipeline-\u003e\u003eGit: CloneRepos() Git-\u003e\u003eGit: Authenticate Git-\u003e\u003eGit: Clone/Update Git--\u003e\u003ePipeline: Repository ready Pipeline-\u003e\u003eEventStore: Emit RepositoryCloned Pipeline-\u003e\u003eDocs: DiscoverDocs() Docs-\u003e\u003eDocs: Walk paths Docs-\u003e\u003eDocs: Filter markdown Docs--\u003e\u003ePipeline: DocFile list Pipeline-\u003e\u003eEventStore: Emit DocumentationDiscovered Pipeline-\u003e\u003eHugo: GenerateConfig() Hugo-\u003e\u003eHugo: Apply theme params Hugo-\u003e\u003eHugo: Write hugo.yaml Hugo--\u003e\u003ePipeline: Config ready Pipeline-\u003e\u003eHugo: CopyContent() Hugo-\u003e\u003eHugo: Transform files Hugo--\u003e\u003ePipeline: Content ready Pipeline-\u003e\u003eHugo: RunHugo() Hugo-\u003e\u003eHugo: Execute hugo build Hugo--\u003e\u003ePipeline: Site generated Pipeline-\u003e\u003eEventStore: Emit BuildCompleted Pipeline--\u003e\u003eBuildService: BuildReport BuildService--\u003e\u003eCLI: Success State Persistence sequenceDiagram participant Pipeline participant BuildState participant GitState participant StateStore participant FileSystem Pipeline-\u003e\u003eBuildState: Create() Pipeline-\u003e\u003eGitState: Update(repo, head) GitState-\u003e\u003eBuildState: Merge update Pipeline-\u003e\u003eBuildState: RecordStage(name, duration) Pipeline-\u003e\u003eStateStore: Save(state) StateStore-\u003e\u003eStateStore: Serialize to JSON StateStore-\u003e\u003eFileSystem: Write .docbuilder/state.json FileSystem--\u003e\u003eStateStore: Success StateStore--\u003e\u003ePipeline: State persisted Note over Pipeline,FileSystem: Later: Incremental build Pipeline-\u003e\u003eStateStore: Load() StateStore-\u003e\u003eFileSystem: Read .docbuilder/state.json FileSystem--\u003e\u003eStateStore: JSON data StateStore-\u003e\u003eStateStore: Deserialize StateStore--\u003e\u003ePipeline: Previous BuildState Pipeline-\u003e\u003ePipeline: Compare HEAD refs Pipeline-\u003e\u003ePipeline: Decide skip/clone Component Interactions Relearn Theme Configuration ┌────────────────────────────────────────────────────┐ │ Relearn Theme Configuration │ │ (hardcoded in config_writer.go) │ │ │ │ applyRelearnThemeDefaults(params) │ │ │ │ - themeVariant: \"relearn-light\" │ │ - disableSearch: false │ │ - disableLandingPageButton: true │ │ - collapsibleMenu: true │ │ - showVisitedLinks: true │ └────────────────────────────────────────────────────┘ Generation Flow: 1. Load config → title, baseURL, etc. 2. Apply Relearn defaults 3. User params deep merge (override defaults) 4. Add dynamic fields (build_date) 5. Add hardcoded module: github.com/McShelby/hugo-theme-relearn 6. Write hugo.yaml Forge Integration ┌──────────────────────────────────────────────┐ │ Forge Factory │ │ │ │ NewForge(config) → Forge │ └──────────────┬───────────────────────────────┘ │ │ Based on config.type │ ┌──────────┼──────────┐ │ │ │ ▼ ▼ ▼ ┌────────┐ ┌────────┐ ┌─────────┐ │GitHub │ │GitLab │ │Forgejo │ │Client │ │Client │ │Client │ └───┬────┘ └───┬────┘ └────┬────┘ │ │ │ └──────────┴───────────┘ │ │ All compose ▼ ┌─────────────┐ │ BaseForge │ │ │ │ HTTP Client │ │ Auth Header │ │ Base URL │ └──────┬──────┘ │ │ Uses ▼ ┌─────────────┐ │http.Client │ │ │ │- Timeout │ │- TLS Config │ │- Transport │ └─────────────┘ Operation Flow: 1. Config specifies forge type: \"github\" 2. NewForge(config) creates GitHubClient 3. GitHubClient embeds BaseForge 4. BaseForge.NewRequest(method, path) 5. Add auth header: \"Authorization: Bearer {token}\" 6. Add custom headers: \"X-GitHub-Api-Version: 2022-11-28\" 7. BaseForge.DoRequest(req) 8. Parse response 9. Return Repository, error Change Detection ┌──────────────────────────────────────────────────┐ │ Change Detector │ └──────────────┬───────────────────────────────────┘ │ │ DetectChanges(repos) ▼ ┌──────────────────────┐ │ Load Previous State │ │ - HEAD refs │ │ - Doc hashes │ └──────────┬───────────┘ │ ▼ ┌──────────────────────┐ │ For each repository │ └──────────┬───────────┘ │ ├─ Level 1: HEAD Comparison │ ├─ Read current HEAD │ ├─ Compare to previous │ └─ Changed? → Include │ ├─ Level 2: Quick Hash │ ├─ Hash directory tree │ ├─ Compare to previous │ └─ Changed? → Include │ ├─ Level 3: Doc Files Hash │ ├─ Discover docs │ ├─ Sort paths │ ├─ SHA-256 hash │ ├─ Compare to previous │ └─ Changed? → Include │ └─ Level 4: Deletion Detection ├─ Check removed files └─ Deletions? → Include ▼ ┌──────────────────────┐ │ ChangeSet │ │ │ │ - ChangedRepos: [] │ │ - SkippedRepos: [] │ │ - Reasons: map[] │ └──────────────────────┘ State Machine Diagrams Build State Machine stateDiagram-v2 [*] --\u003e Idle Idle --\u003e Preparing: Build request Preparing --\u003e Cloning: Output ready Cloning --\u003e Discovering: Repos cloned Discovering --\u003e Generating: Docs found Generating --\u003e Processing: Config generated Processing --\u003e Indexing: Content copied Indexing --\u003e Rendering: Indexes created Rendering --\u003e Complete: Hugo finished Processing --\u003e Complete: Skip Hugo Cloning --\u003e Failed: Clone error Discovering --\u003e Failed: Discovery error Generating --\u003e Failed: Config error Processing --\u003e Failed: Copy error Rendering --\u003e Failed: Hugo error Failed --\u003e [*] Complete --\u003e [*] note right of Cloning May skip unchanged repos in incremental mode end note note right of Rendering Optional based on render_mode setting end note Repository State stateDiagram-v2 [*] --\u003e New New --\u003e Cloning: First build Cloning --\u003e Cloned: Success Cloned --\u003e Checking: Incremental build Checking --\u003e Unchanged: No changes Checking --\u003e Updating: Changes detected Updating --\u003e Updated: Pull success Updated --\u003e Ready: Verified Unchanged --\u003e Ready: Skip update Ready --\u003e [*] Cloning --\u003e Error: Network/auth failure Updating --\u003e Error: Pull failure Error --\u003e Retrying: Retryable Retrying --\u003e Cloning: Retry clone Retrying --\u003e Updating: Retry update Error --\u003e Failed: Max retries Failed --\u003e [*] Theme Configuration Loading stateDiagram-v2 [*] --\u003e Loading Loading --\u003e ApplyingDefaults: Config loaded ApplyingDefaults --\u003e MergingParams: Relearn defaults applied MergingParams --\u003e AddingModule: User params merged AddingModule --\u003e Ready: Module path added Ready --\u003e [*] note right of ApplyingDefaults Hardcoded Relearn defaults: themeVariant, disableSearch, collapsibleMenu, etc. end note note right of AddingModule Fixed module: github.com/McShelby/hugo-theme-relearn end note Deployment Architecture Single Instance ┌────────────────────────────────────────┐ │ Server Host │ │ │ │ ┌───────────────────────────────────┐ │ │ │ DocBuilder Binary │ │ │ │ │ │ │ │ ┌─────────┐ ┌────────────┐ │ │ │ │ │ CLI │ │ Daemon │ │ │ │ │ └─────────┘ └─────┬──────┘ │ │ │ │ │ │ │ │ │ ┌────▼─────┐ │ │ │ │ │ Server │ │ │ │ │ │ :8080 │ │ │ │ │ └──────────┘ │ │ │ └───────────────────────────────────┘ │ │ │ │ ┌───────────────────────────────────┐ │ │ │ Workspace │ │ │ │ /tmp/docbuilder-*/ │ │ │ └───────────────────────────────────┘ │ │ │ │ ┌───────────────────────────────────┐ │ │ │ State │ │ │ │ .docbuilder/state.json │ │ │ └───────────────────────────────────┘ │ └────────────────────────────────────────┘ High Availability ┌──────────────┐ │ Load Balancer│ │ (nginx) │ └──────┬───────┘ │ ┌─────────────────┼────────────────┐ │ │ │ ┌────▼────┐ ┌────▼────┐ ┌────▼────┐ │ Worker 1│ │ Worker 2│ │ Worker 3│ │ │ │ │ │ │ │:8080 │ │:8080 │ │:8080 │ └────┬────┘ └────┬────┘ └────┬────┘ │ │ │ └─────────────────┼────────────────┘ │ ┌────────▼────────┐ │ Shared Storage │ │ │ │ - Event Store │ │ - State DB │ │ - Output Files │ └─────────────────┘ References Comprehensive Architecture Architecture Overview Namespacing Rationale Architecture Migration Plan",
    "description": "This document provides visual representations of DocBuilder’s architecture using ASCII diagrams and Mermaid notation.\nLast Updated: December 16, 2025 - Reflects ADR-003 fixed transform pipeline implementation.\nThis document provides visual representations of DocBuilder’s architecture using ASCII diagrams and Mermaid notation.\nTable of Contents High-Level System Architecture Pipeline Flow Package Dependencies Data Flow Component Interactions State Machine Diagrams High-Level System Architecture Layer View ┌──────────────────────────────────────────────────────────────────┐ │ COMMAND LAYER │ │ (cmd/docbuilder/commands/) │ │ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────────────┐ │ │ │ Build │ │ Daemon │ │ Preview │ │ Discover │ │ │ │ (Kong) │ │ (Watch) │ │ (Live) │ │ (Analysis) │ │ │ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────────┬─────────┘ │ │ │ │ │ │ │ └───────┼─────────────┼─────────────┼─────────────────┼────────────┘ │ │ │ │ └─────────────┴─────────────┴─────────────────┘ │ ┌─────────────────────▼───────────────────────────────────────────┐ │ SERVICE LAYER │ │ (internal/build, internal/daemon) │ │ │ │ ┌────────────────┐ ┌─────────────────┐ ┌──────────────────┐ │ │ │ BuildService │ │ DaemonService │ │ DiscoveryService │ │ │ │ │ │ │ │ │ │ │ │ - Run() │ │ - Start() │ │ - Discover() │ │ │ │ - Validate() │ │ - Stop() │ │ - Report() │ │ │ └────────┬───────┘ └────────┬────────┘ └────────┬─────────┘ │ │ │ │ │ │ └───────────┼───────────────────┼────────────────────┼────────────┘ │ │ │ └───────────────────┴────────────────────┘ │ ┌───────────────────────────────▼─────────────────────────────────────┐ │ PROCESSING LAYER │ │ (internal/hugo, internal/docs, internal/hugo/pipeline) │ │ │ │ ┌──────────────────────────────────────────────────────────────┐ │ │ │ Hugo Generator │ │ │ │ │ │ │ │ ┌─────────────┐ ┌─────────────┐ ┌─────────────────────┐ │ │ │ │ │ Pipeline │ │ Theme │ │ Report Builder │ │ │ │ │ │ Processor │ │ System │ │ │ │ │ │ │ └──────┬──────┘ └──────┬──────┘ └──────────┬──────────┘ │ │ │ └─────────┼───────────────┼───────────────────┼────────────────┘ │ │ │ │ │ │ │ ┌─────────▼───────────────▼───────────────────▼──────────────┐ │ │ │ Fixed Transform Pipeline │ │ │ │ (internal/hugo/pipeline/) │ │ │ │ │ │ │ │ 1. parseFrontMatter - Extract YAML │ │ │ │ 2. normalizeIndexFiles - README → _index │ │ │ │ 3. buildBaseFrontMatter - Add defaults │ │ │ │ 4. extractIndexTitle - H1 extraction │ │ │ │ 5. stripHeading - Remove H1 │ │ │ │ 6. rewriteRelativeLinks - Fix .md links │ │ │ │ 7. rewriteImageLinks - Fix image paths │ │ │ │ 8. generateFromKeywords - Create from @keywords │ │ │ │ 9. addRepositoryMetadata - Inject repo info │ │ │ │ 10. addEditLink - Generate editURL │ │ │ │ 11. serializeDocument - Output YAML + content │ │ │ └──────────────────────────────────────────────────────────┘ │ └─────────────────────────────────┬───────────────────────────────────┘ │ ┌─────────────────────────────────▼────────────────────────────────┐ │ DOMAIN LAYER │ │ (internal/config, internal/state, internal/docs) │ │ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────────────┐ │ │ │ Config │ │ State │ │ DocFile │ │ Repository │ │ │ │ │ │ │ │ │ │ │ │ │ │ - Hugo │ │ - Git │ │ - Path │ │ - URL │ │ │ │ - Build │ │ - Docs │ │ - Trans │ │ - Branch │ │ │ │ - Forge │ │ - Build │ │ forms │ │ - Auth │ │ │ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────────┬─────────┘ │ └───────┼─────────────┼─────────────┼─────────────────┼────────────┘ │ │ │ │ └─────────────┴─────────────┴─────────────────┘ │ ┌─────────────────────▼───────────────────────────────────────────┐ │ INFRASTRUCTURE LAYER │ │ (internal/git, internal/forge, internal/workspace) │ │ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌─────────────────┐ │ │ │ Git │ │ Forge │ │ Event │ │ Workspace │ │ │ │ Client │ │ Clients │ │ Store │ │ Manager │ │ │ │ │ │ │ │ │ │ │ │ │ │ - Clone │ │ - GitHub │ │ - Append │ │ - Create() │ │ │ │ - Update │ │ - GitLab │ │ - Query │ │ - Cleanup() │ │ │ │ - Auth │ │ - Forgejo│ │ │ │ │ │ │ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────────┬────────┘ │ │ │ │ │ │ │ │ └─────────────┴─────────────┴─────────────────┘ │ │ │ │ │ ┌────────▼────────┐ │ │ │ Foundation │ │ │ │ Errors │ │ │ │ │ │ │ │ - ClassifiedErr │ │ │ │ - Categories │ │ │ │ - Retry Logic │ │ │ └─────────────────┘ │ └─────────────────────────────────────────────────────────────────┘ Pipeline Flow Sequential Stage Execution graph TD A[Build Request] --\u003e B[PrepareOutput Stage] B --\u003e C[CloneRepos Stage] C --\u003e D[DiscoverDocs Stage] D --\u003e E[GenerateConfig Stage] E --\u003e F[Layouts Stage] F --\u003e G[CopyContent Stage] G --\u003e H[Indexes Stage] H --\u003e I{Render Mode?} I --\u003e|always| J[RunHugo Stage] I --\u003e|auto| K{Has Hugo?} K --\u003e|yes| J K --\u003e|no| L[Skip Hugo] I --\u003e|never| L J --\u003e M[Build Complete] L --\u003e M M --\u003e N[Generate Report] N --\u003e O[Emit Events] O --\u003e P[Return Result] style B fill:#e1f5ff style C fill:#e1f5ff style D fill:#e1f5ff style E fill:#e1f5ff style F fill:#e1f5ff style G fill:#e1f5ff style H fill:#e1f5ff style J fill:#e1f5ff style M fill:#c8e6c9 style P fill:#c8e6c9 Stage Detail: CloneRepos CloneRepos Stage │ ├─ For each repository config: │ │ │ ├─ 1. Authenticate │ │ ├─ SSH Key │ │ ├─ Token │ │ └─ Basic Auth │ │ │ ├─ 2. Check Incremental │ │ ├─ Compare HEAD ref │ │ ├─ Check doc hash │ │ └─ Skip if unchanged │ │ │ ├─ 3. Clone or Update │ │ ├─ git clone (first time) │ │ └─ git pull (update) │ │ │ ├─ 4. Read HEAD │ │ └─ Store ref in state │ │ │ └─ 5. Emit Event │ ├─ RepositoryCloned │ └─ RepositoryUpdated │ └─ Update GitState Stage Detail: CopyContent CopyContent Stage │ ├─ For each DocFile: │ │ │ ├─ Fixed Transform Pipeline (11 sequential transforms) │ │ │ │ │ ├─ 1. Parse Front Matter │ │ │ ├─ Extract YAML header │ │ │ └─ Parse markdown content │ │ │ │ │ ├─ 2. Normalize Index Files │ │ │ └─ README.md → _index.md │ │ │ │ │ ├─ 3. Build Base Front Matter │ │ │ ├─ Add repository metadata │ │ │ ├─ Add section/path info │ │ │ ├─ Add forge info │ │ │ └─ Add date/timestamp │ │ │ │ │ ├─ 4. Extract Index Title │ │ │ ├─ Find first H1 heading │ │ │ └─ Set as page title │ │ │ │ │ ├─ 5. Strip Heading │ │ │ └─ Remove first H1 from content │ │ │ │ │ ├─ 6. Rewrite Relative Links │ │ │ ├─ Fix .md → / conversions │ │ │ └─ Resolve relative paths │ │ │ │ │ ├─ 7. Rewrite Image Links │ │ │ └─ Fix image path references │ │ │ │ │ ├─ 8. Generate from Keywords │ │ │ └─ Process @keywords directives │ │ │ │ │ ├─ 9. Add Repository Metadata │ │ │ └─ Inject repo context │ │ │ │ │ ├─ 10. Add Edit Link │ │ │ ├─ Check forge capabilities │ │ │ ├─ Build edit URL │ │ │ └─ Add to front matter │ │ │ │ │ └─ 11. Serialize Document │ │ ├─ Generate YAML │ │ └─ Combine with content │ │ │ └─ Write to content/ │ └─ Create target file │ └─ Update DocsState Package Dependencies Dependency Graph ┌──────────────────┐ │ cmd/docbuilder/ │ │ commands/ │ └────────┬─────────┘ │ ▼ ┌───────────────────┐ │ internal/build/ │ │ internal/daemon/ │ └────────┬──────────┘ │ ├────────────────────────────────┐ │ │ ▼ ▼ ┌──────────────┐ ┌──────────────┐ │ config/ │ │ state/ │ └──────┬───────┘ └──────┬───────┘ │ │ ├────────────────────────────────┤ │ │ ▼ ▼ ┌────────────────────────────────────────────────┐ │ Domain Layer │ │ ┌─────────┐ ┌──────────────┐ ┌───────────┐ │ │ │ docs/ │ │ hugo/ │ │ forge/ │ │ │ └────┬────┘ └──────┬───────┘ └─────┬─────┘ │ │ │ │ │ │ │ │ ┌────▼─────────┐ │ │ │ │ │ pipeline/ │ │ │ │ │ │ (transforms)│ │ │ │ │ └──────────────┘ │ │ └───────┼────────────────┼──────────────┼────────┘ │ │ │ └────────────────┴──────────────┘ │ ┌────────────────┴─────────────┐ │ │ ▼ ▼ ┌──────────────┐ ┌──────────────┐ │ git/ │ │ workspace/ │ └──────┬───────┘ └──────┬───────┘ │ │ └─────────────┬─────────────┘ │ ▼ ┌──────────────┐ │ foundation/ │ │ errors/ │ └──────────────┘ Import Rules Layer Dependencies (must respect):",
    "tags": [
      "Architecture",
      "Diagrams",
      "Visualization"
    ],
    "title": "Architecture Diagrams",
    "uri": "/docs/explanation/architecture-diagrams/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "DocBuilder implements a staged pipeline to turn multiple Git repositories into a unified Hugo documentation site.\nPipeline Flow Config → Clone → Discover → Generate Hugo Config → Transform Content → Index Pages → (Optional) Run Hugo Transform Content Stage executes the fixed transform pipeline:\nParse → Normalize → Build → Extract Title → Strip Heading → Rewrite Links → Rewrite Images → Keywords → Metadata → Edit Link → Serialize Each stage records duration, outcome, and issues for observability.\nKey Components Component Responsibility Location Config Loader Parse YAML, expand ${ENV} variables, apply defaults. internal/config/ Build Service Orchestrate build pipeline execution. internal/build/ Git Client Clone/update repositories with auth strategies (token, ssh, basic). internal/git/ Discovery Walk configured doc paths, filter markdown, build DocFile list. internal/docs/ Hugo Generator Emit hugo.yaml, content tree, index pages, theme params. internal/hugo/ Transform Pipeline Fixed-order content processing pipeline with direct mutation. internal/hugo/pipeline/ Relearn Theme Single hardcoded theme with specific parameter defaults (not extensible). internal/hugo/ (config_writer.go) Daemon Service Long-running HTTP service for incremental builds and monitoring. internal/daemon/ Forge Integration GitHub/GitLab/Forgejo API clients. internal/forge/ Error Foundation Classified error system with retry strategies. internal/foundation/errors/ Report Aggregate metrics \u0026 fingerprints for external tooling. internal/hugo/ Namespacing Logic Forge namespacing (conditional content/\u003cforge\u003e/\u003crepo\u003e/...) prevents collisions and yields scalable URL design. Auto mode activates only when more than one forge type exists.\nIdempotence \u0026 Change Detection Repository update strategy (clone_strategy) avoids unnecessary reclones. Delta Detection: QuickHash comparison tracks repository changes between builds quick_hash_diff: Git commit hash changed (most common) assumed_changed: Unable to verify, assumes changed for safety unknown: Change detection failed or unavailable Combined check: unchanged repo heads + identical doc file set ⇒ logged and optionally triggers early exit (when output already valid). Skip Evaluation: Daemon mode intelligently decides between full_rebuild, incremental, or skip doc_files_hash (SHA-256 of sorted content paths) offers external determinism for CI/CD. config_hash enables detection of configuration changes requiring full rebuilds. Error \u0026 Retry Model Error Classification (internal/foundation/errors):\nSeverity Levels:\nFatal - Stops execution completely Error - Fails the current operation Warning - Continues with degraded functionality Info - Informational, no impact Retry Strategies:\nRetryNever - Permanent failure RetryImmediate - Retry immediately RetryBackoff - Exponential backoff RetryRateLimit - Wait for rate limit window RetryUserAction - Requires user intervention Error Categories: User-facing (Config, Validation, Auth, NotFound), External (Network, Git, Forge), Build (Build, Hugo, FileSystem), Runtime (Runtime, Daemon, Internal)\nTransient classification guides retry policy (clone/update network issues; certain Hugo invocations).\nContent Generation Details Transform Pipeline (internal/hugo/pipeline/):\nEach markdown file passes through a fixed-order transform pipeline:\nparseFrontMatter - Extract YAML front matter from markdown normalizeIndexFiles - Rename README.md → _index.md for Hugo buildBaseFrontMatter - Generate default fields (title, type, date) extractIndexTitle - Extract H1 as title for index pages stripHeading - Remove H1 from content when appropriate rewriteRelativeLinks - Fix markdown links (.md → /, directory-style) rewriteImageLinks - Fix image paths to content root generateFromKeywords - Create new documents from keywords (, etc.) addRepositoryMetadata - Inject repository/forge/commit metadata addEditLink - Generate editURL for source links serializeDocument - Output final YAML + markdown Pipeline Features:\nFixed execution order (explicit, no dependency resolution needed) Direct document mutation (no patch merge complexity) Document type with all fields accessible Generators create missing index files before transforms run Theme Integration:\nSupported themes use Hugo Modules (no local theme directory needed) Theme-specific configuration for Relearn Index template override search order ensures safe customization Front matter includes forge, repository, section, editURL for theme logic Pruning Strategy Optional top-level pruning removes non-doc directories to shrink workspace footprint—controlled with allow/deny precedence rules to avoid accidental removal of required assets.\nDaemon Mode DocBuilder can run as a long-running HTTP service for incremental builds and continuous deployment:\nCore Features:\nIncremental Builds: Detects repository changes and rebuilds only affected content HTTP API: Endpoints for triggering builds, health checks, metrics Live Reload: Automatic browser refresh during development Build Queue: Manages concurrent build requests with retry logic State Persistence: Tracks repository state across restarts (daemon-state.json) Event Stream: Real-time build progress notifications Delta Detection Strategy (internal/daemon/delta_manager.go):\nCompare current repository commit hashes with last known state Classify changes: quick_hash_diff, assumed_changed, unknown Decide build strategy: full_rebuild, incremental, or skip Update state file after successful build Scheduler (internal/daemon/scheduler.go):\nPeriodic rebuild scheduling (cron-like intervals) Debouncing to prevent excessive builds Graceful shutdown with build completion Observability:\nPrometheus metrics (build duration, success rate, queue depth) Health endpoints (liveness, readiness) Structured logging with build metadata Design Rationale Highlights Concern Approach Cross-repo collisions Conditional forge prefix + repository segmentation. Performance Incremental fetch + pruning + shallow clones. Theming Module-based imports; param injection per theme. Observability Structured build report + issue taxonomy + stage timing. Reproducibility Environment expansion + explicit config + stable hashing. Extensibility Points Add new transform: Create function in internal/hugo/pipeline/transforms.go and add to defaultTransforms() list Add new generator: Create function in internal/hugo/pipeline/generators.go and add to defaultGenerators() list Theme customization: Relearn is hardcoded; customize via params in config or override templates in layouts/ (see use-relearn-theme.md) Additional issue codes: Augment taxonomy without breaking consumers Future caching: Leverage doc_files_hash for selective downstream regeneration Daemon endpoints: Add new HTTP handlers in internal/daemon/ for custom workflows Non-Goals Rendering arbitrary SSGs other than Hugo. Full-text search indexing logic (delegated to Hugo theme or external indexing).",
    "description": "DocBuilder implements a staged pipeline to turn multiple Git repositories into a unified Hugo documentation site.\nPipeline Flow Config → Clone → Discover → Generate Hugo Config → Transform Content → Index Pages → (Optional) Run Hugo Transform Content Stage executes the fixed transform pipeline:\nParse → Normalize → Build → Extract Title → Strip Heading → Rewrite Links → Rewrite Images → Keywords → Metadata → Edit Link → Serialize Each stage records duration, outcome, and issues for observability.",
    "tags": [
      "Architecture",
      "Design"
    ],
    "title": "Architecture Overview",
    "uri": "/docs/explanation/architecture/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "Build Report Reference DocBuilder writes a machine-readable build-report.json and a summary build-report.txt after each build.\nLifecycle Report initialized at pipeline start. Stages record durations, outcomes, issue codes. On early exit (no changes) outcome and timestamps are still finalized. Report persisted atomically (temp file then rename). JSON Fields Reference Core Metadata Field Type Description schema_version int Schema contract version (currently 1). doc_builder_version string DocBuilder version that generated the report. hugo_version string Hugo version detected during build. start time Build start timestamp (UTC). end time Build completion timestamp (UTC). outcome string Final build status: success, warning, failed, or canceled. Repository Statistics Field Type Description repositories int Count of repositories with at least one doc file. files int Count of discovered documentation files. cloned_repositories int Successful clone/update count. failed_repositories int Failed clone attempts. skipped_repositories int Repositories filtered out pre-clone. clone_stage_skipped bool Whether clone stage was skipped (incremental builds). Build Results Field Type Description rendered_pages int Markdown pages written to content directory. static_rendered bool Hugo build executed successfully. effective_render_mode string Actual render mode used: always, auto, or never. Stage Information Field Type Description stage_durations object Map of stage name → duration (nanoseconds, human-readable when pretty printed). stage_error_kinds object Map of stage name → error kind (fatal, warning, canceled). stage_counts object Detailed per-stage counts (success, skipped, failed). Error Tracking Field Type Description errors []string Fatal error messages that caused build abortion. warnings []string Non-fatal warning messages. issues []Issue Structured issue list (see Issues Array section). retries int Aggregate retry attempts across all stages. retries_exhausted bool True if any stage exhausted its retry budget. Incremental Build Fields Field Type Description doc_files_hash string Stable SHA‑256 hex of sorted Hugo content paths. config_hash string Configuration hash for change detection. delta_decision string Decision made for incremental build: full_rebuild, incremental, skip. delta_changed_repos []string List of repositories that changed (incremental builds). delta_repo_reasons object Map of repository name → change reason (quick_hash_diff, assumed_changed, unknown). skip_reason string Reason for early exit (e.g., no_changes, no_repositories). Template Information Field Type Description index_templates object Map of template kind → template info (source, exists, custom). pipeline_version int Content processing pipeline version number. Issues Array Each issue item:\n1 2 3 4 5 6 7 { \"code\": \"CLONE_FAILURE\", \"stage\": \"clone_repos\", \"severity\": \"error\", \"message\": \"fatal stage clone_repos: ...\", \"transient\": false } Codes include (non-exhaustive):\nCLONE_FAILURE PARTIAL_CLONE ALL_CLONES_FAILED DISCOVERY_FAILURE NO_REPOSITORIES HUGO_EXECUTION BUILD_CANCELED AUTH_FAILURE REPO_NOT_FOUND UNSUPPORTED_PROTOCOL REMOTE_DIVERGED GENERIC_STAGE_ERROR Hash Usage doc_files_hash doc_files_hash lets downstream tasks (search indexing, publishing) short-circuit when the doc set is identical between builds. Computed as SHA-256 of sorted Hugo content paths.\nconfig_hash config_hash enables daemon mode to detect configuration changes and trigger full rebuilds. Changes in repository URLs, paths, authentication, or Hugo settings invalidate incremental builds.\nIncremental Build Support When running in daemon/incremental mode, the report includes delta-specific fields:\ndelta_decision: Strategy chosen (full_rebuild, incremental, skip) delta_changed_repos: Repositories with changes detected delta_repo_reasons: Per-repository change detection method quick_hash_diff: Git commit hash changed assumed_changed: Unable to verify, assumed changed for safety unknown: Change detection failed or unavailable These fields enable observability into incremental build decisions and help diagnose unexpected full rebuilds.\nExample Report 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"schema_version\": 1, \"doc_builder_version\": \"2.1.0\", \"hugo_version\": \"0.139.3\", \"repositories\": 2, \"files\": 15, \"start\": \"2025-12-29T14:00:00Z\", \"end\": \"2025-12-29T14:00:05Z\", \"errors\": [], \"warnings\": [], \"stage_durations\": { \"prepare_output\": 5000000, \"clone_repos\": 1500000000, \"discover_docs\": 100000000, \"generate_config\": 10000000, \"copy_content\": 200000000, \"generate_indexes\": 50000000 }, \"stage_error_kinds\": {}, \"cloned_repositories\": 2, \"failed_repositories\": 0, \"skipped_repositories\": 0, \"rendered_pages\": 15, \"static_rendered\": false, \"retries\": 0, \"retries_exhausted\": false, \"outcome\": \"success\", \"doc_files_hash\": \"abc123def456...\", \"config_hash\": \"789ghi012jkl...\", \"issues\": [], \"skip_reason\": \"\", \"pipeline_version\": 1, \"effective_render_mode\": \"never\" } Stability Notes New fields may be added; existing fields will not be repurposed before v1.0.0. Treat unknown fields as optional to remain forward compatible. Fields marked omitempty may be absent if not applicable to the build type.",
    "description": "Build Report Reference DocBuilder writes a machine-readable build-report.json and a summary build-report.txt after each build.\nLifecycle Report initialized at pipeline start. Stages record durations, outcomes, issue codes. On early exit (no changes) outcome and timestamps are still finalized. Report persisted atomically (temp file then rename). JSON Fields Reference Core Metadata Field Type Description schema_version int Schema contract version (currently 1). doc_builder_version string DocBuilder version that generated the report. hugo_version string Hugo version detected during build. start time Build start timestamp (UTC). end time Build completion timestamp (UTC). outcome string Final build status: success, warning, failed, or canceled. Repository Statistics Field Type Description repositories int Count of repositories with at least one doc file. files int Count of discovered documentation files. cloned_repositories int Successful clone/update count. failed_repositories int Failed clone attempts. skipped_repositories int Repositories filtered out pre-clone. clone_stage_skipped bool Whether clone stage was skipped (incremental builds). Build Results Field Type Description rendered_pages int Markdown pages written to content directory. static_rendered bool Hugo build executed successfully. effective_render_mode string Actual render mode used: always, auto, or never. Stage Information Field Type Description stage_durations object Map of stage name → duration (nanoseconds, human-readable when pretty printed). stage_error_kinds object Map of stage name → error kind (fatal, warning, canceled). stage_counts object Detailed per-stage counts (success, skipped, failed). Error Tracking Field Type Description errors []string Fatal error messages that caused build abortion. warnings []string Non-fatal warning messages. issues []Issue Structured issue list (see Issues Array section). retries int Aggregate retry attempts across all stages. retries_exhausted bool True if any stage exhausted its retry budget. Incremental Build Fields Field Type Description doc_files_hash string Stable SHA‑256 hex of sorted Hugo content paths. config_hash string Configuration hash for change detection. delta_decision string Decision made for incremental build: full_rebuild, incremental, skip. delta_changed_repos []string List of repositories that changed (incremental builds). delta_repo_reasons object Map of repository name → change reason (quick_hash_diff, assumed_changed, unknown). skip_reason string Reason for early exit (e.g., no_changes, no_repositories). Template Information Field Type Description index_templates object Map of template kind → template info (source, exists, custom). pipeline_version int Content processing pipeline version number. Issues Array Each issue item:",
    "tags": [
      "Reports",
      "Builds",
      "Output"
    ],
    "title": "Build Reports Reference",
    "uri": "/docs/reference/report/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Builds",
    "uri": "/tags/builds/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "CI Architecture Detection and Container Builds This document explains how the CI workflow handles different runner architectures and modern container build approaches.\nArchitecture Support The CI workflow automatically detects the runner architecture and downloads the appropriate binaries for:\nHugo Extended: Required for Hugo modules and SCSS processing golangci-lint: Code quality linting Supported Architectures uname -m output Mapped to Description x86_64 amd64 Intel/AMD 64-bit aarch64 arm64 ARM 64-bit arm64 arm64 ARM 64-bit (macOS) Detection Logic 1 2 3 4 5 case \"$(uname -m)\" in x86_64) ARCH=\"amd64\" ;; aarch64|arm64) ARCH=\"arm64\" ;; *) echo \"Unsupported architecture: $(uname -m)\"; exit 1 ;; esac Container Builds with Kaniko The CI uses Kaniko for container image builds instead of Docker-in-Docker.\nWhy Kaniko? Traditional Docker-in-Docker Issues:\nRequires Docker daemon in CI environment Complex permission management (privileged containers) systemd/service management problems in containers Reliability issues with daemon startup Kaniko Advantages:\n✅ No Docker daemon required ✅ Works in any container environment ✅ Simpler security model ✅ Built-in registry authentication ✅ Eliminates Docker-in-Docker complexity Implementation 1 2 3 4 5 6 7 8 9 10 11 docker-build: runs-on: ubuntu-latest container: gcr.io/kaniko-project/executor:latest steps: - name: Build Docker image with Kaniko run: | /kaniko/executor \\ --dockerfile=Dockerfile \\ --context=. \\ --destination=registry/image:tag \\ --no-push # or push to registry Download URLs Hugo Extended x86_64: https://github.com/gohugoio/hugo/releases/download/v{VERSION}/hugo_extended_{VERSION}_linux-amd64.tar.gz ARM64: https://github.com/gohugoio/hugo/releases/download/v{VERSION}/hugo_extended_{VERSION}_linux-arm64.tar.gz golangci-lint x86_64: https://github.com/golangci/golangci-lint/releases/download/v{VERSION}/golangci-lint-{VERSION}-linux-amd64.tar.gz ARM64: https://github.com/golangci/golangci-lint/releases/download/v{VERSION}/golangci-lint-{VERSION}-linux-arm64.tar.gz Version Configuration Binary versions are configured at the top of each installation step:\n1 2 3 4 5 6 7 8 9 - name: Install Hugo run: | HUGO_VERSION=\"0.151.0\" # ... architecture detection and download - name: Download golangci-lint run: | GOLANGCI_VERSION=\"1.59.1\" # ... architecture detection and download Testing Use the provided test scripts to verify both approaches:\n1 2 3 4 5 6 7 8 # Test architecture detection and URL accessibility ./scripts/test-arch-detection.sh # Test complete CI setup (including Hugo/linter availability) ./scripts/test-ci-hugo.sh # Test Kaniko container build functionality ./scripts/test-kaniko-ci.sh Benefits Architecture Detection Multi-architecture support: Works on both x86_64 and ARM64 runners Faster builds: Downloads prebuilt binaries instead of compiling from source Reliable: Fails fast with clear error messages for unsupported architectures Maintainable: Architecture detection logic is centralized and consistent Container Build Migration Eliminates Docker daemon issues: No more startup failures or permission problems Better CI reliability: Consistent builds across different CI environments Simplified security: No privileged containers or complex volume mounts required Faster feedback: Immediate build failures instead of timeout loops",
    "description": "CI Architecture Detection and Container Builds This document explains how the CI workflow handles different runner architectures and modern container build approaches.\nArchitecture Support The CI workflow automatically detects the runner architecture and downloads the appropriate binaries for:\nHugo Extended: Required for Hugo modules and SCSS processing golangci-lint: Code quality linting Supported Architectures uname -m output Mapped to Description x86_64 amd64 Intel/AMD 64-bit aarch64 arm64 ARM 64-bit arm64 arm64 ARM 64-bit (macOS) Detection Logic 1 2 3 4 5 case \"$(uname -m)\" in x86_64) ARCH=\"amd64\" ;; aarch64|arm64) ARCH=\"arm64\" ;; *) echo \"Unsupported architecture: $(uname -m)\"; exit 1 ;; esac Container Builds with Kaniko The CI uses Kaniko for container image builds instead of Docker-in-Docker.",
    "tags": [
      "Continuous-Integration",
      "Docker",
      "Multi-Architecture"
    ],
    "title": "CI Architecture Detection",
    "uri": "/docs/ci-architecture-detection/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Ci-Cd",
    "uri": "/categories/ci-cd/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "DocBuilder CI/CD This document describes the CI/CD pipeline setup for DocBuilder using both GitHub Actions and Forgejo Actions.\nPipeline Overview DocBuilder supports CI/CD pipelines on multiple platforms:\nGitHub Actions Multi-architecture Docker image builds are available through GitHub Actions workflow (.github/workflows/docker-multiarch.yml).\nForgejo Actions Complete CI/CD pipeline including testing, building, and deployment is available through Forgejo Actions.\nGitHub Actions Workflows 1. Multi-Arch Docker Build (.github/workflows/docker-multiarch.yml) Builds and pushes multi-architecture Docker images to GitHub Container Registry (GHCR).\nTriggered on:\nPush to main branch Git tags matching v*.*.* (e.g., v1.0.0) Manual workflow dispatch Features:\nMulti-architecture builds (linux/amd64, linux/arm64) Automatic tagging strategy: Semver tags: v1.2.3, v1.2, v1 Branch tags: main, develop SHA tags: main-abc1234 latest tag for releases GitHub Container Registry (ghcr.io) integration Build caching with GitHub Actions cache Comprehensive build summaries Usage:\nThe workflow runs automatically on tag pushes. To manually trigger:\nGo to Actions → “Build and Push Multi-Arch Docker Images” Click “Run workflow” Optionally specify Hugo version Choose whether to push images to registry Pulling images:\n1 2 3 4 5 6 7 8 # Latest release docker pull ghcr.io/inful/docbuilder:latest # Specific version docker pull ghcr.io/inful/docbuilder:v1.0.0 # Branch build docker pull ghcr.io/inful/docbuilder:main Registry Authentication:\nThe workflow uses the built-in GITHUB_TOKEN for authentication. No additional secrets are required.\nForgejo Actions Workflows The CI/CD pipeline consists of several workflows:\n1. Main CI/CD Pipeline (.forgejo/workflows/ci.yml) Triggered on:\nPush to main, master, or develop branches Pull requests to these branches Release publications Jobs:\nTest - Runs tests, linting, and formatting checks Build - Compiles the binary and uploads artifacts Docker Build - Builds and tests Docker images Integration Test - Tests the complete DocBuilder + Hugo pipeline Security Scan - Scans Docker images for vulnerabilities Deploy Staging - Deploys to staging on main branch pushes Deploy Production - Deploys to production on releases 2. Maintenance Pipeline (.forgejo/workflows/maintenance.yml) Triggered:\nWeekly schedule (Sundays at 2 AM UTC) Manual dispatch Jobs:\nAutomated dependency updates Hugo version updates Artifact cleanup Setup Requirements 1. Forgejo Secrets Configure these secrets in your Forgejo repository settings:\nREGISTRY_TOKEN - Token for container registry authentication 2. Container Registry The pipeline assumes you’re using a container registry at git.luguber.info. Update the REGISTRY environment variable in the workflow if using a different registry.\n3. Environments Configure these environments in Forgejo:\nstaging - For staging deployments production - For production deployments (with approval required) Local Development Using Docker Compose Build and test locally:\n1 2 3 4 5 # Build the application docker-compose build # Run DocBuilder docker-compose up docbuilder Development with Hugo server:\n1 2 3 4 # Run with development profile (includes Hugo server) docker-compose --profile dev up # Access at http://localhost:1313 Monitoring stack:\n1 2 3 4 5 # Run with monitoring profile docker-compose --profile monitoring up # Prometheus: http://localhost:9090 # Grafana: http://localhost:3000 (admin/admin) Manual Testing Test binary build:\n1 2 make build ./bin/docbuilder --version Run tests:\n1 2 make test make test-coverage Test Docker image:\n1 2 docker build -t docbuilder:test . docker run --rm docbuilder:test --version Pipeline Features Caching Go module cache for faster builds Docker layer cache for efficient image builds Multi-platform Support Docker images built for linux/amd64 and linux/arm64 GitHub Actions: Uses Docker Buildx with QEMU for cross-platform builds Forgejo Actions: Uses Docker Buildx with native multi-arch support Automatic platform detection and optimization Testing Unit tests with coverage reporting Integration tests with real Hugo builds Docker functionality tests Security vulnerability scanning Artifact Management Binary artifacts uploaded for each build Docker images tagged with branch, SHA, and semver Automatic cleanup of old artifacts Quality Gates Code formatting validation Linting with golangci-lint Test coverage reporting Security scanning with Trivy Deployment Strategy Staging Automatic deployment on main branch pushes Uses latest successful build Environment: staging Production Manual deployment trigger via GitHub releases Requires environment approval Uses versioned tags Environment: production Customization Adding New Tests Add test files following Go conventions (*_test.go). The pipeline will automatically discover and run them.\nModifying Build Process Update the Makefile targets. The CI pipeline uses these targets:\nmake deps - Install dependencies make fmt - Format code make lint - Run linting make build - Build binary make test-coverage - Run tests with coverage Changing Docker Configuration Modify the Dockerfile and docker-compose.yml as needed. The pipeline will use the updated configuration.\nEnvironment Variables Key environment variables in the pipeline:\nGO_VERSION - Go version to use REGISTRY - Container registry URL IMAGE_NAME - Docker image name Troubleshooting Common Issues Build Failures\nCheck Go version compatibility Verify all dependencies are available Review test output for specific failures Docker Build Issues\nEnsure Dockerfile syntax is correct Check if all required files are included in build context Verify base images are available Registry Authentication\nEnsure REGISTRY_TOKEN secret is configured Verify token has push permissions to the registry Check registry URL is correct Debug Commands 1 2 3 4 5 6 7 8 # Local test run make dev # Docker build test docker build --no-cache -t docbuilder:debug . # Integration test docker run --rm -v $(pwd)/test-config.yaml:/config.yaml docbuilder:debug build -c /config.yaml Monitoring The pipeline includes monitoring and observability:\nMetrics: Prometheus metrics collection Logs: Structured logging with slog Health Checks: Docker health checks Performance: Build time and resource usage tracking Access monitoring dashboards:\nPrometheus: Available in monitoring profile Grafana: Pre-configured dashboards for DocBuilder metrics",
    "description": "DocBuilder CI/CD This document describes the CI/CD pipeline setup for DocBuilder using both GitHub Actions and Forgejo Actions.\nPipeline Overview DocBuilder supports CI/CD pipelines on multiple platforms:\nGitHub Actions Multi-architecture Docker image builds are available through GitHub Actions workflow (.github/workflows/docker-multiarch.yml).",
    "tags": [
      "Continuous-Integration",
      "Continuous-Deployment",
      "Docker",
      "Gitea"
    ],
    "title": "CI/CD Setup Guide",
    "uri": "/docs/ci-cd-setup/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Cli",
    "uri": "/tags/cli/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "Primary commands (Kong-based CLI):\nCommand Description build Run full pipeline (clone → discover → generate config → copy content → indexes → (optional) Hugo run). init Create an example configuration file. discover (If present) Run discovery only (debug content paths). daemon Run long-lived service (if implemented in current codebase). Global Flags (Common) Flag Purpose -c, --config Path to YAML config file. -v Verbose logging. --version Print version/build info. Environment Variables (Behavior Modifiers) Variable Effect --render-mode always Force running Hugo after scaffolding. --render-mode never Force skipping Hugo even when enabled in config. Build Report Outputs Generated in output directory:\nbuild-report.json — machine-readable summary (contains doc_files_hash). build-report.txt — human summary line. Key JSON fields:\nField Meaning repositories Number of repositories that produced at least one doc file. files Number of discovered documentation files. outcome Final build result. One of: success, warning, failed, canceled. cloned_repositories Successfully cloned or updated repos. failed_repositories Repos that failed clone/auth. rendered_pages Markdown files written to content directory. static_rendered True if Hugo was run and succeeded. doc_files_hash Stable fingerprint of doc file set. issues[] Structured issue entries (code, stage, severity, message, transient). Exit Codes Condition Exit Code Success 0 Fatal error Non-zero (varies by underlying error path) Canceled (context) Non-zero Logging Highlights Clone stage logs per-repo successes/failures and update method (fast-forward vs already up-to-date). Discovery stage logs unchanged doc set when identical to prior run (and repository heads unchanged). Early exit path logs when entire pipeline is skipped due to no changes and valid prior output.",
    "description": "Primary commands (Kong-based CLI):\nCommand Description build Run full pipeline (clone → discover → generate config → copy content → indexes → (optional) Hugo run). init Create an example configuration file. discover (If present) Run discovery only (debug content paths). daemon Run long-lived service (if implemented in current codebase). Global Flags (Common) Flag Purpose -c, --config Path to YAML config file. -v Verbose logging. --version Print version/build info. Environment Variables (Behavior Modifiers) Variable Effect --render-mode always Force running Hugo after scaffolding. --render-mode never Force skipping Hugo even when enabled in config. Build Report Outputs Generated in output directory:",
    "tags": [
      "Cli",
      "Commands",
      "Usage"
    ],
    "title": "CLI Reference",
    "uri": "/docs/reference/cli/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Code-Organization",
    "uri": "/tags/code-organization/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Coding-Standards",
    "uri": "/tags/coding-standards/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Commands",
    "uri": "/tags/commands/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "Comprehensive Architecture Documentation Table of Contents Overview Core Principles System Architecture Package Structure Data Flow Key Subsystems Extension Points Operational Considerations Overview DocBuilder is a Go CLI tool and daemon that aggregates documentation from multiple Git repositories into a unified Hugo static site. It implements a staged pipeline architecture with event sourcing, typed configuration/state management, and comprehensive observability.\nKey Characteristics Event-Driven: Build lifecycle modeled as events in an event store Type-Safe: Strongly typed configuration and state (no map[string]any in primary paths) Observable: Unified error system, structured logging, metrics, and tracing Incremental: Change detection and partial rebuilds for performance Multi-Tenant: Supports forge namespacing and per-repository configuration Theme-Aware: Hugo theme integration via modules (Relearn only) Core Principles 1. Clean Architecture The codebase follows clean architecture principles with clear dependency direction:\npresentation → application → domain → infrastructure ↓ ↓ ↓ ↓ cmd/ services/ forge/ git/ cli/ pipeline/ config/ storage/ server/ state/ workspace/ Dependency Rules:\nInner layers never depend on outer layers Domain logic has no infrastructure dependencies Infrastructure adapters implement domain interfaces 2. Event Sourcing Build lifecycle is captured as events in an immutable event store:\n1 2 3 4 5 6 type Event struct { ID string Timestamp time.Time Type EventType Data json.RawMessage } Event Types:\nBuildStarted, BuildCompleted, BuildFailed RepositoryCloned, RepositoryUpdated DocumentationDiscovered HugoSiteGenerated 3. Typed State Management State is decomposed into focused sub-states:\n1 2 3 4 5 type BuildState struct { Git *GitState // Repository management Docs *DocsState // Documentation discovery Pipeline *PipelineState // Execution metadata } Each sub-state has:\nClear ownership boundaries Validation methods JSON serialization Test builders 4. Unified Error Handling All errors use internal/foundation/errors.ClassifiedError:\n1 2 3 4 5 6 7 8 type ClassifiedError struct { category ErrorCategory // Type-safe category enum severity ErrorSeverity // Fatal, Error, Warning, Info retry RetryStrategy // Never, Immediate, Backoff, RateLimit, User message string cause error context ErrorContext // map[string]any } Error Categories:\nCategoryConfig, CategoryValidation (non-retryable, user-facing) CategoryNetwork, CategoryGit (retryable with backoff) CategoryFileSystem (transient, retry immediate) CategoryAuth, CategoryNotFound (non-retryable) Key Features:\nType-safe categories and severity levels Built-in retry semantics HTTP/CLI adapters for boundary translation Fluent builder API for error construction Structured context via WithContext(key, value) System Architecture High-Level Components ┌───────────────────────────────────────────────────────────┐ │ Presentation Layer │ │ ┌──────────┐ ┌──────────┐ ┌───────────┐ ┌──────────┐ │ │ │ CLI │ │ Daemon │ │ Server │ │ Tests │ │ │ └─────┬────┘ └─────┬────┘ └─────┬─────┘ └─────┬────┘ │ └────────┼─────────────┼─────────────┼──────────────┼───────┘ │ │ │ │ └─────────────┴─────────────┴──────────────┘ │ ┌─────────────▼─────────────┐ │ Service Layer │ │ ┌─────────────────────┐ │ │ │ BuildService │ │ │ │ PreviewService │ │ │ │ DiscoveryService │ │ │ └──────────┬──────────┘ │ └─────────────┼─────────────┘ │ ┌─────────────▼─────────────┐ │ Pipeline Layer │ │ ┌─────────────────────┐ │ │ │ StageExecutor │◄─┤ PrepareOutput │ │ PipelineRunner │◄─┤ CloneRepos │ │ ChangeDetector │◄─┤ DiscoverDocs │ └─────────┬───────────┘◄─┤ GenerateConfig └────────────┼──────────────┤ Layouts │ ┤ CopyContent ┌────────────▼──────────────┤ Indexes │ Domain Layer ┤ RunHugo │ ┌─────────────────────┐ │ │ │ Config │ │ │ │ State │ │ │ │ DocFile │ │ │ │ Repository │ │ │ └─────────┬───────────┘ │ └────────────┼──────────────┘ │ ┌────────────▼──────────────┐ │ Infrastructure Layer │ │ ┌─────────────────────┐ │ │ │ Git Client │ │ │ │ Forge Clients │ │ │ │ Storage │ │ │ │ Workspace Manager │ │ │ │ Event Store │ │ │ └─────────────────────┘ │ └───────────────────────────┘ Pipeline Stages The build process executes 8 sequential stages:\n1. PrepareOutput → Initialize directories 2. CloneRepos → Git operations 3. DiscoverDocs → Find markdown files 4. GenerateConfig → Create hugo.yaml 5. Layouts → Copy theme templates 6. CopyContent → Process markdown with transforms 7. Indexes → Generate index pages 8. RunHugo → Render static site (optional) Each stage:\nImplements StageExecutor interface Records duration and outcome Emits events to event store Returns typed errors Package Structure Foundation Packages internal/foundation/ Core types used across all layers:\nerrors/ - Unified error system (DocBuilderError) validation/ - Validation result types logging/ - Structured logging setup internal/config/ Configuration management:\nconfig/ ├── v2.go # YAML loading with env expansion ├── validation.go # Top-level validation orchestration ├── typed/ # Domain-specific config structs │ ├── hugo_config.go │ ├── daemon_config.go │ └── forge_config.go └── normalize.go # Configuration normalization Key Types:\nConfig - Root configuration RepositoryConfig - Per-repo settings HugoConfig - Hugo site configuration BuildConfig - Build behavior settings internal/state/ State management:\nstate/ ├── build_state.go # Root build state ├── git_state.go # Repository state ├── docs_state.go # Documentation discovery state ├── pipeline_state.go # Execution metadata └── store/ # State persistence ├── json_*.go # JSON-based stores └── helpers.go # Common store utilities Core Domain Packages internal/forge/ Git hosting platform abstraction:\nforge/ ├── base_forge.go # Common HTTP operations ├── github.go # GitHub implementation ├── gitlab.go # GitLab implementation ├── forgejo.go # Forgejo/Gitea implementation └── capabilities.go # Feature detection Key Abstractions:\nForge interface - Platform operations BaseForge - Shared HTTP client Capabilities - Feature flags (webhooks, tokens, etc.) internal/git/ Git operations:\ngit/ ├── git.go # Client implementation ├── auth.go # Authentication strategies ├── workspace.go # Workspace management └── head.go # HEAD reference reading Auth Methods:\nSSH keys Personal access tokens Basic username/password internal/docs/ Documentation discovery:\ndocs/ ├── discovery.go # File discovery logic ├── doc_file.go # DocFile model └── filters.go # Ignore patterns Discovery Rules:\nWalk configured paths Filter .md and .markdown files Ignore README.md, CONTRIBUTING.md, etc. Respect .docignore files internal/hugo/ Hugo site generation:\nhugo/ ├── generator.go # Main generator ├── config.go # hugo.yaml generation ├── content_copy.go # Content processing ├── index.go # Index page generation ├── runner.go # Hugo binary execution ├── models/ # Data models │ ├── frontmatter.go │ ├── editlink.go │ └── config.go └── pipeline/ # Fixed transform pipeline (ADR-003) ├── transforms.go # All 11 transforms └── generators.go # Document generators Content Pipeline:\nParse Front Matter ↓ Build Front Matter (metadata injection) ↓ Edit Link Injection ↓ Merge Front Matter ↓ Apply Transforms ↓ Serialize Front Matter Infrastructure Packages internal/workspace/ Temporary directory management:\n1 2 3 4 5 6 type Manager struct { basePath string } func (m *Manager) Create() (string, error) func (m *Manager) Cleanup() error Lifecycle:\nCreates timestamped temp directories Tracks creation for cleanup Safe concurrent operations internal/storage/ (Removed) Note: This package was removed to simplify CLI build complexity. The daemon’s skip evaluation uses internal/state instead.\nHistorical Purpose: Content-addressed storage for CLI incremental builds with hash-based object paths, Put/Get/Delete/List operations, and garbage collection.\ninternal/eventstore/ Event persistence:\neventstore/ ├── store.go # Event store interface ├── memory.go # In-memory implementation └── file.go # File-based implementation Operations:\nAppend events (immutable) Query by type, time range, correlation ID Projections for state reconstruction Application Packages internal/services/ Service interfaces for lifecycle management:\nservices/ └── interfaces.go # ManagedService, StateManager interfaces Defines contracts for service lifecycle (Start, Stop, Health) and state persistence (Load, Save).\ninternal/build/ Sequential pipeline for documentation generation:\nbuild/ ├── service.go # BuildService interface ├── default_service.go # Default pipeline executor ├── stages.go # Stage definitions └── report.go # Build reporting Executes pipeline stages (PrepareOutput → CloneRepos → DiscoverDocs → GenerateConfig → Layouts → CopyContent → Indexes → RunHugo).\ninternal/incremental/ (Removed) Note: This package was removed to simplify CLI build complexity. It overlapped with the daemon’s skip evaluation system (which uses internal/state with rule-based validation).\nHistorical Purpose: Change detection for CLI incremental builds. Compared repository HEAD refs, hashed discovered documentation files, and cached signatures to skip unchanged repositories.\nPresentation Packages cmd/docbuilder/commands Command-line interface:\ncmd/docbuilder/ ├── main.go # CLI entry point └── commands/ ├── build.go # Build command ├── init.go # Init command ├── discover.go # Discovery command ├── daemon.go # Daemon command ├── preview.go # Preview command ├── generate.go # Generate command ├── visualize.go # Visualize command └── common.go # Shared helpers Uses Kong for argument parsing. Error handling via internal/foundation/errors CLI adapter.\ninternal/server/ HTTP server:\nserver/ ├── server.go # Server setup ├── handlers/ # Request handlers │ ├── webhook.go │ ├── build.go │ ├── status.go │ └── metrics.go ├── middleware/ # Middleware │ ├── logging.go │ ├── auth.go │ └── recovery.go └── responses/ # Response types Testing Packages internal/testing/ Test utilities:\ntesting/ ├── config_builder.go # Fluent config builders ├── file_assertions.go # File/directory assertions ├── cli_runner.go # CLI integration testing └── fixtures.go # Test data internal/testforge/ Forge test doubles:\ntestforge/ ├── mock_forge.go # Mock forge implementation └── README.md Data Flow Build Flow User invokes CLI/API ↓ BuildService.Build(config) ↓ PipelineRunner.Run(stages) ↓ ┌─────────────────────────────────┐ │ Stage 1: PrepareOutput │ │ - Create/clean output dirs │ │ - Initialize staging │ └────────────┬────────────────────┘ ↓ ┌─────────────────────────────────┐ │ Stage 2: CloneRepos │ │ - Authenticate with forges │ │ - Clone/update repositories │ │ - Detect changes │ └────────────┬────────────────────┘ ↓ ┌─────────────────────────────────┐ │ Stage 3: DiscoverDocs │ │ - Walk documentation paths │ │ - Filter markdown files │ │ - Build DocFile list │ │ - Compute doc set hash │ └────────────┬────────────────────┘ ↓ ┌─────────────────────────────────┐ │ Stage 4: GenerateConfig │ │ - Load theme capabilities │ │ - Apply theme params │ │ - Merge user config │ │ - Write hugo.yaml │ └────────────┬────────────────────┘ ↓ ┌─────────────────────────────────┐ │ Stage 5: Layouts │ │ - Copy custom layouts │ │ - Set up index templates │ └────────────┬────────────────────┘ ↓ ┌─────────────────────────────────┐ │ Stage 6: CopyContent │ │ - Parse front matter │ │ - Inject metadata │ │ - Add edit links │ │ - Apply transforms │ │ - Write to content/ │ └────────────┬────────────────────┘ ↓ ┌─────────────────────────────────┐ │ Stage 7: Indexes │ │ - Generate _index.md files │ │ - Repository indexes │ │ - Section indexes │ └────────────┬────────────────────┘ ↓ ┌─────────────────────────────────┐ │ Stage 8: RunHugo (optional) │ │ - Execute hugo build │ │ - Generate public/ │ └────────────┬────────────────────┘ ↓ BuildReport generated ↓ Events persisted ↓ Metrics updated ↓ Return to caller Configuration Loading Flow 1. Load YAML file ↓ 2. Expand ${ENV_VAR} references ↓ 3. Parse into Config struct ↓ 4. Apply defaults ↓ 5. Normalize (fill implicit values) ↓ 6. Validate (orchestration) ├→ ValidateHugoConfig() ├→ ValidateDaemonConfig() ├→ ValidateForgeConfig() └→ ValidateRepositories() ↓ 7. Return validated Config State Persistence Flow In-Memory State (BuildState) ↓ Sub-State Updates ├→ GitState.Update() ├→ DocsState.AddDocFile() └→ PipelineState.RecordStage() ↓ State Store Operations ├→ DaemonInfoStore.Save() ├→ StatisticsStore.Save() └→ JSONStore.Save() ↓ Filesystem Persistence └→ .docbuilder/state.json Event Flow Pipeline Stage Execution ↓ Emit Event(s) ├→ BuildStarted ├→ RepositoryCloned ├→ DocumentationDiscovered └→ BuildCompleted ↓ Event Store Append ↓ Event Handlers (async) ├→ Metrics Update ├→ Webhook Notification └→ Log Aggregation Key Subsystems 1. Relearn Theme Configuration DocBuilder is now hardcoded to use the Relearn theme exclusively. Theme configuration is applied directly in the Hugo generator without a plugin system or theme registry.\nConfiguration Location: internal/hugo/config_writer.go\n1 2 3 4 5 6 7 func (g *Generator) applyRelearnThemeDefaults(params map[string]any) { // Set Relearn-specific defaults // - themeVariant // - disableSearch // - disableLandingPageButton // etc. } Hugo Config Generation:\nCore defaults (title, baseURL, markup) Relearn-specific defaults (via applyRelearnThemeDefaults) User param deep-merge (user values override) Dynamic fields (build_date) Hardcoded module import: github.com/McShelby/hugo-theme-relearn Language configuration for i18n No Theme Abstraction:\nPrevious multi-theme system removed No theme registry or plugin system Relearn configuration is hardcoded Simplifies codebase and maintenance 2. Forge Integration Forges implement the Forge interface:\n1 2 3 4 5 6 7 8 type Forge interface { Name() string Type() string GetRepository(owner, repo string) (*Repository, error) GetFileContent(owner, repo, path, ref string) ([]byte, error) ListRepositories(org string) ([]*Repository, error) Capabilities() Capabilities } HTTP Consolidation: All forge clients use BaseForge for common operations:\n1 2 3 4 5 6 type BaseForge struct { client *http.Client baseURL string authHeader string customHeaders map[string]string } Auth Methods:\nGitHub: Bearer token + custom headers GitLab: Bearer token Forgejo: Token prefix + dual event headers 3. Change Detection The incremental system uses multiple strategies:\n1 2 3 type ChangeDetector interface { DetectChanges(repos []*RepositoryConfig) (*ChangeSet, error) } Detection Levels:\nRepository HEAD - Git ref comparison Quick Hash - Fast directory tree hashing Doc Files Hash - SHA-256 of sorted Hugo paths Deletion Detection - Optional file removal tracking Skip Conditions:\nUnchanged HEAD ref Identical doc set hash No deletions detected (if enabled) 4. Content Transform Pipeline Content processing uses a pipeline pattern:\n1 2 3 type Transformer interface { Transform(ctx context.Context, doc *DocFile) error } Built-in Transformers:\nFrontMatterParser - Extract YAML headers FrontMatterBuilder - Add metadata EditLinkInjector - Generate edit URLs FrontMatterMerger - Combine metadata FrontMatterSerializer - Write YAML Custom Transformers: Users can add custom transforms in config:\n1 2 3 4 5 hugo: content_transforms: - type: replace pattern: \"{{OLD}}\" replacement: \"{{NEW}}\" 5. Observability Stack Logging Structured logging with slog:\n1 2 3 4 logger.Info(\"Documentation discovered\", \"repository\", repoName, \"files\", fileCount, ) Metrics Prometheus-compatible metrics:\n1 2 3 buildDuration.Observe(duration.Seconds()) buildsTotal.WithLabelValues(\"success\").Inc() reposProcessed.WithLabelValues(repoName).Inc() Tracing Context-based distributed tracing:\n1 2 ctx, span := tracer.Start(ctx, \"clone-repository\") defer span.End() Error Handling All errors use unified type:\n1 2 3 4 5 6 7 8 9 10 return foundation.NewDocBuilderError( foundation.ErrCodeGitClone, \"failed to clone repository\", err, foundation.WithContext(map[string]any{ \"repository\": repoURL, \"branch\": branch, }), foundation.WithRetryable(true), ) Extension Points 1. Custom Stages Add new pipeline stages:\n1 2 3 4 5 6 7 8 9 10 11 12 type CustomStage struct { config *config.Config } func (s *CustomStage) Execute(ctx context.Context, state *state.BuildState) error { // Custom logic return nil } func (s *CustomStage) Name() string { return \"custom\" } Register in pipeline configuration.\n2. Custom Transformers Implement transformer interface:\n1 2 3 4 5 6 type CustomTransformer struct {} func (t *CustomTransformer) Transform(ctx context.Context, doc *docs.DocFile) error { // Modify doc.Content or doc.FrontMatter return nil } Register in content copy stage.\n3. Custom Stores Implement state store interface:\n1 2 3 4 5 6 7 8 9 10 11 type CustomStore struct {} func (s *CustomStore) Save(ctx context.Context, data any) error { // Custom persistence return nil } func (s *CustomStore) Load(ctx context.Context) (any, error) { // Custom retrieval return nil, nil } Register in state management.\n4. Event Handlers Subscribe to build events:\n1 2 3 4 5 6 type CustomHandler struct {} func (h *CustomHandler) Handle(ctx context.Context, event *eventstore.Event) error { // React to events return nil } Register with event store.\nOperational Considerations Performance Incremental Builds:\nEnable with build.incremental: true Typically 10-100x faster for unchanged repos Requires persistent workspace Pruning:\nEnable with pruning.enabled: true Removes non-doc directories Reduces workspace size by 50-90% Shallow Clones:\nEnable with git.shallow: true Depth 1 clones Faster for large repositories Scalability Multi-Tenancy:\nPer-tenant configuration Isolated workspaces Resource quotas Horizontal Scaling:\nStateless build workers Shared event store Load balancer distribution Resource Limits:\nMemory: ~100MB base + 10MB per repo CPU: 1-2 cores for typical builds Disk: 500MB workspace + output size Reliability Error Recovery:\nRetryable errors auto-retry (3x default) Partial build state preserved Atomic staging promotion Health Checks:\n/health endpoint Workspace availability Git connectivity Hugo binary presence Monitoring:\nBuild success/failure rates Stage durations Repository update lag Disk usage Security Authentication:\nToken-based API auth SSH key management Credential encryption at rest Authorization:\nRepository access control API endpoint permissions Webhook signature verification Secrets Management:\nEnvironment variable expansion .env file support External secret stores (planned) Maintenance Configuration Updates:\nRestart daemon to apply new configuration Validate before restart State Management:\nState stored in .docbuilder/ JSON format for portability Manual cleanup supported Dependency Management:\nGo modules for dependencies Hugo binary external Theme modules auto-fetched Architecture Decision Records See docs/adr/ for detailed architectural decisions:\nADR-000: Uniform Error Handling ADR-001: Forge Integration Daemon Migration Status The architecture has undergone significant evolution. See ARCHITECTURE_MIGRATION_PLAN.md for:\n19 completed phases (A-M, O-P, R-S-T-U) 2 deferred phases (Q, J) ~1,290 lines eliminated Zero breaking changes Current State: Architecture migration complete. Codebase follows:\nEvent-driven patterns Typed configuration/state Unified observability Single execution pipeline Clean domain boundaries References Getting Started Tutorial CLI Reference Configuration Reference Build Report Reference Forge Integration",
    "description": "Comprehensive Architecture Documentation Table of Contents Overview Core Principles System Architecture Package Structure Data Flow Key Subsystems Extension Points Operational Considerations Overview DocBuilder is a Go CLI tool and daemon that aggregates documentation from multiple Git repositories into a unified Hugo static site. It implements a staged pipeline architecture with event sourcing, typed configuration/state management, and comprehensive observability.",
    "tags": [
      "Architecture",
      "Design",
      "Deep-Dive"
    ],
    "title": "Comprehensive Architecture",
    "uri": "/docs/explanation/comprehensive-architecture/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Configuration",
    "uri": "/tags/configuration/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "This page enumerates the primary configuration sections and fields supported by DocBuilder for both direct build and daemon modes.\nTop-Level Structure 1 2 3 4 5 6 repositories: [] # List of repositories to aggregate build: {} # Performance \u0026 workspace tuning daemon: {} # Daemon mode settings (link verification, sync, storage) versioning: {} # Multi-version documentation (optional) hugo: {} # Hugo site metadata \u0026 theme output: {} # Output directory behavior Repositories Field Type Required Description url string yes Git clone URL. name string yes Unique repository name (used in content paths). branch string no Branch to checkout (default per remote). paths []string no Documentation root paths (default: [“docs”]). auth.type enum no Authentication mode: token, ssh, or basic. auth.token string conditional Required when type=token. auth.username string conditional Required when type=basic. auth.password string conditional Required when type=basic. auth.key_path string conditional SSH private key path when type=ssh. Build Section Field Type Default Description clone_concurrency int 4 Parallel clone/update workers (bounded to repo count). clone_strategy enum fresh Repository acquisition mode: fresh, update, or auto. shallow_depth int 0 If \u003e0 use shallow clones of that depth. prune_non_doc_paths bool false Remove non-doc top-level directories after clone. prune_allow []string [] Keep-listed directories/files (glob). prune_deny []string [] Force-remove directories/files (glob) except .git. hard_reset_on_diverge bool false Force align local branch to remote on divergence. clean_untracked bool false Remove untracked files after successful update. max_retries int 2 Retry attempts for transient clone/update failures. retry_backoff enum linear Backoff strategy: fixed, linear, or exponential. retry_initial_delay duration 1s First retry delay. retry_max_delay duration 30s Maximum backoff delay cap. workspace_dir string derived Explicit workspace override path. namespace_forges enum auto Forge prefixing: auto, always, or never. skip_if_unchanged bool daemon:true, CLI:false Skip builds when nothing changed (daemon only). Daemon Section Configuration for daemon mode operation, including link verification, sync scheduling, and storage paths.\nLink Verification Automated link validation using NATS for caching and event publishing. Requires NATS server with JetStream enabled.\nField Type Default Description enabled bool true Enable automatic link verification after builds. nats_url string nats://localhost:4222 NATS server connection URL (supports clustering). subject string docbuilder.links.broken NATS subject for publishing broken link events. kv_bucket string docbuilder-link-cache KV bucket name for caching link verification results. cache_ttl duration 24h TTL for successful link checks in cache. cache_ttl_failures duration 1h TTL for failed link checks in cache. max_concurrent int 10 Maximum concurrent link verification requests. request_timeout duration 10s HTTP timeout for link verification requests. rate_limit_delay duration 100ms Delay between link verification requests. verify_external_only bool false Verify only external links (skip internal links). skip_edit_links bool true Skip edit links that require authentication. follow_redirects bool true Follow HTTP redirects during verification. max_redirects int 3 Maximum number of redirects to follow. Link Verification Examples Basic Configuration:\n1 2 3 4 daemon: link_verification: enabled: true nats_url: \"nats://localhost:4222\" Remote NATS with Authentication:\n1 2 3 4 5 6 daemon: link_verification: enabled: true nats_url: \"nats://username:password@nats.example.com:4222\" subject: \"docbuilder.links.broken\" kv_bucket: \"prod-link-cache\" NATS Cluster Configuration:\n1 2 3 4 daemon: link_verification: enabled: true nats_url: \"nats://server1:4222,nats://server2:4222,nats://server3:4222\" TLS/Secure Connection:\n1 2 3 4 daemon: link_verification: enabled: true nats_url: \"tls://nats.example.com:4222\" Custom Verification Settings:\n1 2 3 4 5 6 7 8 9 10 daemon: link_verification: enabled: true nats_url: \"nats://localhost:4222\" cache_ttl: \"48h\" # Cache successful checks for 2 days cache_ttl_failures: \"30m\" # Recheck failures after 30 minutes max_concurrent: 20 # Increase parallelism request_timeout: \"15s\" # Longer timeout for slow sites verify_external_only: true # Skip internal link checks skip_edit_links: true # Skip edit links (default, requires auth) Include Edit Links in Verification:\n1 2 3 4 daemon: link_verification: enabled: true skip_edit_links: false # Verify edit links (will likely fail without auth) Disable Link Verification:\n1 2 3 daemon: link_verification: enabled: false NATS Requirements Link verification requires NATS with JetStream enabled:\n1 2 3 4 5 6 7 nats-server -js jetstream { store_dir: /var/lib/nats max_memory_store: 1GB max_file_store: 10GB } Sync Configuration Field Type Default Description schedule string */5 * * * * Cron expression for periodic repository sync. Storage Configuration Field Type Default Description output_dir string ./site Output directory (must match output.directory). repo_cache_dir string - Persistent repository cache directory. Daemon Configuration Example 1 2 3 4 5 6 7 8 9 daemon: link_verification: enabled: true nats_url: \"nats://localhost:4222\" cache_ttl: \"24h\" sync: schedule: \"*/10 * * * *\" # Sync every 10 minutes storage: repo_cache_dir: \"./daemon-data/repos\" Versioning Section Enables multi-version documentation by discovering and building multiple branches/tags from each repository.\nField Type Default Description enabled bool false Enable multi-version documentation. strategy enum branches_and_tags Version selection: branches_and_tags, branches_only, or tags_only. default_branch_only bool false Build only the default branch (overrides strategy). branch_patterns []string [\"*\"] Branch name patterns to include (glob). tag_patterns []string [\"*\"] Tag name patterns to include (glob). max_versions_per_repo int 10 Maximum versions to build per repository. Versioning Examples 1 2 3 4 5 6 7 8 9 10 11 versioning: enabled: true strategy: branches_and_tags max_versions_per_repo: 5 tag_patterns: - \\\"v*\\\" # Match semantic versions - \\\"[0-9]*\\\" # Match numeric tags branch_patterns: - \\\"main\\\" - \\\"develop\\\" - \\\"release/*\\\" With versioning enabled, DocBuilder:\nDiscovers available branches/tags from each repository Expands each repository into multiple versioned builds Clones each version separately (branches use refs/heads/, tags use refs/tags/) Organizes content under repository-version paths Generates Hugo configuration with version metadata for version switchers Hugo Section Field Type Description title string Site title. description string Site description. base_url string Hugo BaseURL. params map[string]any Relearn theme parameters (optional). taxonomies map[string]string Custom taxonomy definitions (optional). Note: Theme selection has been removed. DocBuilder uses the Relearn theme exclusively.\nRelearn Theme Parameters Customize Relearn theme behavior via hugo.params:\n1 2 3 4 5 6 hugo: params: themeVariant: \"relearn-dark\" disableSearch: false collapsibleMenu: true showVisitedLinks: true See Use Relearn Theme for complete parameter reference.\nTaxonomies Hugo taxonomies allow you to classify and organize content. DocBuilder automatically configures the default Hugo taxonomies (tags and categories) but you can customize or extend them.\nDefault Configuration:\n1 2 3 4 hugo: taxonomies: tag: tags category: categories Custom Taxonomies:\n1 2 3 4 5 6 hugo: taxonomies: tag: tags category: categories author: authors # Custom taxonomy topic: topics # Custom taxonomy The taxonomy key (e.g., tag, author) is the singular form used in Hugo’s templates and URLs, while the value (e.g., tags, authors) is the plural form used for the collection.\nUsage in Frontmatter:\nOnce configured, you can use taxonomies in your markdown frontmatter:\n1 2 3 4 5 6 7 --- title: \"My Documentation Page\" tags: [\"api\", \"golang\", \"tutorial\"] categories: [\"guides\"] authors: [\"john-doe\"] topics: [\"backend-development\"] --- DocBuilder’s FrontMatter model supports tags, categories, and keywords fields by default. Custom taxonomies can be added through the Custom field or by extending the FrontMatter structure.\nOutput Section Field Type Default Description directory string ./site Output root. clean bool true Remove directory before build. Output Directory Unification DocBuilder treats output.directory as the canonical output root for both direct builds and daemon mode. In daemon mode, daemon.storage.output_dir must match output.directory. If not provided, it is derived from output.directory. A validation check enforces this equality (after path normalization). Mismatches cause configuration loading to fail. Recommendation: set only output.directory; avoid setting daemon.storage.output_dir unless absolutely necessary. Build Report Fields (Selected) Field Purpose cloned_repositories Successful clones/updates failed_repositories Failed clone/auth attempts rendered_pages Markdown pages written static_rendered Hugo run succeeded doc_files_hash Fingerprint of docs set issues[] Structured issue list Environment Variable Expansion Values like ${GIT_ACCESS_TOKEN} in YAML are expanded using the current process environment. .env and .env.local files are loaded automatically (last one wins on conflicts).\nHealth and Readiness Endpoints Endpoints are exposed on both the docs port and the admin port. GET /health: basic liveness endpoint; returns 200 when the server is responsive. GET /ready: readiness endpoint tied to render state. Returns 200 only when \u003coutput.directory\u003e/public exists. Returns 503 before the first successful render or if the public folder is missing. When serving on the docs port, if the site is not yet rendered and the request path is /, DocBuilder returns a short 503 HTML placeholder indicating that the documentation is being prepared. This switches automatically to the rendered site once available. Kubernetes Probes Probe either the docs port or the admin port. Use /ready for readiness and /health for liveness. Prefer probing /ready instead of the docs root /, since the root may return a temporary 503 placeholder before the first render. Example: probe the docs port (8080)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 containers: - name: docbuilder ports: - containerPort: 8080 # docs - containerPort: 8081 # webhooks - containerPort: 8082 # admin readinessProbe: httpGet: path: /ready port: 8080 periodSeconds: 5 failureThreshold: 3 livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 5 periodSeconds: 10 Example: probe the admin port (8082)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 containers: - name: docbuilder readinessProbe: httpGet: path: /ready port: 8082 periodSeconds: 5 failureThreshold: 3 livenessProbe: httpGet: path: /health port: 8082 initialDelaySeconds: 5 periodSeconds: 10 Namespacing Behavior When namespace_forges=auto and more than one distinct forge is present across repositories, content paths are written under content/\u003cforge\u003e/\u003crepo\u003e/.... Otherwise they remain content/\u003crepo\u003e/....\nSkip Evaluation (Daemon Mode) When running in daemon mode, builds are automatically skipped when nothing has changed:\nState Tracking: Repository commits, config hash, and doc file hashes are saved after each successful build Skip Rules: Validates version, config, repository commits, and content integrity Automatic: Enabled by default in daemon mode (skip_if_unchanged: true) CLI Mode: Not used - CLI builds always run when explicitly requested This prevents unnecessary rebuilds when daemon polls/watches for changes but repositories haven’t been updated.\nFor CLI mode, simply don’t run docbuilder build if you don’t want a build. No caching is needed.\nRecommendations Use clone_strategy: auto for most CI and daemon scenarios. Pin commit or module versions externally until v1.0.0 stability. Compare successive doc_files_hash values to drive conditional downstream jobs.",
    "description": "This page enumerates the primary configuration sections and fields supported by DocBuilder for both direct build and daemon modes.\nTop-Level Structure 1 2 3 4 5 6 repositories: [] # List of repositories to aggregate build: {} # Performance \u0026 workspace tuning daemon: {} # Daemon mode settings (link verification, sync, storage) versioning: {} # Multi-version documentation (optional) hugo: {} # Hugo site metadata \u0026 theme output: {} # Output directory behavior Repositories Field Type Required Description url string yes Git clone URL. name string yes Unique repository name (used in content paths). branch string no Branch to checkout (default per remote). paths []string no Documentation root paths (default: [“docs”]). auth.type enum no Authentication mode: token, ssh, or basic. auth.token string conditional Required when type=token. auth.username string conditional Required when type=basic. auth.password string conditional Required when type=basic. auth.key_path string conditional SSH private key path when type=ssh. Build Section Field Type Default Description clone_concurrency int 4 Parallel clone/update workers (bounded to repo count). clone_strategy enum fresh Repository acquisition mode: fresh, update, or auto. shallow_depth int 0 If \u003e0 use shallow clones of that depth. prune_non_doc_paths bool false Remove non-doc top-level directories after clone. prune_allow []string [] Keep-listed directories/files (glob). prune_deny []string [] Force-remove directories/files (glob) except .git. hard_reset_on_diverge bool false Force align local branch to remote on divergence. clean_untracked bool false Remove untracked files after successful update. max_retries int 2 Retry attempts for transient clone/update failures. retry_backoff enum linear Backoff strategy: fixed, linear, or exponential. retry_initial_delay duration 1s First retry delay. retry_max_delay duration 30s Maximum backoff delay cap. workspace_dir string derived Explicit workspace override path. namespace_forges enum auto Forge prefixing: auto, always, or never. skip_if_unchanged bool daemon:true, CLI:false Skip builds when nothing changed (daemon only). Daemon Section Configuration for daemon mode operation, including link verification, sync scheduling, and storage paths.",
    "tags": [
      "Configuration",
      "Yaml",
      "Settings"
    ],
    "title": "Configuration Reference",
    "uri": "/docs/reference/configuration/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "Content Transform Pipeline (DEPRECATED) ⚠️ DEPRECATED: This document describes the old registry-based transform system that was removed on December 16, 2025.\nFor current pipeline documentation, see:\nADR-003: Fixed Transform Pipeline Architecture Overview Implementation: internal/hugo/pipeline/ Historical Documentation This document explains DocBuilder’s old markdown content transform architecture: how the registry worked, built‑in transforms, and the merge semantics for front matter.\nGoals Deterministic, ordered transformation of markdown files before writing to the Hugo content/ tree. Extensibility: adding a new transform should not require editing existing ones. Behavioral parity with the legacy inline pipeline (now validated via parity + conflict tests). Safe front matter augmentation with explicit conflict reporting. High-Level Flow Source markdown (optionally with YAML front matter) discovered. Registry pipeline executes transformers in ascending priority order. Each transformer mutates a PageShim (bridging to the internal Page). Front matter is parsed, patches are accumulated, merged, then content and merged front matter are serialized back to bytes. Transform Coverage All transforms registered in the pipeline apply to all markdown files including:\nRegular documentation pages README.md files README.md files promoted to _index.md (repository/section indexes) Custom index pages The index generation stage uses already-transformed content via DocFile.TransformedBytes, ensuring transforms are never bypassed. Previously, the index stage would re-read source files and bypass the transform pipeline, but this has been fixed as of ADR-002 implementation.\nCapability System Forge Capabilities (internal/forge/capabilities.go): Each forge type declares feature support flags:\nSupportsEditLinks - whether the forge can generate web edit URLs SupportsWebhooks - whether web hook integration is possible SupportsTopics - whether repository topics/tags are supported Theme Capabilities (internal/hugo/theme/capabilities.go): Each theme declares UI integration flags:\nWantsPerPageEditLinks - whether theme UI expects editURL front matter SupportsSearchJSON - whether theme can consume search index JSON These capability maps are snapshotted by golden tests to detect unintentional changes. Adding a new forge or theme requires updating the corresponding capability map and golden test expectations.\nRegistry \u0026 Ordering Transforms implement:\n1 2 3 4 5 6 type Transformer interface { Name() string // stable identifier, lowercase snake_case Stage() TransformStage // execution stage (parse, build, enrich, etc.) Dependencies() TransformDependencies // explicit ordering constraints Transform(PageAdapter) error } Registration occurs in init() via transforms.Register(t). The registry produces an ordered slice by:\nStage order (parse → build → enrich → merge → transform → finalize → serialize) Dependency resolution within each stage using topological sort Current Transform Stages Stage Transforms Purpose parse front_matter_parser Extract existing front matter \u0026 strip it build front_matter_builder_v2 Generate baseline fields (title/date/repository/forge/section/metadata) (no editURL) enrich edit_link_injector_v2 Adds editURL with set-if-missing semantics using centralized resolver \u0026 capability-gated theme detection merge front_matter_merge Apply ordered patches into merged map transform relative_link_rewriter Rewrite intra-repo markdown links to Hugo-friendly paths (strip .md) finalize strip_first_heading, shortcode_escaper, hextra_type_enforcer Post-process content serialize front_matter_serialize Serialize merged front matter + body Why split builder and edit link? Decoupling eliminates implicit coupling between title/metadata generation and theme-specific edit link logic, enabling future themes to provide alternative edit URL policies or disable them entirely via transform filters.Edit Link Generation (Post-Consolidation): Edit links are now generated exclusively through hugo.EditLinkResolver which centralizes path normalization, forge type detection, and theme capability checking. The previous fmcore.ResolveEditLink function has been removed. This ensures consistent behavior and eliminates the risk of docs/docs/ path duplication. Theme capability flags (ThemeCapabilities.WantsPerPageEditLinks) and forge capability flags (ForgeCapabilities.SupportsEditLinks) gate edit link injection at the transform level.\nRemoved legacy transforms (front_matter_builder, edit_link_injector) under the greenfield policy (no backward compatibility shims maintained). If you had explicit allowlists containing them, replace with their V2 counterparts.\nPageShim \u0026 Hooks PageShim (in internal/hugo/transforms/defaults.go) exposes only required fields + function hooks. The transform layer now operates via facade-style getters/setters and an adapter that allows future direct PageFacade implementations. Custom transformers MUST avoid reaching into unlisted struct fields; rely on the facade methods below. A golden test (pipeline_golden_test.go) locks baseline behavior.\nAs of 2025-09-30 the shim gained a BackingAddPatch hook so facade patches registered by transforms propagate to the underlying concrete Page early (supporting conflict recording that previously happened only during final merge).\nPageFacade Methods (Current Stable Set) Method Purpose GetContent() Retrieve mutable markdown body (without serialized front matter). SetContent(string) Replace markdown body in-place. GetOriginalFrontMatter() Access parsed original front matter (immutable baseline). SetOriginalFrontMatter(map[string]any, bool) Set baseline front matter \u0026 had-front-matter flag (used by parser). AddPatch(FrontMatterPatch) Append a pending front matter patch (also forwarded via BackingAddPatch). ApplyPatches() Merge pending patches into MergedFrontMatter. HadOriginalFrontMatter() Whether the source file originally contained front matter. Serialize() Serialize merged front matter + body (facade method; serializer transform delegates here). The stability test (page_facade_stability_test.go) enforces that this method set does not change without deliberate update. Treat additions as a versioned change—prefer helper functions if possible. Serialize() was promoted into the facade (2025-09-30) eliminating the special serializer closure path.\nSupporting Functions The transform system uses these standalone functions to process pages:\nBuildFrontMatter(FrontMatterInput) – Generates front matter from file metadata, config, and existing values (in internal/hugo/frontmatter.go). Used by front_matter_builder_v2 transform. ApplyPatches() – Merges pending patches into final front matter (facade method on Page.applyPatches). Front Matter Patching Semantics Patches (FrontMatterPatch) are applied in transform execution order (stage + dependencies); later patches win unless keys are protected.\nMerge modes:\nMergeDeep – recursively merges maps, with array strategy heuristics. MergeReplace – overwrites targeted keys entirely. MergeSetIfMissing – sets keys only if absent (records kept_original conflicts otherwise). Array strategies (effective via effectiveArrayStrategy):\nDefault heuristics: Taxonomies (tags, categories, keywords) → union when existing. outputs → union. resources → append. Otherwise replace. Explicit overrides via patch ArrayStrategy take precedence (ArrayUnion, ArrayAppend, ArrayReplace). Conflict Recording FrontMatterConflict{Key, Original, Attempt, Source, Action} actions:\nkept_original – protected or set-if-missing existing key retained overwritten – value replaced by later patch set_if_missing – value added by set-if-missing patch Taxonomy unions and new-key additions without replacement do not generate conflicts.\nSee transform_conflicts_test.go for locked expectations.\nAdding a New Transform Select a stage (StageParse, StageBuild, StageEnrich, StageMerge, StageTransform, StageFinalize, or StageSerialize). Declare dependencies on other transforms using MustRunAfter and/or MustRunBefore. Implement a struct with Name(), Stage(), Dependencies(), Transform(PageAdapter) error. Register in init() inside a new file under internal/hugo/transforms/ (keep single concern per file when possible). Access PageShim via type assertion (shim, ok := p.(*PageShim)); guard if absent. Mutate only shim.Content or attach front matter patches through hooks if you need to prepend/append modifications; avoid directly mutating internal maps outside merge stage unless you know the ordering implications. Add unit tests: Ordering: confirm your transform appears at expected position relative to neighbors. Behavior: given sample content, assert expected modifications. If transform alters front matter, add/update conflict tests if behavior touches protected keys. Example Skeleton 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 type CodeBlockCounter struct{} func (t CodeBlockCounter) Name() string { return \"code_block_counter\" } func (t CodeBlockCounter) Stage() TransformStage { return StageTransform // Runs during content transformation stage } func (t CodeBlockCounter) Dependencies() TransformDependencies { return TransformDependencies{ MustRunAfter: []string{\"relative_link_rewriter\"}, } } func (t CodeBlockCounter) Transform(p transforms.PageAdapter) error { shim, ok := p.(*transforms.PageShim) if !ok { return nil } count := strings.Count(shim.Content, \"```\") / 2 // Add patch with set-if-missing semantics shim.AddPatch(fmcore.FrontMatterPatch{ Key: \"code_block_count\", Value: count, Mode: fmcore.MergeSetIfMissing, }) return nil } func init() { transforms.Register(CodeBlockCounter{}) } Testing Strategy Current safety nets:\nParity hash tests (extended scenarios) ensure registry mirrors legacy semantics. Conflict logging test locks merge outcome \u0026 conflict classification. Front matter builder tests ensure time injection \u0026 edit link logic stable. Ordering golden test ensures deterministic registry order. Capability golden tests snapshot forge and theme feature matrices. No-reflection guard test prevents accidental reflection usage in transforms. Path normalization tests ensure edit links handle multi-level docs bases correctly. Recommended for new transforms:\nUnit test its isolated mutation. A golden test if serialization changes (normalize volatile fields like timestamps). Future Enhancements The project is intentionally and permanently self-contained. We will NOT introduce a dynamic external plugin / extension / runtime injection mechanism. All transforms and behavioral changes must land via normal code changes and review.\nPlanned/possible incremental internal improvements (still in-repo only):\nPer-transform param injection sourced from config (extending existing enable/disable filtering). Evaluating parallelization for pure content transforms after merge but before serialization. Additional shared metrics dimensions (e.g., bytes processed, patch counts) leveraging existing duration/error counters. Continued pruning or consolidation of transforms if responsibilities shift (greenfield policy). Migration Notes Legacy inline transformer pipeline fully removed; registry is authoritative. The project charter forbids third‑party or dynamically loaded transformer plugins—this will not change. All runtime transformer code lives in-repo. Greenfield policy: we remove obsolete paths aggressively and accept minor backward incompatibilities during pre‑1.0 to keep surface area minimal.\nQuestions or additions? Extend this doc as the pipeline evolves.",
    "description": "Content Transform Pipeline (DEPRECATED) ⚠️ DEPRECATED: This document describes the old registry-based transform system that was removed on December 16, 2025.\nFor current pipeline documentation, see:\nADR-003: Fixed Transform Pipeline Architecture Overview Implementation: internal/hugo/pipeline/ Historical Documentation This document explains DocBuilder’s old markdown content transform architecture: how the registry worked, built‑in transforms, and the merge semantics for front matter.",
    "tags": [
      "Transforms",
      "Content-Processing",
      "Deprecated"
    ],
    "title": "Content Transforms Reference (DEPRECATED)",
    "uri": "/docs/reference/content-transforms/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Content-Transforms",
    "uri": "/tags/content-transforms/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Continuous-Deployment",
    "uri": "/tags/continuous-deployment/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Continuous-Integration",
    "uri": "/tags/continuous-integration/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Customization",
    "uri": "/tags/customization/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Deep-Dive",
    "uri": "/tags/deep-dive/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Deprecated",
    "uri": "/tags/deprecated/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Design",
    "uri": "/tags/design/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Design-Decisions",
    "uri": "/tags/design-decisions/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Development",
    "uri": "/categories/development/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Development",
    "uri": "/tags/development/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Diagrams",
    "uri": "/tags/diagrams/index.html"
  },
  {
    "breadcrumb": "Test",
    "content": "Welcome to the DocBuilder documentation! This documentation follows the Diátaxis framework, organizing content by user needs.\nDocumentation Structure 📚 Tutorials Learning-oriented - Step-by-step lessons to get you started.\nGetting Started - Your first DocBuilder project Multi-Repository Setup - Aggregate docs from multiple repos Theme Customization - Customize your Hugo theme Best for: First-time users, learning the basics\n🛠️ How-To Guides Task-oriented - Practical guides for specific tasks.\nAdd Content Transforms - Create custom markdown transformations Add Theme Support - Integrate a new Hugo theme Configure Forge Namespacing - Set up multi-forge projects Customize Index Pages - Tailor index page generation Enable Hugo Render - Configure Hugo rendering Prune Workspace Size - Optimize disk usage Run Incremental Builds - Speed up builds Best for: Users with specific goals, solving problems\n📖 Reference Information-oriented - Technical specifications and API documentation.\nCLI Reference - Command-line interface documentation Configuration Reference - Complete config file specification Index File Handling - How index files are processed and precedence rules Build Report Reference - Build report format and fields Best for: Looking up specific information, API details\n💡 Explanation Understanding-oriented - Conceptual documentation and design rationale.\nArchitecture Documentation Architecture Documentation Index - Start here for architecture overview Comprehensive Architecture - Complete system design Architecture Diagrams - Visual system representations Package Architecture Guide - Detailed package documentation Architecture Overview - Quick reference guide Namespacing Rationale - Forge namespacing design Renderer Testing - Hugo rendering tests Architecture Decision Records (ADRs) ADR-000: Uniform Error Handling ADR-001: Forge Integration Daemon Best for: Understanding why things work the way they do, architectural decisions\nQuick Start Guide New Users Start with Getting Started Tutorial Review CLI Reference for commands Check Configuration Reference for options Developers/Contributors Read Architecture Documentation Index Study Comprehensive Architecture Review Package Architecture Guide Check Contributing Guide Operations/DevOps Review Getting Started Tutorial Check How-To: Run Incremental Builds Review Build Report Reference Study operational considerations in Comprehensive Architecture Documentation by Feature Basic Usage Getting Started Tutorial CLI Reference Configuration Reference Multi-Repository Aggregation Multi-Repository Setup Tutorial Configure Forge Namespacing Namespacing Rationale Theme Integration Theme Customization Tutorial Add Theme Support Relearn Theme Configuration Performance Optimization Run Incremental Builds Prune Workspace Size Change Detection Customization Add Content Transforms Customize Index Pages Index File Handling Theme Customization Tutorial Configuration Reference Advanced Topics Comprehensive Architecture Package Architecture Guide Architecture Diagrams Additional Resources Project Documentation README - Project overview and quick reference CHANGELOG - Version history and changes CONTRIBUTING - Contribution guidelines LICENSE - Project license Architecture \u0026 Planning Architecture Migration Plan - Migration status Plan Directory - Feature plans and ADRs Docs Archive - Historical documentation Examples Examples Directory - Sample configurations and tools Example Configs - Ready-to-use configurations Documentation Principles This documentation follows these principles:\nUser-Centric - Organized by what users want to achieve Progressive Disclosure - Start simple, add complexity as needed Searchable - Clear structure, consistent terminology Maintained - Updated with code changes Tested - Examples are verified to work Contributing to Documentation We welcome documentation contributions! When contributing:\nFollow the Diátaxis framework structure Use clear, concise language Include code examples where applicable Test all commands and configurations Update index files when adding new documents See Contributing Guide for details.\nGetting Help Questions: Open a GitHub Discussion Issues: Report bugs or feature requests via GitHub Issues Architecture Questions: Review Architecture Documentation first Usage Help: Start with Tutorials and How-To Guides Documentation Status Last Major Update: December 2025\nCoverage:\n✅ Getting Started Tutorial ✅ CLI Reference ✅ Configuration Reference ✅ Build Report Reference ✅ Comprehensive Architecture Documentation ✅ Package-Level Documentation ✅ Visual Architecture Diagrams ✅ How-To Guides ⏳ Additional tutorials (in progress) Feedback: Documentation feedback is highly appreciated! Please open an issue if you find areas that need improvement.",
    "description": "Welcome to the DocBuilder documentation! This documentation follows the Diátaxis framework, organizing content by user needs.\nDocumentation Structure 📚 Tutorials Learning-oriented - Step-by-step lessons to get you started.\nGetting Started - Your first DocBuilder project Multi-Repository Setup - Aggregate docs from multiple repos Theme Customization - Customize your Hugo theme Best for: First-time users, learning the basics",
    "tags": [
      "Overview",
      "Getting-Started"
    ],
    "title": "DocBuilder Documentation",
    "uri": "/docs/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Docker",
    "uri": "/tags/docker/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Docsy",
    "uri": "/tags/docsy/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Documentation",
    "uri": "/categories/documentation/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "Architecture Documentation Index This directory contains comprehensive architecture documentation for DocBuilder. The documentation is organized to provide different views and levels of detail for various audiences.\nDocumentation Structure High-Level Overview Architecture Overview (architecture.md)\nQuick reference for the staged pipeline Key components and responsibilities Namespacing and idempotence strategies Design rationale highlights Audience: New developers, product managers, technical leads Comprehensive Documentation Comprehensive Architecture (comprehensive-architecture.md)\nComplete system architecture with all layers Core principles (clean architecture, event sourcing, typed state) Detailed package structure and responsibilities Data flow diagrams and sequences Key subsystems deep dive (Relearn configuration, forges, change detection) Extension points and operational considerations Migration status summary Audience: Senior engineers, architects, contributors Architecture Diagrams (architecture-diagrams.md)\nVisual representations using ASCII and Mermaid System architecture diagrams Pipeline flow visualizations Package dependency graphs Component interaction sequences State machine diagrams Deployment architectures Audience: Visual learners, architects, documentation reviewers Package Architecture Guide (package-architecture.md)\nDetailed package-by-package documentation Key types and interfaces for each package Usage patterns and code examples Design rationale for architectural decisions Dependency rules and import matrix Testing support infrastructure Audience: Contributors, maintainers, package users Specialized Topics Namespacing Rationale (namespacing-rationale.md)\nForge-level namespacing design Collision prevention strategies Configuration options Audience: Users configuring multi-forge setups Renderer Testing (renderer-testing.md)\nHugo rendering test strategies Golden test patterns Audience: Contributors working on Hugo integration Quick Navigation By Role New Developer Getting Started:\nStart with Architecture Overview Review Architecture Diagrams for visual understanding Deep dive into Package Architecture Guide for code structure Contributing to Core Pipeline:\nRead Comprehensive Architecture - Pipeline Flow section Study Package Architecture Guide - Pipeline section Review Architecture Diagrams - Pipeline Flow Adding New Theme Support:\nRead Comprehensive Architecture - Theme System Review Package Architecture Guide - internal/hugo section Follow How-To Guide Implementing Forge Integration:\nRead Comprehensive Architecture - Forge Integration Study Package Architecture Guide - internal/forge section Review Namespacing Rationale Understanding State Management:\nReview Comprehensive Architecture - State Management Study Package Architecture Guide - internal/state section Check Architecture Diagrams - State Persistence Flow By Topic Configuration System:\nComprehensive Architecture Package Architecture Guide Configuration Reference Pipeline Stages:\nArchitecture Overview Architecture Diagrams Package Architecture Guide Event Sourcing:\nComprehensive Architecture Package Architecture Guide Error Handling:\nComprehensive Architecture Package Architecture Guide ADR-000: Uniform Error Handling Relearn Configuration:\nComprehensive Architecture Architecture Diagrams How-To: Use Relearn Theme Change Detection:\nComprehensive Architecture Architecture Diagrams Package Architecture Guide Testing Infrastructure:\nPackage Architecture Guide Renderer Testing Architecture Evolution The architecture has undergone significant evolution documented in:\nArchitecture Migration Plan 19 completed phases (A-M, O-P, R-S-T-U) 2 deferred phases (Q, J) ~1,290 lines eliminated Zero breaking changes Architecture Decision Records (ADRs) For detailed architectural decisions, see the ADR directory:\nADR-000: Uniform Error Handling ADR-001: Forge Integration Daemon Visual Reference System Layers ┌─────────────────────────┐ │ Presentation Layer │ cmd/docbuilder/commands, server/ ├─────────────────────────┤ │ Application Layer │ build/, services/, daemon/ ├─────────────────────────┤ │ Domain Layer │ config/, state/, docs/, hugo/, forge/ ├─────────────────────────┤ │ Infrastructure Layer │ git/, workspace/, eventstore/, auth/ ├─────────────────────────┤ │ Foundation Layer │ foundation/errors, logfields/, metrics/ └─────────────────────────┘ Pipeline Stages PrepareOutput → CloneRepos → DiscoverDocs → GenerateConfig → Layouts → CopyContent → Indexes → RunHugo (optional) Key Principles Clean Architecture - Clear dependency direction Event Sourcing - Immutable event log Typed State - No map[string]any in primary paths Unified Errors - Single error type system Observable - Logging, metrics, tracing Contributing to Architecture When making architectural changes:\nUpdate relevant documentation in this directory Add ADR for significant decisions Update diagrams if structure changes Update migration plan if part of migration Update package guide for API changes Questions and Feedback For questions about the architecture:\nCheck existing documentation first Review ADRs for historical context Open discussion in GitHub issues Tag architecture-related PRs with architecture label Related Documentation Getting Started Tutorial How-To Guides Reference Documentation Contributing Guide Changelog Last Updated: December 2025\nArchitecture Status: Migration complete (19 phases), production-ready",
    "description": "Architecture Documentation Index This directory contains comprehensive architecture documentation for DocBuilder. The documentation is organized to provide different views and levels of detail for various audiences.\nDocumentation Structure High-Level Overview Architecture Overview (architecture.md)",
    "tags": [
      "Documentation"
    ],
    "title": "Explanation Documentation",
    "uri": "/docs/explanation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Forge",
    "uri": "/tags/forge/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e tutorials",
    "content": "Getting Started with DocBuilder This tutorial walks you through producing a multi‑repository Hugo documentation site in minutes.\nPrerequisites Go toolchain (\u003e=1.21) installed. Hugo installed (optional unless you want automatic static rendering). Git access tokens / SSH keys for the repositories you want to aggregate. 1. Install / Build 1 2 3 go build -o ./bin/docbuilder ./cmd/docbuilder # Or run directly without building: go run ./cmd/docbuilder \u003ccommand\u003e 2. Initialize Configuration 1 ./bin/docbuilder init -c config.yaml This creates a starter config.yaml you can customize.\n3. Add Repositories Edit config.yaml and list repositories:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 repositories: - url: https://git.example.com/org/service-a.git name: service-a branch: main paths: [docs] auth: type: token token: \"${GIT_ACCESS_TOKEN}\" - url: https://git.example.com/org/monorepo.git name: monorepo branch: main paths: [docs, documentation/guides] auth: type: token token: \"${GIT_ACCESS_TOKEN}\" hugo: title: \"My Documentation Site\" description: \"Aggregated documentation from multiple repositories\" base_url: \"https://docs.example.com\" theme: \"hextra\" params: search: enable: true type: flexsearch output: directory: ./site clean: true 4. Run a Build 1 ./bin/docbuilder build -c config.yaml -v On success you’ll have:\nsite/hugo.yaml (generated Hugo config) site/content/\u003crepo\u003e/... (or content/\u003cforge\u003e/\u003crepo\u003e/... if multi‑forge) Optional site/public/ if Hugo rendering enabled site/build-report.json with metrics (doc_files_hash fingerprint) 5. Serve (Optional) If you enabled Hugo rendering (for example with --render-mode always), serve the generated site directly:\n1 hugo server -s site Or run Hugo manually afterwards:\n1 (cd site \u0026\u0026 hugo) 6. Incremental Workflow Enable workspace persistence to reuse existing clones:\n1 2 build: workspace_dir: ./site/_workspace # Persist clones between builds On subsequent runs, DocBuilder will:\nReuse existing clones when possible Only fetch updates (git pull) instead of full clones Significantly speed up builds 7. Continuous Updates (Daemon Mode) For live documentation updates, use daemon mode:\n1 ./bin/docbuilder daemon -c config.yaml -v This will:\nPoll repositories for changes Automatically rebuild when changes detected Serve documentation with live reload Or for local development without git polling:\n1 ./bin/docbuilder preview --docs-dir ./docs 8. Next Steps Customize landing pages with templates/index/*.tmpl. Pick a supported theme (hextra, docsy, or relearn). Integrate with CI: compare doc_files_hash between runs to skip downstream jobs. Additional Commands Linting Documentation Validate documentation follows best practices:\n1 2 3 4 5 6 7 ./bin/docbuilder lint docs/ # Automatically fix issues ./bin/docbuilder lint docs/ --fix # Preview fixes without applying ./bin/docbuilder lint docs/ --fix --dry-run Generate from Local Directory Generate a Hugo site from local documentation (no git required):\n1 ./bin/docbuilder generate --docs-dir ./docs --output ./site You are ready to explore How‑To guides for specific tasks.",
    "description": "Getting Started with DocBuilder This tutorial walks you through producing a multi‑repository Hugo documentation site in minutes.\nPrerequisites Go toolchain (\u003e=1.21) installed. Hugo installed (optional unless you want automatic static rendering). Git access tokens / SSH keys for the repositories you want to aggregate. 1. Install / Build 1 2 3 go build -o ./bin/docbuilder ./cmd/docbuilder # Or run directly without building: go run ./cmd/docbuilder \u003ccommand\u003e 2. Initialize Configuration 1 ./bin/docbuilder init -c config.yaml This creates a starter config.yaml you can customize.",
    "tags": [
      "Getting-Started",
      "Quickstart",
      "Introduction"
    ],
    "title": "Getting Started Tutorial",
    "uri": "/docs/tutorials/getting-started/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Getting-Started",
    "uri": "/tags/getting-started/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Gitea",
    "uri": "/tags/gitea/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Go",
    "uri": "/tags/go/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation",
    "content": "DocBuilder Go Style Guide This document defines naming conventions and coding style for the DocBuilder project to ensure consistency across the codebase.\nTable of Contents General Principles Variable Naming Function Naming Type Naming Package Naming Error Handling Comments and Documentation General Principles Clarity over cleverness: Code should be self-documenting Consistency: Follow existing patterns in the codebase Brevity with context: Use short names in small scopes, descriptive names in larger scopes Go idioms: Follow standard Go conventions from golang.org/wiki/CodeReviewComments Variable Naming Abbreviations Use consistent abbreviations throughout the codebase:\nFull Term Abbreviation Usage application app Function parameters, local variables argument arg Function parameters, local variables authentication auth Function parameters, local variables configuration cfg Struct fields, function parameters context ctx Standard Go convention database db Struct fields, function parameters destination dst Function parameters, local variables directory dir Function parameters, local variables document doc Function parameters, local variables environment env Struct fields, function parameters error err Standard Go convention identifier id Function parameters, local variables information info Function parameters, local variables maximum max Local variables message msg Function parameters, local variables minimum min Local variables parameter param Function parameters, local variables recorder rec Function parameters, local variables reference ref Local variables only repository repo Function parameters, local variables request req Function parameters, local variables response resp Function parameters, local variables source src Function parameters, local variables specification spec Function parameters, local variables statistics stats Function parameters, local variables temporary tmp Local variables only Examples:\n1 2 3 4 5 6 7 8 // ✅ Good func (c *Client) CloneRepo(repo appcfg.Repository) (string, error) func (c *Client) getAuth(authCfg *appcfg.AuthConfig) (transport.AuthMethod, error) buildCfg := \u0026appcfg.BuildConfig{} // ❌ Bad - inconsistent abbreviation func (c *Client) CloneRepository(repo appcfg.Repository) (string, error) func (c *Client) getAuthentication(authConfig *appcfg.AuthConfig) Exception: When using external library types, use the library’s naming:\n1 2 3 // ✅ Correct - go-git uses \"repository\" in type name repository, err := git.PlainOpen(repoPath) var gitRepo *git.Repository Scope-Based Naming Single-letter variables: Only in very short scopes (\u003c 10 lines)\n1 2 3 for i, v := range items { // i and v are acceptable here } Abbreviated names: For function parameters and local variables\n1 func processRepo(repo appcfg.Repository, cfg *BuildConfig) error Descriptive names: For package-level variables and struct fields\n1 2 3 4 5 type Client struct { workspaceDir string buildCfg *appcfg.BuildConfig remoteHeadCache *RemoteHeadCache } Receiver Names Use 1-2 letter abbreviations based on the type name Be consistent throughout a type’s methods 1 2 3 4 5 6 7 8 9 10 // ✅ Good func (c *Client) CloneRepo(...) func (c *Client) UpdateRepo(...) // ✅ Good for multi-word types func (rhc *RemoteHeadCache) Get(...) // ❌ Bad - inconsistent func (client *Client) CloneRepo(...) func (c *Client) UpdateRepo(...) Configuration Variables Always suffix with Cfg or Config depending on context:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // ✅ Good - struct fields use abbreviated suffix type Client struct { buildCfg *appcfg.BuildConfig } // ✅ Good - function parameters use abbreviated suffix func NewGenerator(cfg *config.Config, outputDir string) *Generator // ✅ Good - package-level uses full name for clarity var DefaultConfig = \u0026config.Config{...} // ❌ Bad - no suffix type Client struct { build *appcfg.BuildConfig } Boolean Variables Prefix with is, has, should, can, or enable:\n1 2 3 4 5 6 7 8 9 10 11 // ✅ Good isValid := true hasAuth := repo.Auth != nil shouldRetry := attempt \u003c maxRetries canFastForward := true enableCache := cfg.EnableCache // ❌ Bad valid := true auth := repo.Auth != nil retry := attempt \u003c maxRetries Function Naming Private vs Public Functions 1 2 3 4 5 6 7 8 // ✅ Public - descriptive, full words func (c *Client) CloneRepo(repo appcfg.Repository) error func ComputeRepoHash(repoPath string, commit string) (string, error) // ✅ Private - can use abbreviations func (c *Client) getAuth(authCfg *appcfg.AuthConfig) error func classifyError(err error) error func (c *Client) fetchOrigin(repo *git.Repository) error Getter/Setter Patterns Go doesn’t use Get/Set prefixes for simple accessors:\n1 2 3 4 5 6 7 // ✅ Good func (c *Client) WorkspaceDir() string { return c.workspaceDir } func (c *Client) SetWorkspaceDir(dir string) { c.workspaceDir = dir } // ❌ Bad func (c *Client) GetWorkspaceDir() string func (c *Client) SetWorkspaceDir(dir string) Exception: Use Get when fetching requires computation or I/O:\n1 2 3 4 5 // ✅ Correct - involves network I/O func (c *Client) GetRemoteHead(repo appcfg.Repository) (string, error) // ✅ Correct - involves cache lookup func (c *RemoteHeadCache) Get(url, branch string) *RemoteHeadEntry Action Verbs Use clear action verbs that describe what the function does:\nAction Usage Example Clone Create a new copy CloneRepo Update Modify existing UpdateRepo Fetch Retrieve from remote FetchOrigin Create Construct new instance CreateAuth Build Construct complex object BuildConfig Generate Produce output GenerateHugoSite Process Transform data ProcessDocs Compute Calculate value ComputeRepoHash Classify Categorize ClassifyError Resolve Determine value ResolveTargetBranch Ensure Guarantee state EnsureWorkspace Check Validate or test CheckRemoteChanged Validate Verify correctness ValidateConfig Predicate Functions Functions returning bool should read like questions:\n1 2 3 4 5 6 7 8 // ✅ Good func (c *Client) IsAncestor(a, b plumbing.Hash) (bool, error) func HasAuth(repo appcfg.Repository) bool func ShouldRetry(err error) bool // ❌ Bad func (c *Client) Ancestor(a, b plumbing.Hash) (bool, error) func Auth(repo appcfg.Repository) bool Error Classification Functions Use consistent patterns for error classification:\n1 2 3 4 5 6 7 8 9 10 11 12 // ✅ Good - consistent \"classify\" pattern func classifyFetchError(url string, err error) error func classifyCloneError(url string, err error) error // ✅ Good - consistent \"is\" pattern for boolean checks func isPermanentGitError(err error) bool func isTransientError(err error) bool // ❌ Bad - mixing patterns func classifyFetchError(url string, err error) error func isPermanentGitError(err error) bool func classifyTransientType(err error) string // Different return type Constructor Functions 1 2 3 4 5 6 7 // ✅ Standard pattern func NewClient(workspaceDir string) *Client func NewRemoteHeadCache(cacheDir string) (*RemoteHeadCache, error) // ✅ Builder pattern - use \"With\" prefix func (c *Client) WithBuildConfig(cfg *appcfg.BuildConfig) *Client func (c *Client) WithRemoteHeadCache(cache *RemoteHeadCache) *Client Verb Ordering in Compound Function Names When function names contain multiple actions, order verbs to reflect execution flow and emphasize the primary operation:\n1. Validation/Check Before Action\nPlace validation verbs first when they guard the main operation:\n1 2 3 4 5 6 7 8 // ✅ Good - validation happens first func ValidateAndCreate(cfg *Config) error func CheckAndUpdate(repo Repository) error func EnsureAndClone(dir string) error // ❌ Bad - suggests action happens before validation func CreateAndValidate(cfg *Config) error func UpdateAndCheck(repo Repository) error 2. Setup Before Execution\nPreparation verbs come before execution verbs:\n1 2 3 4 5 6 7 8 // ✅ Good - setup before main operation func PrepareAndExecute(cmd Command) error func InitializeAndRun(service Service) error func LoadAndProcess(file string) error // ❌ Bad - execution before preparation func ExecuteAndPrepare(cmd Command) error func RunAndInitialize(service Service) error 3. Primary Action First\nWhen combining a main operation with a side effect, lead with the primary action:\n1 2 3 4 5 6 7 8 // ✅ Good - primary action leads func CreateWithNotification(resource Resource) error func UpdateWithLogging(entity Entity) error func DeleteWithCleanup(path string) error // ✅ Also acceptable - \"And\" pattern for equal importance func CreateAndNotify(resource Resource) error func UpdateAndLog(entity Entity) error 4. CRUD Operation Ordering\nWhen implementing multiple CRUD operations, follow this conventional order:\n1 2 3 4 5 6 7 8 9 10 11 // ✅ Good - conventional CRUD order func Create(...) func Get(...) or Read(...) func Update(...) func Delete(...) // For bulk operations func CreateBatch(...) func GetAll(...) func UpdateBatch(...) func DeleteBatch(...) 5. Method Chaining Order\nBuilder pattern methods should follow logical construction sequence:\n1 2 3 4 5 6 7 8 // ✅ Good - logical build sequence client := NewClient(). WithAuth(auth). WithConfig(cfg). WithRetry(3). Build() // Methods ordered: identity → configuration → options → finalization 6. Cleanup and Finalization\nCleanup operations should be explicit in compound names:\n1 2 3 4 5 6 7 // ✅ Good - clear cleanup semantics func CloseAndCleanup() error func StopAndRemove() error func CompleteAndArchive() error // ❌ Bad - ambiguous cleanup timing func CleanupAndClose() error // Does cleanup happen before closing? Type Naming Struct Types Use clear, descriptive names without abbreviations:\n1 2 3 4 5 6 7 8 9 // ✅ Good type RemoteHeadCache struct { ... } type RemoteHeadEntry struct { ... } type BuildConfig struct { ... } // ❌ Bad type RHCache struct { ... } type RHEntry struct { ... } type BldCfg struct { ... } Interface Types Prefer single-method interfaces with -er suffix:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // ✅ Good type Cloner interface { Clone(repo Repository) error } type Generator interface { Generate() error } // ✅ Good - multi-method interface type GitClient interface { CloneRepo(repo Repository) error UpdateRepo(repo Repository) error } Error Types Suffix all error types with Error:\n1 2 3 4 5 6 7 8 // ✅ Good type AuthError struct { ... } type NotFoundError struct { ... } type NetworkTimeoutError struct { ... } // ❌ Bad type AuthFailure struct { ... } type NotFound struct { ... } Struct Field Names 1 2 3 4 5 6 7 8 9 10 11 type Client struct { // ✅ Private fields - use abbreviated suffixes workspaceDir string buildCfg *appcfg.BuildConfig remoteHeadCache *RemoteHeadCache // ✅ Exported fields - full words for clarity Name string Description string Repository appcfg.Repository } Package Naming Package Names Use short, lowercase, single-word names No underscores or camelCase Name should describe package purpose 1 2 3 4 5 6 7 8 9 10 // ✅ Good package git package config package hugo package docs // ❌ Bad package gitOperations package config_manager package hugoSiteGenerator Package Import Aliases Use consistent aliases when avoiding conflicts:\n1 2 3 4 5 6 7 8 9 10 11 // ✅ Good - descriptive prefix import ( appcfg \"git.home.luguber.info/inful/docbuilder/internal/config\" ggitcfg \"github.com/go-git/go-git/v5/config\" ) // ❌ Bad - unclear abbreviation import ( cfg1 \"git.home.luguber.info/inful/docbuilder/internal/config\" cfg2 \"github.com/go-git/go-git/v5/config\" ) Error Handling Unified Error System DocBuilder uses internal/foundation/errors package for all error handling. This provides:\nType-safe error categories (ErrorCategory) Structured error context Retry semantics HTTP and CLI adapters Error Variables Prefix package-level sentinel errors with Err:\n1 2 3 4 5 6 7 8 9 10 11 12 // ✅ Good var ( ErrNotFound = errors.New(errors.CategoryNotFound, \"repository not found\").Build() ErrUnauthorized = errors.New(errors.CategoryAuth, \"authentication failed\").Build() ErrInvalidConfig = errors.ValidationError(\"invalid configuration\").Build() ) // ❌ Bad var ( NotFoundError = errors.New(\"repository not found\") // No category Unauthorized = errors.New(\"authentication failed\") // No category ) Error Construction Use the fluent builder API:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // ✅ Good - validation error with context return errors.ValidationError(\"invalid forge type\"). WithContext(\"input\", forgeType). WithContext(\"valid_values\", []string{\"github\", \"gitlab\"}). Build() // ✅ Good - wrapping errors return errors.WrapError(err, errors.CategoryGit, \"failed to clone repository\"). WithContext(\"url\", repo.URL). WithContext(\"branch\", repo.Branch). Build() // ❌ Bad - raw errors return fmt.Errorf(\"failed to clone: %w\", err) // No category or context Error Messages Start with lowercase Be specific and actionable Add context using WithContext(key, value) instead of string formatting Error Categories Available categories in internal/foundation/errors/categories.go:\nUser-facing: CategoryConfig, CategoryValidation, CategoryAuth, CategoryNotFound, CategoryAlreadyExists External: CategoryNetwork, CategoryGit, CategoryForge Build: CategoryBuild, CategoryHugo, CategoryFileSystem Runtime: CategoryRuntime, CategoryDaemon, CategoryInternal Error Detection 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // ✅ Good - extract classified error if classified, ok := errors.AsClassified(err); ok { if classified.Category() == errors.CategoryValidation { // Handle validation error } } // ✅ Good - check category using helper if errors.HasCategory(err, errors.CategoryAuth) { // Handle auth error } // ❌ Bad - old pattern var classified *foundation.ClassifiedError if foundation.AsClassified(err, \u0026classified) { // Old API - no longer exists } Typed Errors For package-specific errors, use the builder:\n1 2 3 4 5 6 7 8 9 10 // ✅ Good - use builder for custom errors func NewAuthError(op, url string, cause error) error { return errors.WrapError(cause, errors.CategoryAuth, \"authentication failed\"). WithContext(\"operation\", op). WithContext(\"url\", url). Build() } // Usage return NewAuthError(\"clone\", repo.URL, err) Comments and Documentation Package Documentation Every package should have a package comment:\n1 2 3 4 // Package git provides a client for performing Git operations such as // clone, update, and authentication handling for DocBuilder's // documentation pipeline. package git Function Documentation Document all exported functions:\n1 2 3 // CloneRepo clones a repository to the workspace directory. // If retry is enabled, it wraps the operation with retry logic. func (c *Client) CloneRepo(repo appcfg.Repository) (string, error) Comment Style Use complete sentences with proper punctuation Start with the name of the thing being documented Explain why, not just what for complex logic 1 2 3 4 5 6 7 8 // ✅ Good // ComputeRepoHash computes a deterministic hash for a repository tree. // The hash is based on the commit SHA and the tree structure of configured paths, // enabling content-addressable caching where same commit + same paths = same hash. // ❌ Bad // computes hash // This function hashes repos TODO Comments Include context and optionally an issue number:\n1 2 // TODO(username): Add authentication support for remote.List // TODO: Implement rate limiting (see issue #123) Code Organization File Naming Use lowercase with underscores for multi-word file names Match primary type or functionality 1 2 3 4 5 6 7 8 9 // ✅ Good remote_cache.go // Contains RemoteHeadCache typed_errors.go // Contains error types client.go // Contains Client type // ❌ Bad RemoteCache.go // Wrong case remote-cache.go // Use underscore, not hyphen remotecache.go // Hard to read Function Grouping Group related functions together in files:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // client.go - Core client operations func NewClient() func (c *Client) CloneRepo() func (c *Client) UpdateRepo() // retry.go - Retry logic func (c *Client) withRetry() func isPermanentGitError() func classifyTransientType() // remote_cache.go - Remote caching type RemoteHeadCache func NewRemoteHeadCache() func (c *RemoteHeadCache) Get() func (c *RemoteHeadCache) Set() References This style guide is based on:\nEffective Go Go Code Review Comments Uber Go Style Guide DocBuilder project conventions Enforcement Run golangci-lint before committing Review PRs for style consistency Update this guide as patterns emerge",
    "description": "DocBuilder Go Style Guide This document defines naming conventions and coding style for the DocBuilder project to ensure consistency across the codebase.\nTable of Contents General Principles Variable Naming Function Naming Type Naming Package Naming Error Handling Comments and Documentation General Principles Clarity over cleverness: Code should be self-documenting Consistency: Follow existing patterns in the codebase Brevity with context: Use short names in small scopes, descriptive names in larger scopes Go idioms: Follow standard Go conventions from golang.org/wiki/CodeReviewComments Variable Naming Abbreviations Use consistent abbreviations throughout the codebase:",
    "tags": [
      "Style-Guide",
      "Coding-Standards",
      "Go"
    ],
    "title": "Go Style Guide",
    "uri": "/docs/style_guide/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Hextra",
    "uri": "/tags/hextra/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Add Content Transforms This guide shows you how to add custom transformations to markdown files before Hugo renders them.\nOverview DocBuilder uses a pluggable transform pipeline that processes markdown files during the content copy stage. Each file goes through a series of transformers that can:\nModify markdown content Add or modify front matter metadata Rewrite links Convert syntax between formats Add custom fields Transform Pipeline Architecture Transformers run in dependency-based order organized by stages during the CopyContent stage:\nStage: parse 1. Front Matter Parser - Extract YAML headers Stage: build 2. Front Matter Builder - Add repository metadata Stage: enrich 3. Edit Link Injector - Generate edit URLs Stage: merge 4. Front Matter Merge - Combine metadata Stage: transform 5. Relative Link Rewriter - Fix relative links 6. [Your Custom Transform] - Your transformation Stage: finalize 7. Strip First Heading - Remove duplicate titles 8. Shortcode Escaper - Escape Hugo shortcodes 9. Hextra Type Enforcer - Set page types Stage: serialize 10. Serializer - Write final YAML + content Within each stage, transforms are ordered by their declared dependencies.\nCreating a Custom Transformer Step 1: Create the Transformer File Create a new file in internal/hugo/transforms/:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // internal/hugo/transforms/my_custom.go package transforms import ( \"fmt\" \"strings\" ) type MyCustomTransform struct{} func (t MyCustomTransform) Name() string { return \"my_custom_transform\" } func (t MyCustomTransform) Stage() TransformStage { return StageTransform // Runs during the transform stage } func (t MyCustomTransform) Dependencies() TransformDependencies { return TransformDependencies{ MustRunAfter: []string{\"relative_link_rewriter\"}, // Run after link rewriting } } func (t MyCustomTransform) Transform(p PageAdapter) error { // Type assert to access page data pg, ok := p.(*PageShim) if !ok { return fmt.Errorf(\"unexpected page adapter type\") } // Transform the markdown content pg.Content = strings.ReplaceAll(pg.Content, \"{{OLD}}\", \"{{NEW}}\") return nil } func init() { // Auto-register on package load Register(MyCustomTransform{}) } Step 2: Build the Project 1 go build ./... The transformer is automatically registered and will run on all markdown files.\nStep 3: Test Your Transform 1 docbuilder build -c config.yaml -v Check the output files in content/ to verify your transformation.\nCommon Use Cases Example 1: Content Replacement Replace specific text patterns across all files:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type ContentReplacer struct{} func (t ContentReplacer) Name() string { return \"content_replacer\" } func (t ContentReplacer) Stage() TransformStage { return StageTransform } func (t ContentReplacer) Dependencies() TransformDependencies { return TransformDependencies{ MustRunAfter: []string{\"relative_link_rewriter\"}, } } func (t ContentReplacer) Transform(p PageAdapter) error { pg := p.(*PageShim) // Replace deprecated syntax pg.Content = strings.ReplaceAll(pg.Content, \":warning:\", \"⚠️\") pg.Content = strings.ReplaceAll(pg.Content, \":information_source:\", \"ℹ️\") return nil } func init() { Register(ContentReplacer{}) } Example 2: Add Custom Metadata Inject additional front matter fields:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 type CustomMetadata struct{} func (t CustomMetadata) Name() string { return \"custom_metadata\" } func (t CustomMetadata) Stage() TransformStage { return StageBuild } func (t CustomMetadata) Dependencies() TransformDependencies { return TransformDependencies{ MustRunAfter: []string{\"front_matter_builder_v2\"}, } } func (t CustomMetadata) Transform(p PageAdapter) error { pg := p.(*PageShim) // Add custom fields via patches pg.AddPatch(fmcore.FrontMatterPatch{ Key: \"last_modified\", Value: time.Now().Format(time.RFC3339), }) pg.AddPatch(fmcore.FrontMatterPatch{ Key: \"version\", Value: \"1.0\", }) return nil } func init() { Register(CustomMetadata{}) } Example 3: Convert GitHub Alerts to Hugo Shortcodes Transform GitHub-style alert syntax:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 type AdmonitionConverter struct{} func (t AdmonitionConverter) Name() string { return \"admonition_converter\" } func (t AdmonitionConverter) Stage() TransformStage { return StageTransform } func (t AdmonitionConverter) Dependencies() TransformDependencies { return TransformDependencies{ MustRunAfter: []string{\"relative_link_rewriter\"}, } } func (t AdmonitionConverter) Transform(p PageAdapter) error { pg := p.(*PageShim) // Convert: \u003e [!NOTE] → {{\u003c callout type=\"note\" \u003e}} re := regexp.MustCompile(`(?m)^\u003e \\[!(NOTE|TIP|WARNING|IMPORTANT|CAUTION)\\]\\s*\\n((?:\u003e .+\\n?)+)`) pg.Content = re.ReplaceAllStringFunc(pg.Content, func(match string) string { parts := re.FindStringSubmatch(match) alertType := strings.ToLower(parts[1]) content := strings.ReplaceAll(parts[2], \"\u003e \", \"\") return fmt.Sprintf(`{{\u003c callout type=%q \u003e}}%s{{\u003c /callout \u003e}}`, alertType, content) }) return nil } func init() { Register(AdmonitionConverter{}) } Example 4: Process Code Blocks Add metadata or transform code block syntax:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type CodeBlockEnhancer struct{} func (t CodeBlockEnhancer) Name() string { return \"code_block_enhancer\" } func (t CodeBlockEnhancer) Stage() TransformStage { return StageTransform } func (t CodeBlockEnhancer) Dependencies() TransformDependencies { return TransformDependencies{ MustRunAfter: []string{\"relative_link_rewriter\"}, } } func (t CodeBlockEnhancer) Transform(p PageAdapter) error { pg := p.(*PageShim) // Add line numbers to all code blocks re := regexp.MustCompile(\"(?s)```(\\\\w+)\\\\n(.*?)```\") pg.Content = re.ReplaceAllString(pg.Content, \"```$1 {linenos=true}\\n$2```\") return nil } func init() { Register(CodeBlockEnhancer{}) } Example 5: Conditional Transforms Apply transformations based on file properties:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 type ConditionalTransform struct{} func (t ConditionalTransform) Name() string { return \"conditional_transform\" } func (t ConditionalTransform) Stage() TransformStage { return StageTransform } func (t ConditionalTransform) Dependencies() TransformDependencies { return TransformDependencies{ MustRunAfter: []string{\"relative_link_rewriter\"}, } } func (t ConditionalTransform) Transform(p PageAdapter) error { pg := p.(*PageShim) // Only transform API documentation if strings.Contains(pg.FilePath, \"/api/\") { pg.AddPatch(fmcore.FrontMatterPatch{ Key: \"type\", Value: \"api-reference\", }) // Add API-specific styling pg.Content = \"{{\u003c api-layout \u003e}}\\n\" + pg.Content + \"\\n{{\u003c /api-layout \u003e}}\" } return nil } func init() { Register(ConditionalTransform{}) } PageShim Interface Your transform receives a PageAdapter which you type-assert to *PageShim:\n1 2 3 4 5 6 7 8 9 10 11 12 13 type PageShim struct { FilePath string // Relative file path Content string // Markdown content (no front matter) OriginalFrontMatter map[string]any // Parsed YAML front matter HadFrontMatter bool // Whether file had front matter Doc docs.DocFile // Full document metadata // Methods AddPatch(patch FrontMatterPatch) // Add front matter field ApplyPatches() // Merge patches RewriteLinks(content string) string // Fix relative links Serialize() error // Write final output } Important: Don’t call Serialize() in your transform - it’s automatically called by the pipeline.\nSetting Transform Stage and Dependencies Transforms are organized by stages and dependencies (not priorities):\nAvailable Stages Stage Purpose Example Transforms StageParse Extract/parse source content Front matter parsing StageBuild Generate base metadata Repository info, titles StageEnrich Add computed fields Edit links, custom metadata StageMerge Combine/merge data Merge user + generated data StageTransform Modify content Link rewriting, syntax conversion StageFinalize Post-process Strip headings, escape shortcodes StageSerialize Output generation Write final YAML + content Declaring Dependencies Use Dependencies() to specify ordering within a stage:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (t MyTransform) Dependencies() TransformDependencies { return TransformDependencies{ // This transform must run after these transforms MustRunAfter: []string{\"front_matter_merge\", \"relative_link_rewriter\"}, // This transform must run before these transforms MustRunBefore: []string{\"front_matter_serialize\"}, // Capability flags (for documentation) RequiresOriginalFrontMatter: false, ModifiesContent: true, ModifiesFrontMatter: false, } } Guidelines:\nStageParse: Early processing (parsing, reading) StageBuild-StageEnrich: Metadata manipulation StageTransform: Content modification StageFinalize: Cleanup and validation StageSerialize: Output serialization Within each stage, transforms are ordered by their dependency declarations using topological sort.\nControlling Transforms via Configuration You can enable/disable transforms in config.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 hugo: transforms: enable: - front_matter_parser - front_matter_builder_v2 - my_custom_transform # Your custom transform - edit_link_injector_v2 - merge_front_matter - relative_link_rewriter - serializer disable: - some_transform_to_skip Enable mode: Only listed transforms run (allowlist)\nDisable mode: All except listed transforms run (denylist)\nIf neither is specified, all registered transforms run.\nBest Practices 1. Name Transforms Descriptively The name is used for logging and enable/disable filtering:\n1 2 3 func (t MyTransform) Name() string { return \"descriptive_snake_case_name\" } 2. Handle Errors Gracefully Return errors with context:\n1 2 3 if err != nil { return fmt.Errorf(\"failed to process %s: %w\", pg.FilePath, err) } 3. Log Important Operations Use structured logging for debugging:\n1 2 3 import \"log/slog\" slog.Debug(\"Processing file\", \"path\", pg.FilePath, \"transform\", t.Name()) 4. Don’t Modify Raw Content Only modify pg.Content (without front matter). The serializer handles combining it with front matter.\n5. Test Thoroughly Create test files in internal/hugo/transforms/:\n1 2 3 4 5 6 7 8 9 10 11 func TestMyCustomTransform(t *testing.T) { pg := \u0026PageShim{ Content: \"original content\", } transform := MyCustomTransform{} err := transform.Transform(pg) assert.NoError(t, err) assert.Equal(t, \"expected content\", pg.Content) } 6. Consider Performance Transforms run on every markdown file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Good: Compile regex once var alertRegex = regexp.MustCompile(`pattern`) func (t Transform) Transform(p PageAdapter) error { pg := p.(*PageShim) pg.Content = alertRegex.ReplaceAllString(pg.Content, \"replacement\") return nil } // Bad: Compile regex every time func (t Transform) Transform(p PageAdapter) error { pg := p.(*PageShim) re := regexp.MustCompile(`pattern`) // ❌ Slow! pg.Content = re.ReplaceAllString(pg.Content, \"replacement\") return nil } Debugging Transforms Enable Verbose Logging 1 docbuilder build -c config.yaml -v Check Transform Order The registry logs the execution order at startup with verbose logging enabled.\nTest Individual Transforms Create a unit test with sample content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func TestTransformOutput(t *testing.T) { input := `--- title: Test --- # Content \u003e [!NOTE] \u003e This is a note ` pg := \u0026PageShim{Content: input} transform := AdmonitionConverter{} err := transform.Transform(pg) require.NoError(t, err) assert.Contains(t, pg.Content, \"{{ callout }}\") } Inspect Generated Files Check the actual output in the Hugo site:\n1 2 docbuilder build -c config.yaml cat site/content/repo/path/file.md Advanced: External Transforms For transforms without modifying DocBuilder’s code, consider:\nOption 1: Pre-Processing Script Process files before DocBuilder runs:\n1 2 3 4 #!/bin/bash # pre-process.sh find ./docs -name \"*.md\" -exec sed -i 's/old/new/g' {} \\; docbuilder build -c config.yaml Option 2: Post-Processing Hook Modify Hugo content after DocBuilder generates it:\n1 2 3 4 #!/bin/bash docbuilder build -c config.yaml # Modify files in site/content/ find ./site/content -name \"*.md\" -exec ./my-transform.py {} \\; Option 3: Hugo Modules/Mounts Use Hugo’s own content transformation features in hugo.yaml:\n1 2 3 4 5 6 7 module: mounts: - source: content target: content transforms: - filter: \\.md$ command: my-transform-script Troubleshooting Transform Not Running Check registration:\n1 2 3 func init() { Register(MyTransform{}) // Must be in init() } Verify stage and dependencies:\n1 2 3 4 5 6 7 func (t MyTransform) Stage() TransformStage { return StageTransform // Must return valid stage } func (t MyTransform) Dependencies() TransformDependencies { return TransformDependencies{} // Define dependencies } Content Not Changed Check you’re modifying the right field:\n1 2 pg.Content = newContent // ✅ Correct pg.Raw = newContent // ❌ Wrong - overwritten by serializer Ensure serializer runs after: Your priority must be \u003c 90 (serializer priority).\nFront Matter Issues Use patches, not direct modification:\n1 2 3 4 5 // ✅ Correct pg.AddPatch(fmcore.FrontMatterPatch{Key: \"field\", Value: \"value\"}) // ❌ Wrong - overwritten by merge pg.OriginalFrontMatter[\"field\"] = \"value\" See Also Architecture: Content Transform Pipeline Package Architecture: Hugo Transforms Configuration Reference: Hugo Transforms",
    "description": "Add Content Transforms This guide shows you how to add custom transformations to markdown files before Hugo renders them.\nOverview DocBuilder uses a pluggable transform pipeline that processes markdown files during the content copy stage. Each file goes through a series of transformers that can:",
    "tags": [
      "Content-Transforms",
      "Development",
      "Customization"
    ],
    "title": "How To: Add Content Transforms",
    "uri": "/docs/how-to/add-content-transforms/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Forge namespacing helps avoid repository name collisions when aggregating multiple hosting platforms (GitHub, GitLab, Forgejo, etc.).\nModes Configured via build.namespace_forges:\nauto (default): Add \u003cforge\u003e/ prefix only if more than one distinct forge type is present. always: Always prefix with the forge type when known. never: Never add the prefix (legacy layout). Example Layouts Multiple forges (auto or always):\ncontent/ github/ service-a/... gitlab/ service-b/... Single forge (auto or never):\ncontent/ service-a/... Front Matter Each generated page includes forge in its front matter when the value is known. This lets themes and custom templates branch per forge.\nSelecting a Mode 1 2 build: namespace_forges: auto # or always | never When To Use always You expect to add a second forge later and want stable URLs now. You prefer explicit clarity in paths regardless of ambiguity. When To Use never Migrating from an older installation that hard-coded non-namespaced paths in links. Verifying Run a build with -v and observe resulting content/ tree or inspect a page front matter for forge:.\nTroubleshooting Missing prefix when expected: ensure repositories actually declare forge metadata (tags / detection); confirm more than one forge type is present if using auto. Unexpected prefix: you probably have at least two repo forges; switch to never if undesired.",
    "description": "Forge namespacing helps avoid repository name collisions when aggregating multiple hosting platforms (GitHub, GitLab, Forgejo, etc.).\nModes Configured via build.namespace_forges:\nauto (default): Add \u003cforge\u003e/ prefix only if more than one distinct forge type is present. always: Always prefix with the forge type when known. never: Never add the prefix (legacy layout). Example Layouts Multiple forges (auto or always):",
    "tags": [
      "Forge",
      "Namespacing",
      "Configuration"
    ],
    "title": "How To: Configure Forge Namespacing",
    "uri": "/docs/how-to/configure-forge-namespacing/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "DocBuilder generates three index page kinds using template resolution with safe defaults. You can override any of them.\nKinds Kind Location Purpose main content/_index.md Global landing page repository content/\u003crepo\u003e/_index.md Per repository overview section content/\u003crepo\u003e/\u003csection\u003e/_index.md Section landing Override Search Order For \u003ckind\u003e = main | repository | section (first match wins):\ntemplates/index/\u003ckind\u003e.md.tmpl templates/index/\u003ckind\u003e.tmpl templates/\u003ckind\u003e_index.tmpl Example Override Create templates/index/main.md.tmpl before building:\n1 2 3 4 5 {{ .Site.Description }} {{ range $name, $files := .Repositories }} ## {{ $name }} ({{ len $files }} files) {{ end }} Front Matter Behavior If your template DOES NOT start with --- DocBuilder injects generated front matter:\ntitle, description repository, section (where relevant) forge (when available/namespaced) date (generation timestamp) editURL (theme \u0026 metadata dependent) If your template starts with --- you assume full control; DocBuilder will not add another front matter block.\nHelper Functions Function Description titleCase Capitalize words (simple ASCII) replaceAll Wrapper around strings.ReplaceAll Context Keys Key Scope Description .Site all { Title, Description, BaseURL, Theme } .FrontMatter all Map of computed values prior to serialization .Repositories main Go map map[string][]DocFile (repository name → files) .Files all Slice of relevant DocFile entries .Sections repository Go map map[string][]DocFile (key root for unsectioned files) .SectionName section Name of current section .Stats main/repository { TotalFiles, TotalRepositories } .Now all Build timestamp DocFile Fields Name, Repository, Forge, Section, Path (Hugo relative path base), plus internal metadata like RelativePath.\nTroubleshooting Symptom Cause Fix Default template still used File path mismatch Confirm placement matches search order. Duplicate front matter Custom template began with --- plus injection Remove manual fence or keep and remove conflicting keys. Missing edit links Repo metadata insufficient Verify repo URL + branch in config.",
    "description": "DocBuilder generates three index page kinds using template resolution with safe defaults. You can override any of them.\nKinds Kind Location Purpose main content/_index.md Global landing page repository content/\u003crepo\u003e/_index.md Per repository overview section content/\u003crepo\u003e/\u003csection\u003e/_index.md Section landing Override Search Order For \u003ckind\u003e = main | repository | section (first match wins):",
    "tags": [
      "Customization",
      "Index-Pages"
    ],
    "title": "How To: Customize Index Pages",
    "uri": "/docs/how-to/customize-index-pages/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "By default DocBuilder scaffolds a Hugo site (content + config) without running the hugo binary. Enable automatic rendering to prebuild public/.\nRender Mode Precedence (highest first):\nbuild.render_mode in config (never, auto, always). --render-mode CLI flag, which overrides config for a single run. Run With Rendering 1 ./bin/docbuilder build -c config.yaml --render-mode always Result: public/ under the output directory plus a build-report.json with static_rendered: true.\nVerify Open public/index.html in a browser or run a local server:\n1 (cd site \u0026\u0026 hugo server) When Builds Fail If Hugo execution fails, DocBuilder logs a warning and leaves the scaffold intact. You can inspect and run hugo manually.\nCI Pattern Skip rendering in pull request validation (faster), run rendering only on main branch merges:\n1 2 3 4 5 if test \"$CI_COMMIT_BRANCH\" = \"main\"; then ./bin/docbuilder build -c config.yaml --render-mode always else ./bin/docbuilder build -c config.yaml --render-mode never fi Troubleshooting Symptom Cause Fix No public/ directory Render mode not set to always Run with --render-mode always or set build.render_mode: always. Broken asset links Theme modules not fetched Ensure network access; rerun. Build warning only Hugo error surfaced Read logs; fix Hugo config or content.",
    "description": "By default DocBuilder scaffolds a Hugo site (content + config) without running the hugo binary. Enable automatic rendering to prebuild public/.\nRender Mode Precedence (highest first):\nbuild.render_mode in config (never, auto, always). --render-mode CLI flag, which overrides config for a single run. Run With Rendering 1 ./bin/docbuilder build -c config.yaml --render-mode always Result: public/ under the output directory plus a build-report.json with static_rendered: true.",
    "tags": [
      "Hugo",
      "Rendering",
      "Static-Sites"
    ],
    "title": "How To: Enable Hugo Rendering",
    "uri": "/docs/how-to/enable-hugo-render/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "How to Enable Multi-Version Documentation Build documentation from multiple branches and tags to provide version-specific documentation for your users.\nOverview Multi-version documentation allows you to:\nBuild docs from multiple branches (e.g., main, develop, release-1.x) Build docs from version tags (e.g., v1.0.0, v2.0.0) Provide version switchers in your documentation site Maintain historical documentation alongside current docs Basic Setup Enable Versioning Add to your configuration:\n1 2 3 4 versioning: enabled: true strategy: branches_and_tags max_versions_per_repo: 10 Configure Version Selection Branches and Tags 1 2 3 4 5 6 7 8 9 10 11 versioning: enabled: true strategy: branches_and_tags branch_patterns: - \"main\" - \"develop\" - \"release/*\" tag_patterns: - \"v*\" # v1.0.0, v2.0.0, etc. - \"[0-9]*\" # 1.0.0, 2.0.0, etc. max_versions_per_repo: 5 Branches Only 1 2 3 4 5 6 7 8 versioning: enabled: true strategy: branches_only branch_patterns: - \"main\" - \"develop\" - \"feature/*\" max_versions_per_repo: 3 Tags Only 1 2 3 4 5 6 versioning: enabled: true strategy: tags_only tag_patterns: - \"v[0-9]*\" # Semantic versions only max_versions_per_repo: 10 How It Works Version Discovery Remote Query: DocBuilder queries Git remote for available refs Pattern Matching: Filters branches/tags by configured patterns Sorting: Orders by creation time (newest first) Limiting: Applies max_versions_per_repo limit Expansion: Creates separate repository entry for each version Repository Expansion Given this configuration:\n1 2 3 4 5 6 7 8 9 repositories: - url: https://github.com/org/project.git name: project branch: main versioning: enabled: true strategy: branches_and_tags max_versions_per_repo: 3 DocBuilder expands it to:\nproject-main (branch) project-v2.0.0 (tag) project-v1.0.0 (tag) Content Organization Each version is cloned and processed separately:\ncontent/ project-main/ _index.md getting-started.md project-v2.0.0/ _index.md getting-started.md project-v1.0.0/ _index.md getting-started.md Hugo Configuration DocBuilder generates Hugo config with version metadata:\n1 2 3 4 5 6 7 8 9 params: versions: - name: main url: /project-main/ is_default: true - name: v2.0.0 url: /project-v2.0.0/ - name: v1.0.0 url: /project-v1.0.0/ Complete Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 version: \"2.0\" repositories: - url: https://github.com/myorg/api-server.git name: api paths: [\"docs\"] auth: type: token token: ${GITHUB_TOKEN} versioning: enabled: true strategy: branches_and_tags max_versions_per_repo: 5 # Include main development branches branch_patterns: - \"main\" - \"develop\" # Include semantic version tags tag_patterns: - \"v[0-9]*.[0-9]*.[0-9]*\" hugo: title: \"API Documentation\" theme: hextra params: version_menu: true output: directory: ./site clean: true Version Patterns Semantic Versioning 1 2 3 tag_patterns: - \"v[0-9]*.[0-9]*.[0-9]*\" # v1.0.0, v2.1.3 - \"[0-9]*.[0-9]*.[0-9]*\" # 1.0.0, 2.1.3 Release Branches 1 2 3 4 branch_patterns: - \"main\" - \"release-[0-9]*\" # release-1, release-2 - \"release/v[0-9]*.[0-9]*\" # release/v1.0, release/v2.1 Development Branches 1 2 3 4 5 branch_patterns: - \"main\" - \"develop\" - \"next\" - \"feature/*\" # All feature branches Authentication Versioning works with all authentication methods:\n1 2 3 4 5 6 7 8 9 repositories: - url: https://gitlab.example.com/org/project.git name: project auth: type: token token: ${GITLAB_TOKEN} versioning: enabled: true Performance Considerations Clone Time Each version requires a separate clone:\n1 repository × 5 versions = 5 clones 10 repositories × 3 versions = 30 clones Use shallow clones to reduce time:\n1 2 3 4 5 6 build: shallow_depth: 1 # Only fetch latest commit versioning: enabled: true max_versions_per_repo: 3 # Limit versions Build Time More versions = longer builds:\nDiscovery: O(1) per repository (remote query) Clone: O(n) per version Hugo: O(n) content files Incremental Builds Combine with incremental builds for best performance:\n1 2 3 4 5 6 7 build: enable_incremental: true shallow_depth: 1 versioning: enabled: true max_versions_per_repo: 5 Theme Integration Hextra Theme Hextra automatically detects version metadata:\n1 2 3 4 5 hugo: theme: hextra params: navbar: version_dropdown: true Docsy Theme Docsy requires version configuration:\n1 2 3 4 5 6 7 8 9 hugo: theme: docsy params: version_menu: \"v2.0.0\" versions: - version: \"v2.0.0\" url: \"https://docs.example.com/\" - version: \"v1.0.0\" url: \"https://v1.docs.example.com/\" Default Version The default version is determined by:\nDefault branch from Git (usually main or master) First branch alphabetically if default not found Marked as is_default: true in Hugo config Troubleshooting Tags Not Cloning Ensure patterns match your tag names:\n1 2 3 versioning: tag_patterns: - \"*\" # Temporarily match all tags for testing Check logs for:\nDEBUG msg=\"Evaluating reference for inclusion\" name=v1.0.0 type=tag include=true DEBUG msg=\"Cloning tag reference\" name=project-v1.0.0 tag=v1.0.0 ref=refs/tags/v1.0.0 Too Many Versions Reduce the limit:\n1 2 versioning: max_versions_per_repo: 3 # Keep only 3 most recent No Versions Discovered Check:\nRepository has branches/tags matching patterns Authentication is working Patterns are correct (test with * wildcard) Enable verbose logging:\n1 docbuilder build -c config.yaml -v Look for:\nINFO msg=\"Discovering versions for repository\" repo_url=... INFO msg=\"Repository expansion complete\" original=1 expanded=5 Wrong Versions Selected Adjust patterns to be more specific:\n1 2 3 4 5 6 7 8 # Too broad tag_patterns: - \"*\" # More specific tag_patterns: - \"v[0-9]*.[0-9]*.[0-9]*\" # Only semantic versions - \"!v*-rc*\" # Exclude release candidates Disabling Versioning To disable multi-version builds:\n1 2 versioning: enabled: false # Or remove versioning section entirely Or build only default branch:\n1 2 3 versioning: enabled: true default_branch_only: true",
    "description": "How to Enable Multi-Version Documentation Build documentation from multiple branches and tags to provide version-specific documentation for your users.\nOverview Multi-version documentation allows you to:\nBuild docs from multiple branches (e.g., main, develop, release-1.x) Build docs from version tags (e.g., v1.0.0, v2.0.0) Provide version switchers in your documentation site Maintain historical documentation alongside current docs Basic Setup Enable Versioning Add to your configuration:",
    "tags": [
      "Versioning",
      "Documentation"
    ],
    "title": "How To: Enable Multi-Version Documentation",
    "uri": "/docs/how-to/enable-multi-version-docs/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Enable Page Transitions This guide explains how to enable smooth page transitions using the View Transitions API in your Hugo-themed documentation site.\nOverview Page transitions provide a smooth, animated navigation experience between pages in your documentation site. DocBuilder supports the View Transitions API for all supported themes (Hextra, Docsy, and Relearn), creating fluid animations when users navigate between documentation pages.\nThe implementation uses browser-native CSS-only transitions with the @view-transition { navigation: auto; } rule, which means no JavaScript is required and all interactive elements (like search) continue to work correctly during and after transitions.\nPrerequisites Hugo theme: hextra, docsy, or relearn Modern browser with View Transitions API support: Chrome 126+ Edge 126+ Safari 18.2+ Opera 112+ Firefox: In review Configuration Add the following to your config.yaml under the hugo section:\n1 2 3 4 5 6 hugo: title: \"My Documentation Site\" theme: \"hextra\" # or \"docsy\" or \"relearn\" # Enable page transitions enable_page_transitions: true Configuration Options Option Type Default Description enable_page_transitions boolean false Enable/disable View Transitions API Example Configuration Basic Setup 1 2 3 hugo: theme: \"hextra\" enable_page_transitions: true How It Works When enabled, DocBuilder:\nInjects View Transitions API CSS into your Hugo site Adds the @view-transition { navigation: auto; } rule to enable browser-native transitions Automatically applies transitions to all page navigations Preserves all interactive elements (search, menus, etc.) without any DOM manipulation Uses theme-appropriate head partial integration: Hextra: layouts/_partials/custom/head-end.html Docsy: layouts/partials/hooks/head-end.html Relearn: layouts/partials/custom-header.html Browser Compatibility The View Transitions API is supported in:\n✅ Chrome 126+ ✅ Edge 126+ ✅ Safari 18.2+ ✅ Opera 112+ ⚠️ Firefox (in review) For browsers without View Transitions support, the site will function normally without animations (graceful degradation).\nVerifying Transitions After enabling transitions and rebuilding your site:\nNavigate to your documentation site Click between different pages You should see smooth fade animations between pages Verify interactive elements (search, menus) continue to work correctly Check browser console for any errors Troubleshooting Transitions not working:\nCheck browser compatibility (use Chrome 126+ or Safari 18.2+ for testing) Ensure you rebuilt the site after changing configuration In daemon mode, restart the daemon to apply configuration changes Verify the CSS file exists at /static/view-transitions.css Interactive elements not working after transition:\nThis should not happen with the CSS-only implementation If you experience issues, please report a bug Theme-Specific Notes Hextra Works seamlessly with FlexSearch and theme switcher.\nDocsy Compatible with Algolia/local search and Bootstrap components.\nRelearn Works with Lunr search, Mermaid diagrams, and math rendering.\nRelated Configuration View Transitions work well with other theme features:\n1 2 3 4 5 6 7 8 9 hugo: theme: \"relearn\" # or \"hextra\" or \"docsy\" enable_page_transitions: true params: # Theme-specific parameters work alongside transitions search: enable: true mermaid: enable: true Disabling Transitions To disable transitions, set enable_page_transitions: false or remove the option entirely:\n1 2 3 hugo: theme: \"hextra\" # enable_page_transitions: false # Explicitly disabled Performance Considerations Transitions add minimal overhead (~1KB of CSS) Static assets are embedded at build time No runtime performance impact on browsers without View Transitions support Transitions do not affect SEO or accessibility Technical Details The implementation uses the browser’s native View Transitions API with a simple CSS rule:\n1 2 3 @view-transition { navigation: auto; } This tells the browser to automatically handle cross-document page transitions without any JavaScript intervention. The browser manages the DOM updates, preserving all event handlers and script contexts, which is why interactive elements continue to work correctly.\nFor more information, see:\nLincoln Loop Blog: View Transitions with Hugo MDN: View Transitions API See Also Hugo Configuration Reference Enable Hugo Render",
    "description": "Enable Page Transitions This guide explains how to enable smooth page transitions using the View Transitions API in your Hugo-themed documentation site.\nOverview Page transitions provide a smooth, animated navigation experience between pages in your documentation site. DocBuilder supports the View Transitions API for all supported themes (Hextra, Docsy, and Relearn), creating fluid animations when users navigate between documentation pages.",
    "tags": [
      "Ui",
      "Transitions",
      "Hextra",
      "Docsy",
      "Relearn"
    ],
    "title": "How To: Enable Page Transitions",
    "uri": "/docs/how-to/enable-page-transitions/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Reduce disk usage by enabling top-level pruning of non‑documentation directories inside cloned repositories.\nEnable Pruning 1 2 build: prune_non_doc_paths: true Control Allow / Deny Lists 1 2 3 build: prune_allow: [LICENSE*, README.*] prune_deny: [\"*.bak\", test] Precedence (highest wins):\n.git (never removed) Explicit deny (glob or exact) Docs roots (derived from first segment of each configured docs path) Explicit allow Removal When To Use Large monorepos where only docs/ subtree is needed. CI environments with tight ephemeral storage quotas. Risks Removing assets (images, includes) referenced by Markdown if they live outside allowed roots. Mitigation Add required top-level directories to prune_allow. Temporarily disable pruning to confirm root cause of missing references. Example Repository tree:\nREADME.md build/ cmd/ docs/ scripts/ Config:\n1 2 3 build: prune_non_doc_paths: true prune_allow: [README.*] Result keeps .git, docs/, README.md; prunes build/, cmd/, scripts/.\nVerification Inspect repo directory after clone stage (verbose logging) or script a simple check:\n1 find workspace/service-a -maxdepth 1 -mindepth 1 -type d",
    "description": "Reduce disk usage by enabling top-level pruning of non‑documentation directories inside cloned repositories.\nEnable Pruning 1 2 build: prune_non_doc_paths: true Control Allow / Deny Lists 1 2 3 build: prune_allow: [LICENSE*, README.*] prune_deny: [\"*.bak\", test] Precedence (highest wins):",
    "tags": [
      "Optimization",
      "Workspace",
      "Performance"
    ],
    "title": "How To: Prune Workspace Size",
    "uri": "/docs/how-to/prune-workspace-size/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Incremental builds avoid recloning repositories and only fetch updates, saving time and bandwidth.\nChoose a Clone Strategy Set in build.clone_strategy:\nfresh: Always reclone (slowest, cleanest). update: Always attempt fast-forward/hard reset existing clones. auto: (Recommended) Use update if the directory exists else fresh. Example:\n1 2 3 build: clone_strategy: auto shallow_depth: 10 Persisting Workspaces To persist clones across runs, either:\nLeave workspace_dir unset and keep clone_strategy at auto (default resolves a stable path), or Explicitly set build.workspace_dir to a persistent location. Handling Divergence If local branches diverge (e.g., manual changes), enable hard reset:\n1 2 build: hard_reset_on_diverge: true Without this flag divergence becomes a reported issue (REMOTE_DIVERGED).\nCleaning Untracked Files Enable to remove stray generated or stale files after updates:\n1 2 build: clean_untracked: true Shallow Clones Use shallow_depth to limit history and speed up fetches:\n1 2 build: shallow_depth: 5 Detecting No-Op Builds The build log will emit:\nNo repository head changes detected (all repos unchanged) Documentation files unchanged (doc file set identical) You can also compare doc_files_hash in successive build-report.json files.\nCI Optimization Pattern Pseudocode:\n1 2 3 4 5 6 prev=$(cat prev-report.json | jq -r .doc_files_hash 2\u003e/dev/null) ./bin/docbuilder build -c config.yaml -v current=$(cat site/build-report.json | jq -r .doc_files_hash) if test \"$prev\" = \"$current\"; then echo \"Docs unchanged; skipping search index regeneration\" fi Retry Behavior Transient clone/update failures (network flake, intermittent Hugo issues) can be retried with backoff:\n1 2 3 4 5 build: max_retries: 3 retry_backoff: exponential retry_initial_delay: 1s retry_max_delay: 30s Permanent failures (auth, repo not found, unsupported protocol) short‑circuit retries and surface granular issue codes.\nSummary Use clone_strategy: auto, shallow depth, and hash comparison to keep builds fast and conditional.",
    "description": "Incremental builds avoid recloning repositories and only fetch updates, saving time and bandwidth.\nChoose a Clone Strategy Set in build.clone_strategy:\nfresh: Always reclone (slowest, cleanest). update: Always attempt fast-forward/hard reset existing clones. auto: (Recommended) Use update if the directory exists else fresh. Example:\n1 2 3 build: clone_strategy: auto shallow_depth: 10 Persisting Workspaces To persist clones across runs, either:",
    "tags": [
      "Performance",
      "Incremental",
      "Builds"
    ],
    "title": "How To: Run Incremental Builds",
    "uri": "/docs/how-to/run-incremental-builds/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "Hugo Relearn Theme Support DocBuilder includes built-in support for the Hugo Relearn theme, a documentation-focused theme with extensive features for technical documentation.\nFeatures The Relearn theme integration provides:\nHugo Modules: Automatic theme installation via Hugo Modules (no manual theme setup required) Math Support: Built-in MathJax integration for mathematical and chemical formulae Mermaid Diagrams: Native support for Mermaid diagram rendering Search: Lunr.js-powered offline search Edit Links: Automatic “Edit this page” links to source repositories Customizable Appearance: Multiple color variants and dark mode support Responsive Design: Mobile-friendly layout Multilingual: Support for 20+ languages with RTL support Quick Start Basic Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 version: \"2.0\" repositories: - name: my-docs url: https://github.com/example/docs.git branch: main paths: - docs hugo: title: \"Documentation\" theme: \"relearn\" base_url: \"https://docs.example.com\" output: directory: \"./site\" Advanced Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 hugo: title: \"Technical Documentation\" theme: \"relearn\" base_url: \"https://docs.example.com\" params: # Theme appearance themeVariant: \"auto\" # auto, relearn-light, relearn-dark, learn, neon, blue, green, red # Navigation showVisitedLinks: true collapsibleMenu: true alwaysopen: false disableBreadcrumb: false # Search disableSearch: false # Features disableLandingPageButton: true disableShortcutsTitle: false disableTagHiddenPages: false # Footer disableGeneratorVersion: false # Edit links (automatically configured by DocBuilder) editURL: enable: true # Mermaid diagrams mermaid: enable: true # Math support math: enable: true Theme Variants Relearn includes multiple built-in color schemes that can be configured in simple or advanced modes.\nShipped Variants The theme ships with the following color variants:\nRelearn Family:\nrelearn-light - Classic Relearn default with signature green, dark sidebar and light content relearn-dark - Dark variant with signature green, dark sidebar and dark content relearn-bright - Alternative with signature green, green sidebar and light content Zen Family:\nzen-light - Relaxed white/grey variant with blue accents, light sidebar and light content zen-dark - Dark variant with blue accents, dark sidebar and dark content Experimental:\nneon - Glowing dark theme with gradient sidebar Retro (Learn Theme):\nlearn - Original Learn theme with light purple, dark sidebar and light content blue - Blue-tinted Learn theme green - Green-tinted Learn theme red - Red-tinted Learn theme Simple Configuration Single Variant Use a single variant for your entire site:\n1 2 3 hugo: params: themeVariant: \"relearn-dark\" Multiple Variants with Selector Let users choose between variants via a switcher in the menu:\n1 2 3 4 5 6 hugo: params: themeVariant: - \"relearn-light\" - \"relearn-dark\" - \"neon\" The first variant is the default. A variant selector appears automatically when multiple variants are configured.\nAuto Mode (OS Light/Dark Detection) Use auto to match the operating system’s light/dark preference:\n1 2 3 4 5 6 hugo: params: themeVariant: - \"auto\" - \"relearn-light\" - \"relearn-dark\" By default, auto uses relearn-light for light mode and relearn-dark for dark mode. You can customize this:\n1 2 3 4 5 6 7 8 9 hugo: params: themeVariant: - \"auto\" - \"zen-light\" - \"neon\" themeVariantAuto: - \"zen-light\" # Light mode variant - \"neon\" # Dark mode variant Advanced Configuration For more control over variant names, auto-mode behavior, and logos, use the advanced array format:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 hugo: params: themeVariant: # Auto mode with custom name - identifier: \"relearn-auto\" name: \"Relearn Light/Dark\" auto: - \"relearn-light\" - \"relearn-dark\" # Standard variants - identifier: \"relearn-light\" - identifier: \"relearn-dark\" - identifier: \"neon\" name: \"Neon Glow\" # Zen auto mode - identifier: \"zen-auto\" name: \"Zen Light/Dark\" auto: - \"zen-light\" - \"zen-dark\" - identifier: \"zen-light\" - identifier: \"zen-dark\" Advanced Parameters:\nParameter Required Description identifier Yes Name of the color variant (must match theme-\u003cidentifier\u003e.css) name No Display name in variant selector (defaults to identifier in human-readable form) auto No Array of two variants: [light-mode, dark-mode] for OS detection logo No Override the default logo for this variant Custom Variants You can create custom variants by:\nCopy and modify: Copy a shipped variant from themes/hugo-theme-relearn/assets/css/theme-*.css to your site’s assets/css/ directory Import and extend: Create a new CSS file that imports a base variant and overrides specific variables Example custom variant (assets/css/theme-my-brand.css):\n1 2 3 4 5 6 7 8 9 @import \"theme-relearn-light.css\"; :root { --PRIMARY-color: rgba(96, 125, 139, 1); /* Your brand color */ --SECONDARY-color: rgba(236, 239, 241, 1); /* Accent color */ --CODE-theme: neon; /* Code highlighting */ --CODE-BLOCK-color: rgba(226, 228, 229, 1); --CODE-BLOCK-BG-color: rgba(40, 42, 54, 1); } Then use it in your config:\n1 2 3 hugo: params: themeVariant: \"my-brand\" See the Relearn Color Documentation and Stylesheet Generator for more customization options.\nDefault Settings DocBuilder automatically configures these defaults for Relearn:\nParameter Default Description themeVariant [\"auto\", \"zen-light\", \"zen-dark\"] Auto light/dark mode with variant selector themeVariantAuto [\"zen-light\", \"zen-dark\"] OS light/dark mode fallbacks showVisitedLinks true Mark visited pages collapsibleMenu true Collapsible sidebar sections alwaysopen false Don’t force menu sections open disableBreadcrumb false Show breadcrumb navigation disableLandingPageButton true Hide landing page button disableShortcutsTitle false Show shortcuts menu in sidebar disableTagHiddenPages false Tag hidden pages disableGeneratorVersion false Show generator version in footer mermaid.enable true Enable Mermaid diagrams math.enable true Enable MathJax support Note: editURL is not set by default. Configure it manually if you want “Edit this page” links.\nAll defaults can be overridden in your configuration’s hugo.params section.\nHugo Module Configuration DocBuilder uses Hugo Modules to automatically install Relearn. The theme is configured as:\nModule Path: github.com/McShelby/hugo-theme-relearn Version: v8.3.0 No manual theme installation required - Hugo will download the theme on first build.\nContent Structure Relearn builds navigation from your content structure. Place an _index.md in each directory to create sections:\ncontent/ ├── _index.md # Home page ├── getting-started/ │ ├── _index.md # Section index │ └── installation.md └── advanced/ ├── _index.md └── configuration.md Shortcodes Relearn includes many built-in shortcodes for rich content:\nnotice - Styled notice boxes (info, warning, tip, note) expand - Expandable content sections tabs and tab - Tabbed content button - Styled buttons mermaid - Mermaid diagrams math - Mathematical formulae See Relearn Shortcodes Documentation for full list.\nAdvanced Customization Template Overriding DocBuilder allows you to override the default index page templates. Place your custom templates in the output directory before running the build:\n1 2 3 4 5 6 outputDir/ templates/ index/ main.md.tmpl # Override main index repository.md.tmpl # Override repository index section.md.tmpl # Override section index Template Search Order (first match wins):\ntemplates/index/\u003ckind\u003e.md.tmpl templates/index/\u003ckind\u003e.tmpl templates/\u003ckind\u003e_index.tmpl If no custom template matches, DocBuilder uses its embedded default template.\nFront Matter Control DocBuilder automatically injects front matter into generated index pages with computed metadata (title, repository, section, forge, dates, edit links, etc.).\nAutomatic Injection:\n1 2 3 # My Content This gets front matter injected automatically. Disable Injection: If your template already includes YAML front matter (starts with ---), DocBuilder will not inject its own:\n1 2 3 4 5 6 7 8 --- title: \"Custom Title\" weight: 10 --- # My Content DocBuilder won't inject front matter because it's already present. This gives you complete control over front matter when needed while providing sensible defaults for most cases.\nTroubleshooting Theme Not Loading Ensure Hugo is installed and run:\n1 2 3 cd site hugo mod get -u hugo server Search Not Working Relearn search requires JavaScript. Ensure you’re viewing the built site (not raw markdown):\n1 2 cd site hugo server Edit Links Not Appearing Verify your repository configuration includes the forge URL:\n1 2 3 repositories: - url: https://github.com/example/docs.git # Must be a valid forge URL branch: main Theme Assets Missing If theme CSS/JS don’t load, ensure Hugo has been run to build the site. You can also configure render_mode: always to force Hugo execution on every build.\nTheme Variant Not Applied Check for typos in your params.themeVariant configuration. Valid values include: relearn-light, relearn-dark, zen-light, zen-dark, neon, learn, blue, green, red, or auto.\nResources Relearn Theme Documentation Relearn GitHub Repository Hugo Modules Documentation DocBuilder Configuration Reference Why Relearn? DocBuilder uses Relearn exclusively because it provides:\nRich Documentation Features: Built-in search, navigation, breadcrumbs, and shortcuts Strong Multilingual Support: 20+ languages with full i18n capabilities Extensive Shortcodes: Notice boxes, tabs, buttons, diagrams, math - no plugins needed Math \u0026 Diagrams: MathJax and Mermaid built-in Excellent Mobile Support: Responsive design that works on all devices High Customizability: Theme variants, custom CSS, configurable layouts Low Learning Curve: Intuitive structure and clear documentation Active Maintenance: Regular updates and responsive community Relearn strikes the optimal balance between features, ease of use, and maintenance burden for technical documentation.",
    "description": "Hugo Relearn Theme Support DocBuilder includes built-in support for the Hugo Relearn theme, a documentation-focused theme with extensive features for technical documentation.\nFeatures The Relearn theme integration provides:\nHugo Modules: Automatic theme installation via Hugo Modules (no manual theme setup required) Math Support: Built-in MathJax integration for mathematical and chemical formulae Mermaid Diagrams: Native support for Mermaid diagram rendering Search: Lunr.js-powered offline search Edit Links: Automatic “Edit this page” links to source repositories Customizable Appearance: Multiple color variants and dark mode support Responsive Design: Mobile-friendly layout Multilingual: Support for 20+ languages with RTL support Quick Start Basic Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 version: \"2.0\" repositories: - name: my-docs url: https://github.com/example/docs.git branch: main paths: - docs hugo: title: \"Documentation\" theme: \"relearn\" base_url: \"https://docs.example.com\" output: directory: \"./site\" Advanced Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 hugo: title: \"Technical Documentation\" theme: \"relearn\" base_url: \"https://docs.example.com\" params: # Theme appearance themeVariant: \"auto\" # auto, relearn-light, relearn-dark, learn, neon, blue, green, red # Navigation showVisitedLinks: true collapsibleMenu: true alwaysopen: false disableBreadcrumb: false # Search disableSearch: false # Features disableLandingPageButton: true disableShortcutsTitle: false disableTagHiddenPages: false # Footer disableGeneratorVersion: false # Edit links (automatically configured by DocBuilder) editURL: enable: true # Mermaid diagrams mermaid: enable: true # Math support math: enable: true Theme Variants Relearn includes multiple built-in color schemes that can be configured in simple or advanced modes.",
    "tags": [
      "Themes",
      "Relearn",
      "Hugo"
    ],
    "title": "How To: Use Relearn Theme",
    "uri": "/docs/how-to/use-relearn-theme/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e how-to",
    "content": "How to Write Cross-Document Links When writing markdown documentation that will be processed by DocBuilder, you have three options for linking between documents.\nDesign Goal: Dual Compatibility DocBuilder’s link transformation system is designed so your documentation works in both contexts:\nIn the source forge (GitHub, GitLab, Forgejo) - Links work correctly when viewing files directly in the repository web interface In the generated Hugo site - The same links are transformed to work with Hugo’s URL structure This dual compatibility means you can write standard relative markdown links (like [guide](/docs/guide)) and they’ll work correctly when:\nDevelopers browse documentation directly in GitHub/GitLab Users view the rendered documentation site Documentation is reviewed in pull requests/merge requests The transform pipeline automatically rewrites links during the build process. You write links once, and they work everywhere.\nLink Types 1. Page-Relative Links (Recommended for Nearby Files) Use relative paths from the current file’s location. These work like standard filesystem paths.\n1 2 3 4 \u003c!-- From tutorials/getting-started.md --\u003e [Other tutorial](/docs/how-to/advanced-usage) → /repo/tutorials/advanced-usage/ [Up one level](/docs/) → /repo/ [Different section](/docs/how-to/authentication) → /repo/how-to/authentication/ Best for: Links to files in the same directory or nearby directories.\n2. Repository-Root-Relative Links (Recommended for Cross-Section Links) Use paths starting with / to link from the repository root, regardless of where the current file is located.\n1 2 3 4 \u003c!-- Works from any file in the repository --\u003e [API Reference](/api/reference.md) → /repo/api/reference/ [How-To Guide](/how-to/authentication.md) → /repo/how-to/authentication/ [Tutorial](/tutorials/getting-started.md) → /repo/tutorials/getting-started/ Best for: Links between different major sections of documentation, deep directory structures, or when you want stable links that don’t break if files move.\nImportant: Repository-root-relative links are relative to your repository’s root, not the Hugo site root. DocBuilder automatically prefixes them with the repository (and forge, if applicable) during processing.\n3. Absolute Hugo Links (For Advanced Users) Use the full Hugo path including the repository name. Only necessary if linking between different repositories in a multi-repo documentation site.\n1 2 [Other repo docs](/other-repo/guide.md) → /other-repo/guide/ [Forged repo](/github/org-repo/api.md) → /github/org-repo/api/ Best for: Cross-repository links in multi-repo documentation sites.\nLink Syntax Rules Extension Handling DocBuilder automatically removes .md and .markdown extensions and adds trailing slashes for Hugo’s pretty URLs:\n1 2 [Link](/docs/how-to/guide) → [Link](/docs/how-to/guide/) [Link](/docs/how-to/tutorial) → [Link](/docs/how-to/tutorial/) Anchor Support Anchors (fragments) are preserved:\n1 2 [Section link](/docs/how-to/guide.md#installation) → [Section link](/docs/how-to/guide/#installation) [Repo-root anchor](/api/reference.md#errors) → [Repo-root anchor](/repo/api/reference/#errors) Unchanged Links These link types are never modified:\nExternal links: https://example.com/page.md Email links: mailto:user@example.com Anchor-only links: #section-heading Non-markdown links: image.png, document.pdf Common Patterns Linking from Tutorials to How-Tos If your repository has this structure:\ndocs/ tutorials/ getting-started.md how-to/ authentication.md From tutorials/getting-started.md:\n1 2 3 4 5 \u003c!-- Page-relative (requires ../): --\u003e See the [authentication guide](/docs/how-to/authentication) for details. \u003c!-- Repository-root-relative (cleaner): --\u003e See the [authentication guide](/how-to/authentication.md) for details. Linking Within the Same Section 1 2 \u003c!-- From how-to/authentication.md to how-to/authorization.md --\u003e After authentication, see [authorization](/docs/how-to/authorization). Linking to Index Pages 1 2 \u003c!-- Link to a section's index page --\u003e [API Documentation](/api/README.md) → /repo/api/ Troubleshooting Links Break When Files Move Problem: You used page-relative links, and moving files breaks the references.\nSolution: Use repository-root-relative links (starting with /) for stability:\n1 2 3 4 5 \u003c!-- Before (breaks if file moves): --\u003e [Guide](/docs/how-to/authentication) \u003c!-- After (stable): --\u003e [Guide](/how-to/authentication.md) Wrong Path in Generated Site Problem: Link resolves to /repo/tutorials/how-to/authentication/ instead of /repo/how-to/authentication/\nCause: Using page-relative link from tutorials/ directory without going up first.\nSolution: Either use ../how-to/authentication.md or use repository-root-relative /how-to/authentication.md.\nLink Works in Source but Not in Hugo Site Problem: Repository-root-relative link works in source repo but not in DocBuilder output.\nCause: Links starting with / are treated as repository-root-relative and automatically prefixed.\nSolution: This is expected behavior. DocBuilder handles the prefixing automatically.\nBest Practices Use repository-root-relative links (/section/file.md) for major cross-section navigation Use page-relative links (file.md, ../section/file.md) for nearby files in the same section Always use .md extension in source markdown - DocBuilder removes it automatically Don’t worry about trailing slashes - DocBuilder adds them automatically Test in preview before deploying to ensure links work as expected Examples by Use Case Documentation Index 1 2 3 4 5 6 \u003c!-- From README.md linking to major sections --\u003e # Documentation - [Getting Started](/tutorials/getting-started.md) - [How-To Guides](/how-to/README.md) - [API Reference](/api/reference.md) Tutorial Series 1 2 3 \u003c!-- From tutorials/part-2.md --\u003e ← Previous: [Part 1](/docs/how-to/part-1) → Next: [Part 3](/docs/how-to/part-3) API Documentation with Cross-References 1 2 \u003c!-- From api/authentication.md --\u003e See [authorization](/api/authorization.md) and [error handling](/api/errors.md). How-To Guide with Related Content 1 2 3 \u003c!-- From how-to/deploy.md --\u003e Before deploying, complete the [configuration tutorial](/tutorials/configuration.md) and review the [deployment API](/api/deployment.md).",
    "description": "How to Write Cross-Document Links When writing markdown documentation that will be processed by DocBuilder, you have three options for linking between documents.\nDesign Goal: Dual Compatibility DocBuilder’s link transformation system is designed so your documentation works in both contexts:\nIn the source forge (GitHub, GitLab, Forgejo) - Links work correctly when viewing files directly in the repository web interface In the generated Hugo site - The same links are transformed to work with Hugo’s URL structure This dual compatibility means you can write standard relative markdown links (like [guide](/docs/guide)) and they’ll work correctly when:",
    "tags": [
      "Documentation",
      "Links",
      "Markdown"
    ],
    "title": "How To: Write Cross-Document Links",
    "uri": "/docs/how-to/write-cross-document-links/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Hugo",
    "uri": "/tags/hugo/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Incremental",
    "uri": "/tags/incremental/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Index-Pages",
    "uri": "/tags/index-pages/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Introduction",
    "uri": "/tags/introduction/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Links",
    "uri": "/tags/links/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Multi-Architecture",
    "uri": "/tags/multi-architecture/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Namespacing",
    "uri": "/tags/namespacing/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "Forge Namespacing Rationale Problem Aggregating multiple repositories from different hosting forges (GitHub, GitLab, Forgejo, etc.) risks name collisions (service-api existing on two platforms) and makes future cross-forge expansion disruptive if paths are encoded without forge context.\nOptions Considered Option Pros Cons Never prefix Shorter paths Collisions; retrofits painful Always prefix Stable \u0026 explicit Longer paths for single-forge installs Conditional (current) Shorter single-forge, collision-safe multi-forge Slight path shape change when second forge added Chosen Strategy namespace_forges=auto (default): Insert \u003cforge\u003e/ only when \u003e1 forge type detected among active repositories. Users wanting stability ahead of expansion can set always.\nTrade-Offs Pros: Minimal path length for the common single-forge scenario; zero ambiguity in multi-forge. Cons: Path structure changes the first time a second forge appears (mitigated by recommending always when future expansion is known early). Front Matter Inclusion Including forge in generated page front matter allows theming \u0026 navigation to group or filter content by hosting platform.\nFuture Possibilities Per-forge landing pages summarizing repositories \u0026 health. Analytics segmented by forge type (build counts, failure rates). Theme navigation grouping (sidebars by forge). Migration Guidance If you start single-forge and later add a second forge, set namespace_forges: always for one build; adjust any hard-coded links; then leaving it on auto remains safe.",
    "description": "Forge Namespacing Rationale Problem Aggregating multiple repositories from different hosting forges (GitHub, GitLab, Forgejo, etc.) risks name collisions (service-api existing on two platforms) and makes future cross-forge expansion disruptive if paths are encoded without forge context.\nOptions Considered Option Pros Cons Never prefix Shorter paths Collisions; retrofits painful Always prefix Stable \u0026 explicit Longer paths for single-forge installs Conditional (current) Shorter single-forge, collision-safe multi-forge Slight path shape change when second forge added Chosen Strategy namespace_forges=auto (default): Insert \u003cforge\u003e/ only when \u003e1 forge type detected among active repositories. Users wanting stability ahead of expansion can set always.",
    "tags": [
      "Architecture",
      "Namespacing",
      "Design-Decisions"
    ],
    "title": "Namespacing Rationale",
    "uri": "/docs/explanation/namespacing-rationale/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Optimization",
    "uri": "/tags/optimization/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Output",
    "uri": "/tags/output/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Overview",
    "uri": "/tags/overview/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "Package Architecture Guide This document provides detailed information about each internal package in DocBuilder, including responsibilities, key types, interfaces, and usage patterns.\nTable of Contents Foundation Packages Configuration \u0026 State Core Domain Infrastructure Application Services Presentation Layer Testing Support Foundation Packages internal/foundation/errors Purpose: Unified error handling system for all DocBuilder operations.\nKey Types:\n1 2 3 4 5 6 7 8 9 10 11 12 type ClassifiedError struct { category ErrorCategory // Type-safe category enum severity ErrorSeverity // Fatal, Error, Warning, Info retry RetryStrategy // Never, Immediate, Backoff, RateLimit, User message string cause error context ErrorContext // map[string]any } type ErrorCategory string // Type-safe enum type ErrorSeverity string // Type-safe enum type RetryStrategy string // Type-safe enum Error Categories:\nCategoryConfig - Configuration errors CategoryGit - Git operations (clone, fetch, auth) CategoryAuth - Authentication failures CategoryNotFound - Resource not found errors CategoryValidation - Input validation errors CategoryHugo - Hugo generation failures CategoryBuild - Build pipeline errors CategoryNetwork - Network connectivity issues CategoryFileSystem - File I/O errors CategoryDaemon - Daemon/service errors CategoryInternal - Internal/unexpected errors Usage:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Validation error with context return errors.ValidationError(\"invalid parameter\"). WithContext(\"input\", value). WithContext(\"valid_values\", []string{\"option1\", \"option2\"}). Build() // Wrap existing error with category return errors.WrapError(err, errors.CategoryGit, \"failed to clone repository\"). WithContext(\"repository\", repoURL). WithContext(\"branch\", branch). WithRetry(errors.RetryBackoff). Build() // Extract and check error category if classified, ok := errors.AsClassified(err); ok { if classified.Category() == errors.CategoryAuth { // Handle authentication error } } Design Rationale:\nType-safe categories eliminate string-based error codes Fluent builder API makes error construction consistent Context map provides structured debugging information Retry strategy is built into error classification Severity levels support alerting and filtering HTTP/CLI adapters translate errors at system boundaries Configuration \u0026 State internal/config Purpose: Configuration loading, validation, and normalization.\nPackage Structure:\nconfig/ ├── v2.go # Main config loading ├── validation.go # Validation orchestration ├── normalize.go # Fill implicit values ├── typed/ # Domain-specific configs │ ├── hugo_config.go │ ├── daemon_config.go │ ├── forge_config.go │ └── build_config.go └── defaults.go # Default values Key Types:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Config struct { Repositories []RepositoryConfig Hugo HugoConfig Build BuildConfig Output OutputConfig Git GitConfig } type RepositoryConfig struct { URL string Name string Branch string Paths []string Auth AuthConfig } Configuration Flow:\n1. Load YAML file 2. Expand ${ENV_VAR} references 3. Parse into Config struct 4. Apply defaults 5. Normalize (fill implicit values) 6. Validate (all domains) 7. Return validated Config Validation Architecture:\n1 2 3 4 5 6 7 // Top-level orchestration func ValidateConfig(cfg *Config) error // Domain-specific validation func (hc *HugoConfig) Validate() foundation.ValidationResult func (dc *DaemonConfig) Validate() foundation.ValidationResult func (fc *ForgeConfig) Validate() foundation.ValidationResult Design Rationale:\n3-layer architecture: load → validate → typed Environment variable expansion enables secret management Normalization separates user intent from internal representation Domain validation keeps rules with domain logic internal/state Purpose: Build state management and persistence.\nPackage Structure:\nstate/ ├── build_state.go # Root state ├── git_state.go # Repository state ├── docs_state.go # Documentation state ├── pipeline_state.go # Execution metadata └── store/ # Persistence layer ├── interface.go ├── json_daemon_info_store.go ├── json_statistics_store.go └── helpers.go Key Types:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type BuildState struct { Git *GitState Docs *DocsState Pipeline *PipelineState } type GitState struct { Repositories map[string]*RepositoryState } type RepositoryState struct { Name string HEAD string LastUpdate time.Time DocHash string } type DocsState struct { Files []*DocFile TotalSize int64 } State Decomposition:\nBuildState (root) ├─ GitState │ ├─ Repository tracking │ ├─ HEAD references │ └─ Change detection │ ├─ DocsState │ ├─ DocFile list │ ├─ Size metrics │ └─ Hash fingerprint │ └─ PipelineState ├─ Stage durations ├─ Execution metadata └─ Configuration hash Store Interface:\n1 2 3 4 5 type Store interface { Save(ctx context.Context, data any) error Load(ctx context.Context) (any, error) Delete(ctx context.Context) error } Design Rationale:\nSub-states prevent god object Clear ownership boundaries JSON serialization for portability Store interface enables different backends Core Domain internal/docs Purpose: Documentation file discovery and modeling.\nKey Types:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type DocFile struct { SourcePath string // Original file path HugoPath string // Target path in Hugo Repository string // Repository name Section string // Documentation section Content []byte // File content FrontMatter map[string]any // Parsed YAML header } type DiscoveryConfig struct { Paths []string Ignores []string Extensions []string } Discovery Algorithm:\n1. Walk each configured path 2. For each file: a. Check extension (.md, .markdown) b. Apply ignore patterns c. Skip standard files (README, CONTRIBUTING, etc.) d. Create DocFile with paths: - SourcePath: repos/owner/repo/docs/guide.md - HugoPath: content/repo/docs/guide.md 3. Compute doc set hash (SHA-256 of sorted paths) 4. Return DocFile list + hash Standard Ignores:\nREADME.md CONTRIBUTING.md CHANGELOG.md LICENSE.md .github/ node_modules/ Design Rationale:\nDocFile is immutable after creation Hugo path computed at discovery time Hash enables efficient change detection Standard ignores prevent clutter internal/hugo Purpose: Hugo site generation and content processing.\nPackage Structure:\nhugo/ ├── generator.go # Main orchestrator ├── config.go # hugo.yaml generation ├── content_copy.go # Content processing ├── index.go # Index page generation ├── runner.go # Hugo binary execution ├── models/ # Transform pipeline │ ├── frontmatter.go │ ├── frontmatter_builder.go │ ├── editlink.go │ └── config_writer.go # Relearn theme configuration └── pipeline/ # Fixed transform pipeline (ADR-003) ├── transforms.go # All 11 transforms ├── generators.go # Document generators └── pipeline_test.go # Pipeline tests Key Components:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Relearn theme configuration in config_writer.go func (g *Generator) applyRelearnThemeDefaults(params map[string]any) { // Apply Relearn-specific defaults } // Fixed transform pipeline in pipeline/transforms.go func CreatePipeline() []Transform { return []Transform{ parseFrontMatter, normalizeIndexFiles, buildBaseFrontMatter, // ... 8 more transforms } } type Transformer interface { Transform(ctx context.Context, doc *DocFile) error }\n**Content Transform Pipeline:** ```go type Pipeline struct { transformers []Transformer } // Built-in transformers: 1. FrontMatterParser - Extract YAML 2. FrontMatterBuilder - Add metadata 3. EditLinkInjector - Generate edit URLs 4. FrontMatterMerger - Combine metadata 5. CustomTransformers - User-defined 6. FrontMatterSerializer - Write YAML Hugo Config Generation:\n1. Core defaults ├─ title, baseURL ├─ languageCode └─ markup settings 2. Theme.ApplyParams() ├─ Theme-specific defaults └─ Feature configuration 3. User params merge (deep) 4. Dynamic fields └─ build_date 5. Module import (if UsesModules) 6. Automatic menu (if AutoMainMenu) 7. Theme.CustomizeRoot() └─ Final adjustments 8. Write hugo.yaml Design Rationale:\nTheme interface enables extensibility Features flag capabilities declaratively Transform pipeline is composable Hugo binary execution is optional internal/forge Purpose: Git hosting platform abstraction.\nPackage Structure:\nforge/ ├── base_forge.go # Common HTTP operations ├── interface.go # Forge interface ├── capabilities.go # Feature detection ├── github.go # GitHub implementation ├── gitlab.go # GitLab implementation └── forgejo.go # Forgejo/Gitea implementation Key Interfaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type Forge interface { Name() string Type() string GetRepository(owner, repo string) (*Repository, error) GetFileContent(owner, repo, path, ref string) ([]byte, error) ListRepositories(org string) ([]*Repository, error) Capabilities() Capabilities } type Capabilities struct { Webhooks bool AutoDiscovery bool EditLinks bool FileContent bool } type BaseForge struct { client *http.Client baseURL string authHeader string customHeaders map[string]string } HTTP Consolidation:\nAll forge clients compose BaseForge:\n1 2 3 4 5 6 7 8 9 type GitHubClient struct { *BaseForge } func NewGitHubClient(config ForgeConfig) *GitHubClient { base := NewBaseForge(config.BaseURL, config.Token) base.SetCustomHeader(\"X-GitHub-Api-Version\", \"2022-11-28\") return \u0026GitHubClient{BaseForge: base} } Common Operations:\n1 2 3 4 5 6 // BaseForge provides: func (bf *BaseForge) NewRequest(method, path string) (*http.Request, error) func (bf *BaseForge) DoRequest(req *http.Request) ([]byte, error) func (bf *BaseForge) DoRequestWithHeaders(req *http.Request) ([]byte, http.Header, error) func (bf *BaseForge) SetAuthHeaderPrefix(prefix string) func (bf *BaseForge) SetCustomHeader(key, value string) Design Rationale:\nBaseForge eliminates HTTP duplication Composition over inheritance Capabilities enable feature detection Custom headers support API versioning internal/git Purpose: Git operations and authentication.\nPackage Structure:\ngit/ ├── git.go # Main client ├── auth.go # Authentication ├── workspace.go # Workspace management └── head.go # HEAD reference reading Key Types:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type Client struct { workspaceManager *workspace.Manager auth AuthStrategy } type AuthStrategy interface { Name() string Apply(config *CloneConfig) error } type CloneConfig struct { URL string Branch string Depth int Workspace string } Authentication Methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // SSH keys type SSHAuth struct { keyPath string passphrase string } // Personal access tokens type TokenAuth struct { token string } // Basic username/password type BasicAuth struct { username string password string } Git Operations:\n1 2 3 4 5 6 7 8 9 10 11 // Clone or update repository func (c *Client) CloneOrUpdate(ctx context.Context, config CloneConfig) error // Read HEAD reference func (c *Client) ReadRepoHead(repoPath string) (string, error) // Check if repository exists func (c *Client) RepositoryExists(path string) bool // Clean workspace func (c *Client) CleanWorkspace(path string) error Design Rationale:\nStrategy pattern for authentication Workspace manager handles temp directories HEAD reading supports change detection Shallow clones optimize performance Infrastructure internal/workspace Purpose: Temporary directory lifecycle management.\nKey Types:\n1 2 3 4 5 6 7 8 9 10 11 type Manager struct { baseDir string tempDir string persistent bool } func NewManager(baseDir string) *Manager func NewPersistentManager(baseDir, subdirName string) *Manager func (m *Manager) Create() error func (m *Manager) GetPath() string func (m *Manager) Cleanup() error Workspace Lifecycle:\nEphemeral Mode: 1. NewManager() → Configure base directory 2. Create() → /tmp/docbuilder-{timestamp}/ 3. Use workspace for build 4. Cleanup() → Remove directory Persistent Mode: 1. NewPersistentManager() → Configure fixed path 2. Create() → baseDir/subdirName (reused across builds) 3. Use workspace for build 4. Cleanup() → No-op (directory persists) Features:\nTimestamped directories prevent conflicts (ephemeral mode) Persistent directories for incremental builds (daemon mode) Safe concurrent operations Automatic cleanup on error (ephemeral mode only) Design Rationale:\nSimple focused interface Predictable naming convention Explicit cleanup (no GC reliance) internal/storage (Removed) Note: This package was removed as part of simplifying the CLI build process. The daemon’s skip evaluation system (using internal/state) provides equivalent functionality without the complexity of content-addressable storage.\nHistorical Purpose: Content-addressed storage abstraction for CLI incremental builds.\nHash-Based Paths:\nSHA-256 hash of content First 2 chars as directory Remainder as filename Natural deduplication Design Rationale:\nContent-addressed prevents duplicates Flat hierarchy with bucketing GC supports cleanup Filesystem-based for simplicity internal/eventstore Purpose: Immutable event log for build lifecycle.\nKey Types:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type Event struct { ID string Timestamp time.Time Type EventType Data json.RawMessage CorrelationID string } type EventStore interface { Append(ctx context.Context, event *Event) error Query(ctx context.Context, filter EventFilter) ([]*Event, error) } type EventFilter struct { Types []EventType StartTime *time.Time EndTime *time.Time CorrelationID string } Event Types:\n1 2 3 4 5 6 7 8 9 const ( EventBuildStarted EventType = \"build.started\" EventBuildCompleted EventType = \"build.completed\" EventBuildFailed EventType = \"build.failed\" EventRepositoryCloned EventType = \"repository.cloned\" EventRepositoryUpdated EventType = \"repository.updated\" EventDocumentationDiscovered EventType = \"documentation.discovered\" EventHugoSiteGenerated EventType = \"hugo.generated\" ) Implementations:\n1 2 3 4 5 6 7 8 9 10 11 // In-memory (testing) type MemoryEventStore struct { events []*Event mu sync.RWMutex } // File-based (production) type FileEventStore struct { filePath string mu sync.RWMutex } Design Rationale:\nEvents are immutable (append-only) Correlation ID traces related events Query interface supports projections JSON serialization for portability Application Services internal/services Purpose: Service interface definitions for lifecycle management.\nPackage Structure:\nservices/ └── interfaces.go # ManagedService, StateManager interfaces Key Interfaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type ManagedService interface { Name() string Start(ctx context.Context) error Stop(ctx context.Context) error Health() HealthStatus Dependencies() []string } type StateManager interface { Load() error Save() error IsLoaded() bool LastSaved() *time.Time } type HealthStatus struct { Status string Message string CheckAt time.Time } Usage:\nDaemon implements ManagedService interface State managers implement StateManager interface Provides common contracts for service orchestration Design Rationale:\nInterface-only package (no concrete implementations) Enables decoupled service management Standard health checking pattern internal/build (Current Pipeline) Purpose: Sequential pipeline for building documentation sites.\nPackage Structure:\nbuild/ ├── service.go # BuildService implementation ├── default_service.go # Default pipeline executor ├── stages.go # Stage definitions └── report.go # Build reporting Pipeline Stages:\nPrepareOutput → CloneRepos → DiscoverDocs → GenerateConfig → Layouts → CopyContent → Indexes → RunHugo (optional) Key Interfaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 type BuildService interface { Run(ctx context.Context, req BuildRequest) (*BuildReport, error) } type BuildRequest struct { Config *config.Config OutputDir string } type BuildReport struct { Status BuildStatus StartTime time.Time EndTime time.Time Stages []StageReport Issues []BuildIssue Summary string Metrics map[string]interface{} } type StageExecutor interface { Execute(ctx context.Context, state *state.BuildState) error Name() string } type Runner struct { stages []StageExecutor config *config.Config eventStore eventstore.EventStore } func (r *Runner) Run(ctx context.Context) (*BuildReport, error) Stage Execution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 for _, stage := range r.stages { start := time.Now() // Emit start event r.eventStore.Append(ctx, \u0026Event{ Type: EventStageStarted, Data: stage.Name(), }) // Execute stage err := stage.Execute(ctx, state) duration := time.Since(start) // Record in state state.Pipeline.RecordStage(stage.Name(), duration, err) // Emit complete event r.eventStore.Append(ctx, \u0026Event{ Type: EventStageCompleted, Data: StageResult{ Name: stage.Name(), Duration: duration, Error: err, }, }) if err != nil { return nil, err } } Change Detection:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type ChangeDetector struct { state *state.BuildState } type ChangeSet struct { ChangedRepos []*config.RepositoryConfig SkippedRepos []*config.RepositoryConfig Reasons map[string]string } func (cd *ChangeDetector) DetectChanges( ctx context.Context, repos []*config.RepositoryConfig, ) (*ChangeSet, error) Design Rationale:\nSequential stage execution Event emission at stage boundaries Change detection enables incremental Context propagation for cancellation internal/incremental (Removed) Note: This package was removed as part of simplifying the CLI build process. It provided change detection and signature caching for CLI incremental builds, but this functionality overlapped with the daemon’s skip evaluation system. The daemon uses internal/state with rule-based validation instead, which is simpler and more maintainable.\nHistorical Purpose: Change detection and signature management for CLI incremental builds.\nPrevious Design:\nMulti-level detection (HEAD comparison, tree hashing, doc files hashing) Signature cache persisted across builds Hash-based comparison for determinism Optional detection levels for speed vs accuracy trade-offs Presentation Layer cmd/docbuilder/commands Purpose: Command-line interface using Kong framework.\nPackage Structure:\ncmd/docbuilder/ ├── main.go # CLI entry point └── commands/ ├── build.go # Build command ├── init.go # Init command ├── discover.go # Discovery command ├── daemon.go # Daemon command ├── preview.go # Preview command ├── generate.go # Generate command ├── visualize.go # Visualize command └── common.go # Shared helpers Command Structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type CLI struct { Verbose bool `short:\"v\" help:\"Verbose logging\"` Build BuildCmd `cmd:\"\" help:\"Build documentation site\"` Init InitCmd `cmd:\"\" help:\"Initialize configuration\"` Discover DiscoverCmd `cmd:\"\" help:\"Discover documentation\"` Daemon DaemonCmd `cmd:\"\" help:\"Run as daemon\"` Preview PreviewCmd `cmd:\"\" help:\"Preview documentation\"` Generate GenerateCmd `cmd:\"\" help:\"Generate assets\"` Visualize VisualizeCmd `cmd:\"\" help:\"Visualize pipeline\"` } type BuildCmd struct { Config string `short:\"c\" help:\"Configuration file\"` Output string `short:\"o\" help:\"Output directory\"` RenderMode string `help:\"Hugo render mode (always|auto|never)\"` } func (cmd *BuildCmd) Run(ctx *Context) error { // Load config // Create build service // Execute build // Handle errors via foundation/errors CLI adapter } Design Rationale:\nKong provides type-safe parsing Commands are small, focused Services handle business logic CLI only handles I/O and errors internal/server Purpose: HTTP server for API and webhooks.\nPackage Structure:\nserver/ ├── server.go # Server setup ├── handlers/ # Request handlers │ ├── webhook.go # Forge webhooks │ ├── build.go # Build API │ ├── status.go # Status endpoint │ └── metrics.go # Metrics endpoint ├── middleware/ # HTTP middleware │ ├── logging.go # Request logging │ ├── auth.go # Authentication │ ├── recovery.go # Panic recovery │ └── cors.go # CORS headers └── responses/ # Response types ├── json.go # JSON responses └── errors.go # Error responses Server Setup:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type Server struct { config *config.Config router *http.ServeMux buildSvc *services.BuildService } func (s *Server) Start(ctx context.Context) error { // Register routes s.router.HandleFunc(\"/api/v1/build\", s.handleBuild) s.router.HandleFunc(\"/api/v1/status\", s.handleStatus) s.router.HandleFunc(\"/webhook/github\", s.handleGitHubWebhook) s.router.HandleFunc(\"/webhook/gitlab\", s.handleGitLabWebhook) s.router.HandleFunc(\"/webhook/forgejo\", s.handleForgejoWebhook) s.router.HandleFunc(\"/metrics\", s.handleMetrics) // Apply middleware handler := s.applyMiddleware(s.router) // Start server return http.ListenAndServe(\":8080\", handler) } Webhook Handling:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func handleForgeWebhook( w http.ResponseWriter, r *http.Request, eventHeader string, source string, ) { // Read event type eventType := r.Header.Get(eventHeader) // Parse payload var payload WebhookPayload json.NewDecoder(r.Body).Decode(\u0026payload) // Trigger build if relevant event if isPushEvent(eventType) { buildService.Build(context.Background()) } // Respond w.WriteHeader(http.StatusOK) } Design Rationale:\nStandard library HTTP server Middleware pattern for cross-cutting concerns Shared webhook handler eliminates duplication JSON responses with proper error codes Testing Support internal/testing Purpose: Test utilities and builders.\nPackage Structure:\ntesting/ ├── config_builder.go # Fluent config builders ├── file_assertions.go # File/directory assertions ├── cli_runner.go # CLI integration testing └── fixtures.go # Test data ConfigBuilder:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type ConfigBuilder struct { t *testing.T config *config.Config } func NewConfigBuilder(t *testing.T) *ConfigBuilder func (b *ConfigBuilder) WithGitHubForge( name, token string, ) *ConfigBuilder func (b *ConfigBuilder) WithRepository( url, name, branch string, ) *ConfigBuilder func (b *ConfigBuilder) WithAutoDiscovery( org string, ) *ConfigBuilder func (b *ConfigBuilder) Build() *config.Config // Usage: cfg := NewConfigBuilder(t). WithGitHubForge(\"github\", \"token\"). WithRepository(\"https://github.com/user/repo\", \"repo\", \"main\"). WithAutoDiscovery(\"myorg\"). Build() File Assertions:\n1 2 3 4 func AssertFileExists(t *testing.T, path string) func AssertDirExists(t *testing.T, path string) func AssertFileContains(t *testing.T, path, content string) func AssertHugoConfigValid(t *testing.T, path string) CLI Runner:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type CLIRunner struct { binary string } func (r *CLIRunner) Run( t *testing.T, args ...string, ) (stdout, stderr string, err error) // Usage: runner := NewCLIRunner(\"./docbuilder\") stdout, stderr, err := runner.Run(t, \"build\", \"-c\", \"config.yaml\") assert.NoError(t, err) assert.Contains(t, stdout, \"Build complete\") Design Rationale:\nFluent builders reduce test boilerplate Assertions fail with helpful messages CLI runner enables integration tests Fixtures provide realistic test data internal/testforge Purpose: Mock forge implementation for testing.\nKey Types:\n1 2 3 4 5 6 7 8 9 10 11 type MockForge struct { repositories map[string]*Repository files map[string][]byte errors map[string]error } func NewMockForge() *MockForge func (m *MockForge) AddRepository(repo *Repository) func (m *MockForge) AddFile(path string, content []byte) func (m *MockForge) SetError(operation string, err error) Usage:\n1 2 3 4 5 6 7 8 9 10 forge := testforge.NewMockForge() forge.AddRepository(\u0026forge.Repository{ Name: \"test-repo\", Owner: \"test-owner\", }) forge.AddFile(\"README.md\", []byte(\"# Test\")) // Use in tests client := git.NewClient(forge) repo, err := client.GetRepository(\"test-owner\", \"test-repo\") Design Rationale:\nEliminates external dependencies in tests Predictable behavior for edge cases Error injection for failure testing Fast test execution Package Dependencies Summary Dependency Rules:\n✅ Allowed:\nLower layers → Foundation Application → Domain Domain → Infrastructure (via interfaces) Presentation → Application ❌ Prohibited:\nFoundation → Any application package Domain → Application Infrastructure → Presentation Import Matrix:\nPackage | Can Import -----------------|------------------------------------------ cmd/ | cli/, foundation/ cli/ | services/, config/, foundation/ server/ | services/, config/, foundation/ services/ | pipeline/, config/, state/, foundation/ pipeline/ | docs/, hugo/, git/, config/, state/ config/ | foundation/ state/ | foundation/ docs/ | config/, foundation/ hugo/ | config/, docs/, foundation/ forge/ | foundation/ git/ | workspace/, foundation/ workspace/ | foundation/ storage/ | foundation/ eventstore/ | foundation/ testing/ | All (test-only) References Comprehensive Architecture Architecture Diagrams Architecture Overview ADR-000: Uniform Error Handling",
    "description": "Package Architecture Guide This document provides detailed information about each internal package in DocBuilder, including responsibilities, key types, interfaces, and usage patterns.\nTable of Contents Foundation Packages Configuration \u0026 State Core Domain Infrastructure Application Services Presentation Layer Testing Support Foundation Packages internal/foundation/errors Purpose: Unified error handling system for all DocBuilder operations.",
    "tags": [
      "Architecture",
      "Packages",
      "Code-Organization"
    ],
    "title": "Package Architecture",
    "uri": "/docs/explanation/package-architecture/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Packages",
    "uri": "/tags/packages/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Performance",
    "uri": "/tags/performance/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "The docbuilder visualize command provides multiple ways to visualize the transform pipeline, making it easy to understand execution order, stage grouping, and dependency relationships.\nQuick Start 1 2 3 4 5 6 7 8 9 10 docbuilder visualize docbuilder visualize -f mermaid \u003e docs/pipeline.md docbuilder visualize -f dot -o pipeline.dot dot -Tpng pipeline.dot -o pipeline.png docbuilder visualize -f json -o pipeline.json docbuilder visualize --list Output Formats Text (Default) Human-readable ASCII art with boxes, arrows, and dependency indicators.\nTransform Pipeline Visualization ================================= ┌─ Stage 1: parse │ │ └── [front_matter_parser] │ ↓ ┌─ Stage 2: build │ │ └── [front_matter_builder_v2] │ ⤷ depends on: front_matter_parser │ ↓ ... Best for: Quick terminal inspection, debugging, documentation\nMermaid GitHub and GitLab compatible diagram syntax.\n1 2 3 4 5 6 7 8 9 10 11 12 ```mermaid graph TD subgraph parse[\"Stage: parse\"] frontmatterparser[\"front_matter_parser\"] end subgraph build[\"Stage: build\"] frontmatterbuilderv2[\"front_matter_builder_v2\"] end frontmatterparser --\u003e frontmatterbuilderv2 ... ``` Best for: README files, GitHub/GitLab wikis, documentation sites\nRender automatically on: GitHub, GitLab, Notion, Obsidian, many markdown viewers\nDOT (Graphviz) Professional graph visualization format.\ndigraph TransformPipeline { rankdir=TB; node [shape=box, style=rounded]; subgraph cluster_0 { label=\"Stage: parse\"; style=filled; color=lightgrey; \"front_matter_parser\"; } \"front_matter_parser\" -\u003e \"front_matter_builder_v2\"; ... } Best for: High-quality images, presentations, printed documentation\nRender with:\n1 2 3 4 5 dot -Tpng pipeline.dot -o pipeline.png dot -Tsvg pipeline.dot -o pipeline.svg dot -Tpdf pipeline.dot -o pipeline.pdf JSON Structured machine-readable format.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"transforms\": [ { \"name\": \"front_matter_parser\", \"stage\": \"parse\", \"order\": 1, \"dependencies\": { \"mustRunAfter\": [], \"mustRunBefore\": [] } }, ... ], \"totalTransforms\": 9, \"totalStages\": 7 } Best for: Tooling, analysis scripts, CI/CD pipelines, monitoring dashboards\nUse Cases 1. Understanding Pipeline Flow View the complete transform execution order:\n1 docbuilder visualize This shows:\nAll 7 stages in order Transforms within each stage Dependency relationships Total transform count 2. Debugging Transform Issues When a transform isn’t running as expected:\n1 docbuilder visualize | grep my_transform Check:\nWhich stage it’s in What it depends on What depends on it 3. Documenting Architecture Add pipeline diagram to your README:\n1 docbuilder visualize -f mermaid \u003e\u003e README.md The Mermaid diagram will render automatically on GitHub/GitLab.\n4. Creating Presentations Generate high-quality pipeline diagram:\n1 2 docbuilder visualize -f dot -o pipeline.dot dot -Tpng -Gdpi=300 pipeline.dot -o pipeline-hires.png 5. CI/CD Integration Verify pipeline configuration in CI:\n1 2 3 #!/bin/bash docbuilder visualize -f json \u003e pipeline.json jq '.transforms[].name' pipeline.json | grep \"my_required_transform\" 6. Monitoring Pipeline Changes Track pipeline evolution over time:\n1 2 3 4 5 docbuilder visualize -f json \u003e pipeline-baseline.json docbuilder visualize -f json \u003e pipeline-current.json diff pipeline-baseline.json pipeline-current.json CLI Reference Flags -f, --format - Output format: text, mermaid, dot, json (default: text) -o, --output - Write to file instead of stdout -l, --list - List available formats and exit Examples 1 2 3 4 5 6 7 8 9 10 11 12 13 docbuilder visualize docbuilder visualize -f mermaid docbuilder visualize -f dot docbuilder visualize -f json docbuilder visualize -o pipeline.txt docbuilder visualize -f mermaid -o docs/pipeline.md docbuilder visualize -f dot -o pipeline.dot docbuilder visualize -f json -o pipeline.json docbuilder visualize --list docbuilder visualize --help Understanding the Output Stages Transforms are organized into 7 stages that execute in order:\nparse - Extract and parse source content build - Generate base metadata enrich - Add computed fields merge - Combine/merge data transform - Modify content finalize - Post-process serialize - Output generation Dependencies Two types of dependencies control execution order within stages:\nMustRunAfter (⤷) - This transform depends on another MustRunBefore (⤶) - Another transform depends on this Example:\n└── [edit_link_injector_v2] ⤷ depends on: front_matter_builder_v2 Means edit_link_injector_v2 runs after front_matter_builder_v2.\nExecution Order Transforms execute in order:\nGrouped by stage (parse → build → … → serialize) Within each stage, ordered by dependencies (topological sort) If no dependencies, alphabetical order Integration with Other Tools With jq (JSON processing) 1 2 3 4 5 docbuilder visualize -f json | jq '.transforms | group_by(.stage) | map({stage: .[0].stage, count: length})' docbuilder visualize -f json | jq '.transforms[] | {name: .name, deps: .dependencies.mustRunAfter}' docbuilder visualize -f json | jq '.transforms[] | select(.dependencies.mustRunAfter == [])' With graphviz (DOT rendering) 1 2 3 4 5 docbuilder visualize -f dot | dot -Tpng \u003e pipeline.png docbuilder visualize -f dot | dot -Tsvg \u003e pipeline.svg docbuilder visualize -f dot | dot -Tpdf \u003e pipeline.pdf docbuilder visualize -f dot | dot -Tpng -Grankdir=LR \u003e pipeline-horizontal.png With grep (Text searching) 1 2 3 4 5 docbuilder visualize | grep -A 3 \"my_transform\" docbuilder visualize | grep \"depends on\" docbuilder visualize | grep \"Stage\" | wc -l Troubleshooting “No transforms registered” The visualize command needs transforms to be registered. This should work automatically, but if you see this error:\nEnsure you’re running from the project root Check that imports are working correctly Try running a build first: docbuilder build then docbuilder visualize Mermaid diagram not rendering If your Mermaid diagram doesn’t render on GitHub/GitLab:\nEnsure you have the triple backticks with mermaid language identifier Check the syntax is valid at https://mermaid.live GitHub/GitLab must have Mermaid support enabled (it’s usually on by default) DOT rendering issues If dot command is not found:\n1 2 3 4 5 sudo apt-get install graphviz brew install graphviz sudo dnf install graphviz Advanced Usage Custom Styling for DOT Modify the DOT output for custom styling:\n1 2 3 4 docbuilder visualize -f dot -o pipeline.dot dot -Tpng pipeline.dot -o styled-pipeline.png Pipeline Diff Script Track pipeline changes over time:\n1 2 3 4 5 #!/bin/bash DATE=$(date +%Y%m%d) docbuilder visualize -f json \u003e \"pipeline-snapshots/$DATE.json\" git add \"pipeline-snapshots/$DATE.json\" git commit -m \"Pipeline snapshot $DATE\" Documentation Generator Auto-generate pipeline documentation:\n1 2 3 4 5 6 7 8 9 #!/bin/bash echo \"# Transform Pipeline\" \u003e docs/pipeline.md echo \"\" \u003e\u003e docs/pipeline.md echo \"Generated: $(date)\" \u003e\u003e docs/pipeline.md echo \"\" \u003e\u003e docs/pipeline.md docbuilder visualize -f mermaid \u003e\u003e docs/pipeline.md echo \"\" \u003e\u003e docs/pipeline.md echo \"## Transform Details\" \u003e\u003e docs/pipeline.md docbuilder visualize \u003e\u003e docs/pipeline.md See Also Transform Validation - Validate pipeline before running Add Content Transforms - Create custom transforms Content Transforms Reference - Technical architecture",
    "description": "The docbuilder visualize command provides multiple ways to visualize the transform pipeline, making it easy to understand execution order, stage grouping, and dependency relationships.\nQuick Start 1 2 3 4 5 6 7 8 9 10 docbuilder visualize docbuilder visualize -f mermaid \u003e docs/pipeline.md docbuilder visualize -f dot -o pipeline.dot dot -Tpng pipeline.dot -o pipeline.png docbuilder visualize -f json -o pipeline.json docbuilder visualize --list Output Formats Text (Default) Human-readable ASCII art with boxes, arrows, and dependency indicators.",
    "tags": [
      "Pipeline",
      "Visualization",
      "Diagrams"
    ],
    "title": "Pipeline Visualization",
    "uri": "/docs/reference/pipeline-visualization/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Quickstart",
    "uri": "/tags/quickstart/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Relearn",
    "uri": "/tags/relearn/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Renderer",
    "uri": "/tags/renderer/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "Hugo Renderer Testing Strategy Overview The Hugo renderer system uses a dual-path testing approach to ensure both the NoopRenderer (for CI/fast tests) and BinaryRenderer (for integration tests) work correctly.\nTest Files renderer_test.go Purpose: Fast unit tests that don’t require Hugo binary\nKey Tests:\nTestNoopRenderer: Verifies NoopRenderer marks site as rendered without invoking Hugo Uses WithRenderer(\u0026NoopRenderer{}) to inject test renderer Sets render_mode=always to ensure rendering is attempted Verifies StaticRendered=true even when Hugo binary is missing renderer_integration_test.go Purpose: Integration tests that verify actual Hugo execution when available\nKey Tests:\nTestBinaryRenderer_WhenHugoAvailable\nSkips if Hugo not in PATH Verifies BinaryRenderer invokes real Hugo binary Gracefully handles Hugo failures (e.g., missing theme dependencies) Checks for public/ directory creation TestBinaryRenderer_MissingHugoBinary\nVerifies proper error handling when Hugo unavailable Tests the BinaryRenderer error path directly TestRenderMode_Never_SkipsRendering\nVerifies render_mode=never prevents all rendering Ensures no public/ directory is created TestRenderMode_Always_WithNoopRenderer\nVerifies custom renderer takes precedence NoopRenderer should run even with render_mode=always TestRenderMode_Auto_WithoutEnvVars\nVerifies render_mode=auto behavior Tests legacy env var handling TestRendererPrecedence\nComprehensive test matrix documenting renderer selection priority Tests all combinations of render modes and renderer types Testing Strategy CI Environment (No Hugo Binary) All tests use NoopRenderer to avoid requiring Hugo installation:\n1 2 cfg.Build.RenderMode = \"always\" g := NewGenerator(cfg, dir).WithRenderer(\u0026NoopRenderer{}) Benefits:\nFast test execution No external dependencies Tests the rendering pipeline logic Verifies StaticRendered tracking Local Development (Hugo Available) Integration tests automatically detect Hugo and run real rendering:\n1 2 3 4 if _, err := exec.LookPath(\"hugo\"); err != nil { t.Skip(\"Hugo binary not found; skipping integration test\") } // Test runs with real Hugo binary Benefits:\nVerifies end-to-end Hugo integration Catches Hugo-specific issues Tests actual static site generation Renderer Selection Priority The actual priority (from stage_run_hugo.go):\nrender_mode=never → Skip all rendering (return early) Custom renderer set → Use custom renderer (e.g., NoopRenderer) shouldRunHugo() check → Evaluate render mode and Hugo availability Fallback → Use BinaryRenderer with Hugo binary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func stageRunHugo(ctx context.Context, bs *BuildState) error { // 1. Check render_mode=never if mode == config.RenderModeNever { return nil } // 2. Use custom renderer if set if bs.Generator.renderer != nil { // Execute custom renderer (e.g., NoopRenderer) // Sets StaticRendered=true on success } // 3. Check if default Hugo binary should run if !shouldRunHugo(cfg) { return nil } // 4. Fallback to BinaryRenderer bs.Generator.runHugoBuild() } Adding New Renderer Tests For Unit Tests (No Hugo Required) Use NoopRenderer and test the logic:\n1 2 3 4 5 6 7 func TestMyFeature(t *testing.T) { cfg := \u0026config.Config{} cfg.Build.RenderMode = \"always\" g := NewGenerator(cfg, t.TempDir()).WithRenderer(\u0026NoopRenderer{}) // Test your feature } For Integration Tests (Hugo Required) Skip when Hugo unavailable:\n1 2 3 4 5 6 7 8 func TestMyHugoFeature(t *testing.T) { if _, err := exec.LookPath(\"hugo\"); err != nil { t.Skip(\"Hugo not available\") } g := NewGenerator(cfg, t.TempDir()) // Uses BinaryRenderer // Test with real Hugo } Common Patterns Test Renderer Execution Path 1 2 3 4 5 6 // Verify NoopRenderer was used g := NewGenerator(cfg, dir).WithRenderer(\u0026NoopRenderer{}) report, _ := g.GenerateSite(files) if !report.StaticRendered { t.Error(\"NoopRenderer should set StaticRendered=true\") } Test Hugo Failure Handling 1 2 3 4 5 // Hugo may fail but shouldn't crash the build g := NewGenerator(cfg, dir) // BinaryRenderer report, err := g.GenerateSite(files) // err should be nil (warnings don't return errors) // report.StaticRendered may be false if Hugo failed Test Render Mode Behavior 1 2 3 4 5 6 // Test each mode modes := []config.RenderMode{ config.RenderModeNever, // No rendering config.RenderModeAlways, // Always attempt config.RenderModeAuto, // Conditional } Debugging Test Failures “Hugo binary not found” in CI ✅ Expected - Use NoopRenderer for CI tests\n“StaticRendered=false” with BinaryRenderer ✅ Expected - Hugo may fail without proper theme setup\nCheck logs for “Renderer execution failed”\n“public/ directory exists but StaticRendered=false” ✅ Expected - Hugo creates public/ before failing\nThis is normal for partial renders\nCustom renderer not being used ❌ Problem - Check that WithRenderer() is called\nVerify render_mode is not “never”\nBest Practices Always use NoopRenderer in CI - Don’t depend on Hugo being installed Skip integration tests gracefully - Use t.Skip() when Hugo unavailable Test the interface, not implementation - Focus on StaticRendered and public/ dir Handle Hugo failures gracefully - Real Hugo may fail in tests, that’s OK Document test expectations - Use clear test names and comments Example: Complete Test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func TestExampleFeature(t *testing.T) { // Setup cfg := \u0026config.Config{} cfg.Hugo.Theme = \"hextra\" cfg.Build.RenderMode = \"always\" dir := t.TempDir() // Use NoopRenderer for fast CI-friendly tests g := NewGenerator(cfg, dir).WithRenderer(\u0026NoopRenderer{}) doc := docs.DocFile{ Repository: \"test\", Name: \"test\", RelativePath: \"test.md\", DocsBase: \"docs\", Extension: \".md\", Content: []byte(\"# Test\\n\"), } // Execute report, err := g.GenerateSiteWithReportContext(context.Background(), []docs.DocFile{doc}) // Verify if err != nil { t.Fatalf(\"unexpected error: %v\", err) } if !report.StaticRendered { t.Error(\"expected StaticRendered=true with NoopRenderer\") } // NoopRenderer doesn't create public/, that's expected publicDir := filepath.Join(dir, \"public\") if _, err := os.Stat(publicDir); err == nil { t.Error(\"NoopRenderer shouldn't create public/ directory\") } } Related Documentation Renderer Architecture CI/CD Setup Testing Guidelines",
    "description": "Hugo Renderer Testing Strategy Overview The Hugo renderer system uses a dual-path testing approach to ensure both the NoopRenderer (for CI/fast tests) and BinaryRenderer (for integration tests) work correctly.\nTest Files renderer_test.go Purpose: Fast unit tests that don’t require Hugo binary",
    "tags": [
      "Testing",
      "Renderer"
    ],
    "title": "Renderer Testing",
    "uri": "/docs/explanation/renderer-testing/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Rendering",
    "uri": "/tags/rendering/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Reports",
    "uri": "/tags/reports/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Settings",
    "uri": "/tags/settings/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e Explanation Documentation",
    "content": "Skip Evaluation System Overview The skip evaluation system prevents unnecessary full rebuilds when nothing has changed. It uses a rule-based validation approach to determine if a build can be safely skipped, comparing current state against previous build artifacts and persisted metadata.\nWhen Builds Are Skipped A build will be skipped (reusing previous public/ output) when all of the following are true:\nNo repository changes: All repository commits match the previous build No configuration changes: Hugo config hash is identical to previous build No version changes: DocBuilder and Hugo versions match previous build Previous build exists: Valid build-report.json and public/ directory exist Content integrity: All content files and their hashes match previous build When Builds Are Forced A full rebuild will occur when any of these conditions are detected:\nCondition Why Rebuild? Repository added/removed Content structure changed Repository updated (new commits) Documentation content changed DocBuilder version changed New features, bug fixes, compatibility Hugo version changed Rendering engine updates Configuration changed Parameters, theme settings, URLs, etc. Previous build missing/corrupt Cannot validate skip safety Content file modified outside git Integrity violation Architecture Components ┌────────────────────────────────────────────────────────┐ │ BuildService │ │ ┌──────────────────────────────────────────────────┐ │ │ │ SkipEvaluatorFactory │ │ │ │ Creates evaluator with: │ │ │ │ - Output directory │ │ │ │ - State manager (commit/config tracking) │ │ │ │ - Hugo generator (config hash computation) │ │ │ └──────────────────────────────────────────────────┘ │ │ │ │ │ ▼ │ │ ┌──────────────────────────────────────────────────┐ │ │ │ SkipEvaluator (daemon wrapper) │ │ │ │ Delegates to validation-based evaluator │ │ │ └──────────────────────────────────────────────────┘ │ │ │ │ │ ▼ │ │ ┌──────────────────────────────────────────────────┐ │ │ │ validation.SkipEvaluator (core logic) │ │ │ │ Executes validation rule chain │ │ │ └──────────────────────────────────────────────────┘ │ └────────────────────────────────────────────────────────┘ Validation Rules Rules are executed in order, with early exit on first failure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Phase 1: Basic prerequisites BasicPrerequisitesRule // State manager, generator, repos exist ConfigHashRule // Configuration unchanged PublicDirectoryRule // Previous build artifacts exist // Phase 2: Previous build validation PreviousReportRule // build-report.json exists and valid // Phase 3: Change detection VersionMismatchRule // DocBuilder + Hugo versions match ContentIntegrityRule // File tree structure unchanged GlobalDocHashRule // Overall content hash unchanged PerRepoDocHashRule // Per-repository content hashes match CommitMetadataRule // All repository commits match Rule Validation Pattern Each rule implements this interface:\n1 2 3 4 5 6 7 8 9 type Rule interface { Name() string Validate(ctx Context) Result } type Result struct { Passed bool Reason string // Why validation failed } Rules have access to:\nContext.State: Persisted commit/config metadata Context.Generator: Current Hugo configuration Context.Repos: Current repository list Context.OutDir: Output directory path Context.PrevReport: Previous build report (loaded by PreviousReportRule) Configuration Enabling Skip Evaluation Daemon Mode (enabled by default):\n1 2 build: skip_if_unchanged: true # Default for daemon CLI Mode (opt-in):\n1 2 build: skip_if_unchanged: false # Default for CLI Configuration Hash The config hash is computed from:\nHugo configuration (title, base_url, theme, params, etc.) Build configuration (render mode, skip settings, etc.) Repository list (URLs, branches, paths, auth) Changes to any of these trigger a rebuild.\nState Persistence The skip evaluator relies on the StateManager to track:\n1 2 3 4 5 6 7 8 9 10 11 12 13 type DaemonStateManager interface { // Configuration tracking GetLastConfigHash() string SaveConfigHash(hash string) error // Repository tracking GetLastCommit(repoName string) string SaveCommit(repoName, commitSHA string) error // Document hashing GetLastDocHash(repoName string) string SaveDocHash(repoName, hash string) error } State is persisted in /data/state/daemon-state.json and survives daemon restarts.\nBuild Report When a build is skipped, the evaluator returns the previous build report unmodified:\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \"status\": \"success\", \"timestamp\": \"2024-01-15T10:30:00Z\", \"repositories\": [ { \"name\": \"myrepo\", \"commit\": \"abc123def456\", \"docs_found\": 42, \"errors\": [] } ], \"checksum\": \"sha256:...\" } The caller cannot distinguish a skipped build from a successful build - this is intentional for idempotency.\nIntegration with Daemon Factory Pattern The daemon uses a factory to create the skip evaluator with late binding:\n1 2 3 4 5 6 7 8 WithSkipEvaluatorFactory(func(outputDir string) build.SkipEvaluator { if daemon.stateManager == nil { return nil // Not initialized yet } gen := hugo.NewGenerator(daemon.config, outputDir) inner := NewSkipEvaluator(outputDir, daemon.stateManager, gen) return \u0026skipEvaluatorAdapter{inner: inner} }) This allows:\nLazy creation: Evaluator created only when needed during build Late binding: State manager initialized after build service creation Type adaptation: Bridge typed daemon.SkipEvaluator to generic build.SkipEvaluator Type Adapter The skipEvaluatorAdapter bridges the type gap:\n1 2 3 4 5 // daemon.SkipEvaluator (typed) Evaluate(repos []config.Repository) (*hugo.BuildReport, bool) // build.SkipEvaluator (generic) Evaluate(repos []any) (report any, canSkip bool) The adapter performs runtime type checking and conversion.\nTesting Unit Tests Validation rules are tested in isolation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func TestVersionMismatchRule(t *testing.T) { ctx := Context{ State: \u0026mockState{ lastVersion: \"1.0.0\", lastHugoVersion: \"0.120.0\", }, } // Current version differs version.SetVersion(\"1.0.1\") result := VersionMismatchRule{}.Validate(ctx) assert.False(t, result.Passed) assert.Contains(t, result.Reason, \"version mismatch\") } Integration Tests End-to-end skip behavior is tested in daemon integration tests:\n1 2 3 4 5 6 7 8 9 10 func TestDaemon_SkipUnchangedBuilds(t *testing.T) { // First build - should run fully result1 := daemon.Build(ctx, req) assert.Equal(t, BuildStatusSuccess, result1.Status) // Second build - same repos, should skip result2 := daemon.Build(ctx, req) assert.Equal(t, BuildStatusSuccess, result2.Status) assert.True(t, result2.Skipped) // Build was skipped } Performance Impact Skip evaluation adds minimal overhead:\nConfig hash computation: ~5ms (YAML marshaling + SHA256) File tree scan: ~10-50ms (depends on content size) State manager lookups: ~1ms (in-memory with disk cache) Rule validation: ~20-100ms total Total overhead: ~50-200ms vs. full rebuild: 5-30 seconds\nThe cost of skip validation is negligible compared to git operations and Hugo rendering.\nLogging Skip decisions are logged at INFO level:\n# Skip successful INFO Build skipped - no changes detected repositories=3 config_hash=abc123 version=1.2.3 # Skip failed - version changed INFO Build required - version mismatch previous_version=1.2.2 current_version=1.2.3 # Skip failed - repo updated INFO Build required - repository changes detected repository=myrepo previous_commit=abc123 current_commit=def456 # Skip failed - config changed INFO Build required - configuration changed previous_hash=abc123 current_hash=def456 Troubleshooting Skip Not Working Symptom: Builds always run fully even when nothing changed\nDiagnosis:\nCheck skip_if_unchanged is enabled in config Verify state manager is initialized (/data/state/daemon-state.json exists) Check logs for skip validation failures Ensure public/ directory and build-report.json exist from previous build Common Causes:\nConfiguration changes not reflected in config hash Timestamps in config (use static values) File system changes outside git (edited files directly) State file corruption or deletion False Skips Symptom: Build skipped but content appears outdated\nDiagnosis:\nCheck repository commits match: git log -1 --format=%H Verify content file integrity (no manual edits) Compare config hashes (previous vs. current) Common Causes:\nFiles edited outside git (breaks content integrity) Force-pushed branches (commit SHA same but content differs) Symlinked content (changes not tracked) Future Enhancements Potential improvements to the skip system:\nPartial rebuilds: Skip unchanged repos, rebuild only changed ones Content diffing: Detect file-level changes without full tree scan Incremental Hugo: Use Hugo’s --gc and caching for faster builds Parallel validation: Run rules concurrently for large repositories Skip hints: Allow repos to declare “always rebuild” vs. “safe to skip” Related Documentation ADR-002: In-Memory Content Pipeline Incremental Builds Build Service Architecture",
    "description": "Skip Evaluation System Overview The skip evaluation system prevents unnecessary full rebuilds when nothing has changed. It uses a rule-based validation approach to determine if a build can be safely skipped, comparing current state against previous build artifacts and persisted metadata.\nWhen Builds Are Skipped A build will be skipped (reusing previous public/ output) when all of the following are true:",
    "tags": [
      "Optimization",
      "Performance"
    ],
    "title": "Skip Evaluation Logic",
    "uri": "/docs/explanation/skip-evaluation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Static-Sites",
    "uri": "/tags/static-sites/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Style-Guide",
    "uri": "/tags/style-guide/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Testing",
    "uri": "/tags/testing/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Themes",
    "uri": "/tags/themes/index.html"
  },
  {
    "breadcrumb": "Test \u003e DocBuilder Documentation \u003e reference",
    "content": "Transform Pipeline Validation (DEPRECATED) ⚠️ DEPRECATED: This document describes validation for the old registry-based transform system that was removed on December 16, 2025.\nThe new fixed transform pipeline uses explicit ordering and does not require dependency validation. See ADR-003: Fixed Transform Pipeline for current architecture.\nHistorical Documentation Validation Features 1. Missing Dependency Detection The validator checks that all transforms referenced in MustRunAfter and MustRunBefore declarations actually exist in the registry.\nExample Error:\ntransform \"my_enricher\" depends on missing transform \"nonexistent_builder\" (MustRunAfter) 2. Circular Dependency Detection The validator detects circular dependencies that would prevent the pipeline from establishing a valid execution order.\nExample Error:\ncircular dependency detected: transform_a → transform_b → transform_a 3. Invalid Stage Validation Transforms must declare one of the seven valid stages. Unknown stages are rejected.\nValid stages: parse, build, enrich, merge, transform, finalize, serialize\nExample Error:\ntransform \"custom\" has invalid stage \"preprocess\" 4. Cross-Stage Dependency Warnings The validator warns when a transform depends on another transform in a later stage, as this dependency cannot be enforced.\nExample Warning:\ntransform \"builder\" (stage build) depends on \"enricher\" (stage enrich) which runs in a later stage - dependency may not be effective 5. Unused Transform Detection Transforms that are not referenced by any other transform’s dependencies receive a warning (informational only).\nExample Warning:\ntransform \"custom_filter\" is not referenced by any other transform's dependencies - ensure it's intentionally standalone API Reference ValidatePipeline() Performs comprehensive validation of the entire transform pipeline.\n1 2 3 4 5 6 7 8 9 10 11 result := transforms.ValidatePipeline() if !result.Valid { for _, err := range result.Errors { log.Printf(\"ERROR: %s\", err) } } for _, warn := range result.Warnings { log.Printf(\"WARNING: %s\", warn) } Returns: *ValidationResult with Valid flag, Errors slice, and Warnings slice.\nGetPipelineInfo() Returns a human-readable description of the current pipeline configuration.\n1 2 3 4 5 info, err := transforms.GetPipelineInfo() if err != nil { log.Fatal(err) } fmt.Println(info) Output Example:\nTransform Pipeline Execution Order =================================== Stage: parse ---------------------------------------- • front_matter_parser Stage: build ---------------------------------------- • front_matter_builder_v2 MustRunAfter: front_matter_parser Stage: enrich ---------------------------------------- • edit_link_injector_v2 MustRunAfter: front_matter_builder_v2 ... Total transforms: 9 PrintValidationResult() Formats a validation result for display.\n1 2 3 result := transforms.ValidatePipeline() output := transforms.PrintValidationResult(result) fmt.Print(output) Output Example:\nTransform Pipeline Validation ============================== ✓ Pipeline is valid with no warnings or with errors:\nTransform Pipeline Validation ============================== ✗ Errors (2): 1. transform \"builder\" depends on missing transform \"parser_v3\" (MustRunAfter) 2. circular dependency detected: a → b → a ⚠ Warnings (1): 1. transform \"custom\" is not referenced by any other transform's dependencies ValidatePipelineWithSuggestions() Returns validation results along with helpful suggestions for fixing issues.\n1 2 3 4 5 6 7 8 result, suggestions := transforms.ValidatePipelineWithSuggestions() if !result.Valid { fmt.Println(\"Validation failed!\") for _, suggestion := range suggestions { fmt.Println(suggestion) } } ListTransformNames() Returns a sorted list of all registered transform names.\n1 2 3 4 names := transforms.ListTransformNames() for _, name := range names { fmt.Println(name) } Integration Automatic Validation Validation runs automatically in Generator.copyContentFiles() before transform execution:\n1 2 3 4 5 6 7 8 func (g *Generator) copyContentFiles(ctx context.Context, docFiles []docs.DocFile) error { // Validate transform pipeline before execution if err := g.ValidateTransformPipeline(); err != nil { return fmt.Errorf(\"%w: %w\", herrors.ErrContentTransformFailed, err) } // Build and execute transforms... } Manual Validation You can validate the pipeline programmatically:\n1 2 3 4 generator := hugo.NewGenerator(cfg, outputDir) if err := generator.ValidateTransformPipeline(); err != nil { log.Fatalf(\"Pipeline validation failed: %v\", err) } Error Handling Validation errors are fatal and will prevent the build from proceeding. This is intentional - a malformed pipeline would produce incorrect or incomplete output.\nWarnings are informational and logged but do not prevent execution. They indicate potential issues that should be reviewed but may be intentional.\nBest Practices Test New Transforms: Always run validation after adding new transforms to catch missing dependencies early.\nReview Warnings: Don’t ignore warnings - they often indicate configuration issues even if they’re not fatal.\nUse Descriptive Names: Clear transform names make validation messages easier to understand.\nDocument Dependencies: Comment why specific dependencies exist, especially for non-obvious orderings.\nValidate Before Commit: Run validation as part of your pre-commit hooks or CI pipeline.\nDebugging Pipeline Issues View Current Pipeline 1 2 info, _ := transforms.GetPipelineInfo() fmt.Println(info) Check Specific Transform 1 2 3 4 names := transforms.ListTransformNames() if slices.Contains(names, \"my_transform\") { fmt.Println(\"Transform is registered\") } Validate Before Build 1 2 3 4 result := transforms.ValidatePipeline() if !result.Valid { // Fix issues before running build } Testing The validation system includes comprehensive test coverage:\nTestValidatePipeline_Valid - Valid pipeline passes TestValidatePipeline_MissingDependency - Detects missing dependencies TestValidatePipeline_CircularDependency - Detects cycles TestValidatePipeline_InvalidStage - Rejects invalid stages TestValidatePipeline_CrossStageWarning - Warns about cross-stage deps TestValidatePipeline_EmptyRegistry - Handles empty registry Plus 6 more tests for helper functions Run validation tests:\n1 go test ./internal/hugo/transforms -run TestValidate -v Performance Validation is fast and runs only once at the start of content copying. The overhead is negligible (microseconds for typical pipelines with \u003c20 transforms).\nResults are not cached because the registry can change between builds, but within a single build, validation runs exactly once.",
    "description": "Transform Pipeline Validation (DEPRECATED) ⚠️ DEPRECATED: This document describes validation for the old registry-based transform system that was removed on December 16, 2025.\nThe new fixed transform pipeline uses explicit ordering and does not require dependency validation. See ADR-003: Fixed Transform Pipeline for current architecture.\nHistorical Documentation Validation Features 1. Missing Dependency Detection The validator checks that all transforms referenced in MustRunAfter and MustRunBefore declarations actually exist in the registry.",
    "tags": [
      "Validation",
      "Transforms",
      "Testing",
      "Deprecated"
    ],
    "title": "Transform Validation Reference (DEPRECATED)",
    "uri": "/docs/reference/transform-validation/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Transitions",
    "uri": "/tags/transitions/index.html"
  },
  {
    "breadcrumb": "Test \u003e Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category :: Tutorials",
    "uri": "/categories/tutorials/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Ui",
    "uri": "/tags/ui/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Usage",
    "uri": "/tags/usage/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Versioning",
    "uri": "/tags/versioning/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Visualization",
    "uri": "/tags/visualization/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Workspace",
    "uri": "/tags/workspace/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Yaml",
    "uri": "/tags/yaml/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Quality-Assurance",
    "uri": "/tags/quality-assurance/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Error-Handling",
    "uri": "/tags/error-handling/index.html"
  },
  {
    "breadcrumb": "Test \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Foundation",
    "uri": "/tags/foundation/index.html"
  }
]
