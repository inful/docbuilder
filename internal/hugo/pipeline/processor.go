package pipeline

import (
	"fmt"
	"log/slog"

	"git.home.luguber.info/inful/docbuilder/internal/config"
)

// Processor runs the complete content processing pipeline.
// It coordinates generators (create missing files) and transforms (modify content).
type Processor struct {
	config     *config.Config
	generators []FileGenerator
	transforms []FileTransform
}

// NewProcessor creates a new pipeline processor with default generators and transforms.
func NewProcessor(cfg *config.Config) *Processor {
	p := &Processor{
		config:     cfg,
		generators: defaultGenerators(),
		transforms: defaultTransforms(cfg),
	}
	return p
}

// ProcessContent runs the complete content processing pipeline.
// This is the main entry point for content transformation.
//
// Pipeline phases:
// 1. Generation - Create missing files (indexes, etc.)
// 2. Transformation - Process all documents (discovered + generated)
//
// Returns processed documents ready for Hugo site generation.
func (p *Processor) ProcessContent(discovered []*Document, repoMetadata map[string]RepositoryInfo) ([]*Document, error) {
	// Inject repository metadata (URL, commit) into discovered documents
	// This must happen before generation/transformation so edit links work correctly
	for _, doc := range discovered {
		if doc.Repository != "" {
			if repoInfo, ok := repoMetadata[doc.Repository]; ok {
				doc.SourceURL = repoInfo.URL
				doc.SourceCommit = repoInfo.Commit
			}
		}
	}

	// Phase 1: Generation - Create missing files
	slog.Info("Pipeline: Starting generation phase", slog.Int("discovered", len(discovered)))

	ctx := &GenerationContext{
		Discovered:         discovered,
		Config:             p.config,
		RepositoryMetadata: repoMetadata,
	}

	var generated []*Document
	for i, generator := range p.generators {
		docs, err := generator(ctx)
		if err != nil {
			return nil, fmt.Errorf("generator %d failed: %w", i, err)
		}
		if len(docs) > 0 {
			slog.Debug("Generator created files", slog.Int("count", len(docs)), slog.Int("generator", i))
		}
		generated = append(generated, docs...)
	}

	slog.Info("Pipeline: Generation phase complete", slog.Int("generated", len(generated)))

	// Phase 2: Transformation - Process all documents
	allDocs := append(discovered, generated...)
	slog.Info("Pipeline: Starting transformation phase", slog.Int("total_docs", len(allDocs)))

	processedDocs, err := p.processTransforms(allDocs)
	if err != nil {
		return nil, fmt.Errorf("transformation phase failed: %w", err)
	}

	slog.Info("Pipeline: Transformation phase complete", slog.Int("processed", len(processedDocs)))

	return processedDocs, nil
}

// processTransforms runs all transforms on documents, handling dynamic document generation.
// New documents generated by transforms are queued and processed through the full pipeline.
func (p *Processor) processTransforms(docs []*Document) ([]*Document, error) {
	processedDocs := make([]*Document, 0, len(docs))
	queue := append([]*Document{}, docs...)
	processedCount := 0

	for len(queue) > 0 {
		doc := queue[0]
		queue = queue[1:]

		// Run all transforms on this document
		for i, transform := range p.transforms {
			newDocs, err := transform(doc)
			if err != nil {
				return nil, fmt.Errorf("transform %d failed for %s: %w", i, doc.Path, err)
			}

			// Prevent generated documents from creating new documents (infinite loop protection)
			if len(newDocs) > 0 && doc.Generated {
				return nil, fmt.Errorf(
					"generated document %s attempted to create new documents (transforms should not generate from generated docs)",
					doc.Path,
				)
			}

			// Queue new documents for full transform pipeline
			if len(newDocs) > 0 {
				slog.Debug("Transform generated new documents",
					slog.Int("count", len(newDocs)),
					slog.String("source", doc.Path),
					slog.Int("transform", i))
				queue = append(queue, newDocs...)
			}
		}

		processedDocs = append(processedDocs, doc)
		processedCount++

		// Log progress periodically
		if processedCount%100 == 0 {
			slog.Debug("Pipeline: Transform progress",
				slog.Int("processed", processedCount),
				slog.Int("queued", len(queue)))
		}
	}

	return processedDocs, nil
}

// WithGenerators replaces the default generators with custom ones.
// Useful for testing or custom build scenarios.
func (p *Processor) WithGenerators(generators []FileGenerator) *Processor {
	p.generators = generators
	return p
}

// WithTransforms replaces the default transforms with custom ones.
// Useful for testing or custom build scenarios.
func (p *Processor) WithTransforms(transforms []FileTransform) *Processor {
	p.transforms = transforms
	return p
}

// defaultGenerators returns the standard set of file generators.
// Order matters: main index → repository indexes → section indexes.
func defaultGenerators() []FileGenerator {
	return []FileGenerator{
		generateMainIndex,       // 1. Create site _index.md
		generateRepositoryIndex, // 2. Create repo _index.md files
		generateSectionIndex,    // 3. Create section _index.md files
	}
}

// defaultTransforms returns the standard set of content transforms.
// Order matters: this is the explicit, fixed pipeline execution order.
func defaultTransforms(cfg *config.Config) []FileTransform {
	return []FileTransform{
		parseFrontMatter,               // 1. Parse YAML front matter from content
		normalizeIndexFiles,            // 2. Rename README to _index
		buildBaseFrontMatter,           // 3. Build base front matter structure
		extractIndexTitle,              // 4. Extract H1 title from index files
		stripHeading,                   // 5. Strip H1 if appropriate
		rewriteRelativeLinks(cfg),      // 6. Fix markdown links
		rewriteImageLinks,              // 7. Fix image paths
		generateFromKeywords,           // 8. Create new files based on keywords (e.g., @glossary)
		addRepositoryMetadata(cfg),     // 9. Add repo/commit/source metadata
		addEditLink(cfg),               // 10. Generate edit URL
		serializeDocument,              // 11. Serialize to final bytes (FM + content)
	}
}
